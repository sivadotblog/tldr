Petabyte Image Pipelines üñºÔ∏è, Precision on Demand üîß, Identity to Data Control üîê

[TLDR](/)

[Newsletters](/newsletters)

[Advertise](https://advertise.tldr.tech/)

[TLDR](/)

# TLDR Data 2025-10-30

## Petabyte Image Pipelines üñºÔ∏è, Precision on Demand üîß, Identity to Data Control üîê

üì±

### Deep Dives

[### Datology's Distributed Pipelines for Handling PBs of Image Data (15 minute read)](https://www.amplifypartners.com/blog-posts/datologys-distributed-pipelines-for-handling-pbs-of-image-data?utm_source=tldrdata)

Datology built distributed pipelines to curate petabyte-scale image datasets, enabling AI researchers to deduplicate, filter, and cluster billions of web images using custom Spark/Ray operations. Powered by a modified Flyte orchestrator and Postgres catalog, it deploys seamlessly into customer environments, delivering faster, cheaper, and more efficient models.

[### We Built a Vector Search Engine that Lets You Choose Precision at Query Time (26 minute read)](https://clickhouse.com/blog/qbit-vector-search?utm_source=tldrdata)

ClickHouse's QBit is a new column type that stores floating-point vectors as bit planes, enabling users to dynamically choose precision at query time by reading only the needed bits. This eliminates upfront quantization trade-offs, reduces I/O and compute by up to 75% respectively, and delivers tunable recall vs. speed.

[### How Nubank Built an In-house Logging Platform for 1 Trillion Log Entries (5 minute read)](https://blog.bytebytego.com/p/how-nubank-built-an-in-house-logging?utm_source=tldrdata)

Nubank replaced a costly, inflexible third-party logging system with an in-house platform that ingests 1 trillion logs daily using Fluent Bit, micro-batching with custom buffering, and processing services. It stores 45 PB in Parquet on AWS S3 and enables 15,000 fast Trino queries per day while cutting costs by 50%.

[### Vector Sync Patterns: Keeping AI Features Fresh When Your Data Changes (51 minute video)](https://www.infoq.com/presentations/ai-vector-event-driven/?utm_source=tldrdata)

Vector embeddings in AI apps become stale when source data, models, or business rules change, requiring event-driven synchronization via CDC, Kafka, and Flink to keep semantic search and RAG accurate. Five vector sync patterns, such as Dependency-Aware Propagator and Versioned Vector Registry, are designed to solve this complex, multi-dimensional challenge.

üöÄ

### Opinions & Advice

[### Beyond the Perimeter: Practical Patterns for Fine-Grained Data Access (100 minute podcast)](https://www.dataengineeringpodcast.com/identity-credentials-access-management-for-data-systems-episode-486?utm_source=tldrdata)

Composable data stacks fracture Identity, Credentials, and Access Management. This discussion maps how to restore identity and auditability. Propagate short-lived JWT/OIDC across hops, externalize policy (OPA/Rego, Cedar), enforce via DB RLS/CLS or proxies, label from catalog+lineage, and bind policy to data (OpenTDF) and log provenance. Bottom line: compose trust across identity, policy, and data paths, secure choke points, standardize claims, and design streaming interfaces to avoid brittle per-system hacks.

[### Are We Thinking About Ontologies Wrong? (16 minute read)](https://ontologist.substack.com/p/are-we-thinking-about-ontologies?utm_source=tldrdata)

Resource Description Framework reliance on global Internationalized Resource Identifiers (IRI) is limiting for real-world data modeling, where context and local scoping often define semantics more effectively than strict, global standards. Knowledge graphs benefit from contextual, composable property shapes (via SHACL), late binding, and scoped ontologies. This enables schema evolution, effective disambiguation, and federated interoperability without global identifier consensus (via blank nodes). Applying these innovations enables continuous integration of new facts, entity resolution, and knowledge refinement into ever-evolving world views.

üíª

### Launches & Tools

[### Write Kafka streams directly to S3 to slash 80% of costs (Sponsor)](https://aiven.io/inkless?utm_source=---&amp;utm_medium=sponsored&amp;&amp;utm_content=tldr)

Aiven Inkless is diskless Kafka. Run sub-100ms streams and 80% cheaper batch topics ‚Äî in the same Kafka cluster. [Inkless](https://aiven.io/inkless?utm_source=---&utm_medium=sponsored&&utm_content=tldr) replaces Kafka I/O with cloud-native storage to deliver data persistence via decentralized architecture, deployed as a stateless service directly in your VPC. [See how much you can save](https://aiven.io/inkless?utm_source=---&utm_medium=sponsored&&utm_content=tldr)

[### SQLite Graph Database Extension (GitHub Repo)](https://github.com/agentflare-ai/sqlite-graph?utm_source=tldrdata)

sqlite-graph is a SQLite extension that turns SQLite into a graph database, letting you store nodes and relationships and query them using the Cypher graph language. It supports creating and querying graph structures directly from SQL or Cypher, with basic graph algorithms and Python bindings included. Still in alpha, it is useful for prototyping graph workloads without needing a separate graph database.

[### What's New in Apache Polaris 1.2.0: Fine-Grained Access, Event Persistence, and Better Federation (4 minute read)](https://www.dremio.com/blog/whats-new-in-apache-polaris-1-2-0-fine-grained-access-event-persistence-and-better-federation/?utm_source=tldrdata)

Apache Polaris 1.2.0 introduces granular access controls, sub-catalog RBAC for federated catalogs, and persistent catalog event logging (currently in preview), enhancing governance and observability across multi-engine Iceberg lakehouses. Additional features include IAM-based authentication for Amazon RDS/Aurora PostgreSQL, extended S3-compatible storage support, and streamlined credential management. The release strengthens security, catalog integrity, and operational flexibility.

[### Iceberg CDC: Stream a Little Dream of Me (11 minute read)](https://medium.com/fresha-data-engineering/iceberg-cdc-stream-a-little-dream-of-me-a7c9f9e6e11d?utm_source=tldrdata)

Apache Iceberg's immutable snapshots excel at batch processing, but struggle with real-time CDC: frequent small updates rely on costly equality deletes. Iceberg v3 introduces deletion vectors for precise row masking without full scans, and row lineage for stable identities, enabling efficient CDC views. v4 proposes a single Root Manifest per snapshot to consolidate deltas, allowing CDC readers to diff changes with minimal I/O.

[### Valkey 9.0 Debuts Multidatabase Clustering for Massive-Scale Workloads (3 minute read)](https://thenewstack.io/valkey-9-0-debuts-multidatabase-clustering-for-massive-scale-workloads/?utm_source=tldrdata)

Valkey is an in-memory datastore, backward-compatible with Redis, under the BSD 3-Clause License. Valkey 9.0 introduces multidatabase clustering, atomic slot migration, and major performance optimizations, virtually scaling over 1 billion requests per second. This aims to ease migration for large-scale, production-critical workloads across cloud and on-premises environments.

üéÅ

### Miscellaneous

[### Backpressure in Distributed Systems (20 minute read)](https://blog.pranshu-raj.me/posts/backpressure/?utm_source=tldrdata)

Backpressure happens when fast producers overwhelm slower consumers, causing memory issues, dropped data, or high latency. Systems handle it by slowing producers, dropping queued or incoming messages, or scaling consumers so processing keeps pace. The key insight for data professionals is to design pipelines with explicit backpressure strategies rather than relying on infinite buffering, which helps maintain stability and predictable performance in distributed systems.

[### huggingface\_hub v1.0: Five Years of Building the Foundation of Open Machine Learning (8 minute read)](https://huggingface.co/blog/huggingface-hub-v1?utm_source=tldrdata)

huggingface\_hub has reached v1.0 after five years. It now serves as a core dependency for 200,000+ repositories and powers access to over 2 million models, 500,000 datasets, and 1 million Spaces. Version 1.0 delivers major upgrades: a migration to httpx (enabling HTTP/2 and unified async/sync APIs), hf\_xet for chunk-based file transfers (77 PB migrated), and a fully revamped CLI with Typer.

‚ö°Ô∏è

### Quick Links

[### Streaming Datasets: 100x More Efficient (4 minute read)](https://huggingface.co/blog/streaming-datasets?utm_source=tldrdata)

Hugging Face's new `streaming=true` enables instant high-speed training on multi-terabyte remote datasets, often faster than local SSDs.

[### OpenTelemetry Adoption Update: Rust, Prometheus and Other Speed Bumps (5 minute read)](https://thenewstack.io/opentelemetry-adoption-update-rust-prometheus-and-other-speed-bumps/?utm_source=tldrdata)

OpenTelemetry is becoming the standard for observability, but adoption is slowed by complexity and incomplete language support, especially for Rust.

## Curated deep dives, tools and trends in big data, data science and data engineering üìä

Subscribe

Join 308,008 readers for [one daily email](/api/latest/data)

[Privacy](/privacy)[Careers](https://jobs.ashbyhq.com/tldr.tech)[Advertise](/data/advertise)

Timestamp: 1761870426