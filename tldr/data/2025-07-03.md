Agentic Workflows for Data ‚õìÔ∏è, Art of SQL Optimization üé®, Petabyte-scale Observability üîç

[TLDR](/)

[Newsletters](/newsletters)

[Advertise](https://advertise.tldr.tech/)

[TLDR](/)

# TLDR Data 2025-07-03

## Agentic Workflows for Data ‚õìÔ∏è, Art of SQL Optimization üé®, Petabyte-scale Observability üîç

üì±

### Deep Dives

[### DuckDB and the Semantic Layer (7 minute read)](https://performancede.substack.com/p/duckdb-and-the-semantic-layer?utm_source=tldrdata)

Using DuckDB with Cube.dev enables a simple yet powerful way to create a semantic layer. It offers a dynamic, zero-maintenance way to define metrics once and expose them across BI tools via a PostgreSQL-compatible interface. The setup is lightweight (Docker + YAML), supports live edits, and avoids the overhead of traditional ETL pipelines or data cubes.

[### Building a Dynamic Inventory Optimisation System: A Deep Dive (6 minute read)](https://engineering.zalando.com/posts/2025/06/inventory-optimisation-system.html?utm_source=tldrdata)

Zalando's inventory optimization system employs time-series forecasting using gradient-boosted trees on AWS SageMaker to predict demand across its European logistics network, serving over 50 million customers. It processes real-time sales transactions, customer returns, and partner inventory data using PySpark on Databricks, and stores the results in Delta Lake.

[### Why OpenAI Chose ClickHouse for Petabyte-scale Observability (8 minute read)](https://clickhouse.com/blog/why-openai-uses-clickhouse-for-petabyte-scale-observability?utm_source=tldrdata)

OpenAI selected ClickHouse to manage petabytes of log data daily with a 20% monthly growth rate. It uses a 90-shard ClickHouse cluster with two replicas per shard and Fluent Bit agents for log collection. The system employs a cloud-native architecture with tiered storage, keeping 80% of queries on recent data stored on disk for sub-second access, while older logs are offloaded to blob storage.

[### How Agoda Handles Kafka Consumer Failover Across Data Centers (10 minute read)](https://medium.com/agoda-engineering/how-agoda-handles-kafka-consumer-failover-across-data-centers-a3edbacef6d0?utm_source=tldrdata)

Agoda implements an active-passive consumption model across multiple on-premise data centers, where data is replicated using MirrorMaker 2 to synchronize consumer group offsets between separate Kafka clusters. A custom synchronization service maps source cluster offsets to target cluster offsets using OffsetSync records, enabling failover to a secondary data center.

üöÄ

### Opinions & Advice

[### It is Time to Take Agentic Workflows for Data Work Seriously (8 minute read)](https://roundup.getdbt.com/p/it-is-time-to-take-agentic-workflows?utm_source=tldrdata)

Rapid advancements in agentic workflows now enable building a production-ready Semantic Layer in under two hours, reducing manual effort by more than 75%. Tools like Claude Code, dbt MCP, and modern LLMs shift data engineering from traditional coding toward high-level goal decomposition and execution. While current models struggle with complex data context and dependencies, early adoption offers significant productivity boosts and tangible ROI.

[### How Data Products Become the Promethean Fire (5 minute read)](https://medium.com/justeattakeaway-tech/how-data-products-become-the-promethean-fire-8f837fdac6ef?utm_source=tldrdata)

Treating Data Products as menu items in an internal marketplace that is richly described, cataloged, and proactively marketed empowers teams to generate new offerings, simplify discovery, and spark innovation across every business unit. Balancing entrepreneurial freedom with governance guardrails, such as defining minimal metadata, lifecycle policies, and quality certifications, ensures firelight insights do not blaze into chaos while maintaining trust and agility. This shift can transform data teams from reactive order-takers into visionary product entrepreneurs.

[### The Great Data Divergence: Why Generative AI Demands a New Approach Beyond the Data Lake (5 minute read)](https://mlops.community/the-great-data-divergence-why-generative-ai-demands-a-new-approach-beyond-the-data-lake/?utm_source=tldrdata)

Generative AI's need for real-time, contextual data is exposing major latency and duplication issues in traditional data lake architectures, especially with batch-oriented ETL/ELT pipelines. The future of enterprise data strategy lies in decentralized, API-first access‚Äîleveraging API management layers (e.g., Google Apigee) that deliver operational data on-demand, while maintaining robust governance at the API layer.

üíª

### Launches & Tools

[### The Art of SQL Query Optimization (7 minute read)](https://jnidzwetzki.github.io/2025/06/03/art-of-query-optimization.html?utm_source=tldrdata)

SQL's declarative nature leaves execution choices, like index scans versus full table scans, to the optimizer, but understanding exactly when and why it switches plans can be elusive. The Plan Explorer tool turns PostgreSQL plans into artful diagrams by sweeping a two-dimensional parameter space and, for each combination, capturing the chosen query plan and the actual versus estimated tuple counts. Visualizations inspired by the ‚ÄúPicasso‚Äù optimizer visualizer reveal the decision boundaries where PostgreSQL shifts access methods, exposing cost-model blind spots and guiding targeted tuning. The tool is available in pure-WASM mode via PGlite for in-browser use or optionally as a REST-backed server proxy.

[### Airport Extension for DuckDB (GitHub Repo)](https://github.com/Query-farm/duckdb-airport-extension?utm_source=tldrdata)

The Airport extension brings high-performance Arrow Flight support to DuckDB, enabling data engineers to query, insert, update, and delete tables over networked Flight servers directly from DuckDB with zero C++ boilerplate. A companion Python test server (query-farm-airport-test-server) provides an in-memory Arrow Flight endpoint (no persistent state and any auth token accepted), making CI integration and automated testing seamless.

[### Amoro (GitHub Repo)](https://github.com/apache/amoro?utm_source=tldrdata)

Amoro is an incubating Lakehouse management system that delivers a self-optimizing, lake-native data warehouse experience on open table formats like Iceberg and Paimon by unifying metadata and automation across compute engines (Flink, Spark, and Trino) through pluggable services (e.g., AMS, optimizers, and log stores) and rich catalog integrations.

üéÅ

### Miscellaneous

[### How to Cut Data Pipeline Costs by 75% with Kubernetes Spot Instances (10 minute read)](https://www.prefect.io/blog/how-to-cut-data-pipeline-costs-by-75-with-kubernetes-spot-instances?utm_source=tldrdata)

Prefect enables data teams to cut infrastructure costs by up to 75% by running stateful workloads on Kubernetes spot instances without compromising reliability. It does this through smart task caching, infrastructure-aware orchestration, and stateless workers that can recover gracefully from interruptions, making it ideal for dynamic and large-scale data pipelines.

[### A Short History of AutoML: Part Two (7 minute read)](https://thomaswdinsmore.substack.com/p/a-short-history-of-automl-part-two?utm_source=tldrdata)

In the late '90s, predictive modeling tools were manual, fragmented, and SAS-dominated, with early attempts at automation (like Clementine and SAS Enterprise Miner) offering drag-and-drop interfaces but no real AutoML. Vendors made bold claims about replacing analysts, but true automation was minimal, and domain expertise remained essential.

‚ö°Ô∏è

### Quick Links

[### Amazon Redshift Python User Defined Functions Will Reach End of Support After June 30, 2026 (7 minute read)](https://aws.amazon.com/blogs/big-data/amazon-redshift-python-user-defined-functions-will-reach-end-of-support-after-june-30-2026/?utm_source=tldrdata)

You have one year left to refactor your Redshift Python UDFs to Lambda UDFs for better security and scalability.

[### Prescriptive Modeling Makes Causal Bets ‚Äì Whether You Know It or Not! (4 minute read)](https://towardsdatascience.com/prescriptive-modeling-makes-causal-bets-whether-you-know-it-or-not/?utm_source=tldrdata)

Prescriptive modeling inherently relies on causal assumptions, meaning any recommendations or optimizations act as if interventions truly cause the predicted outcomes - ignoring this risks generating misleading or even harmful decisions.

## Curated deep dives, tools and trends in big data, data science and data engineering üìä

Subscribe

Join 308,008 readers for [one daily email](/api/latest/data)

[Privacy](/privacy)[Careers](https://jobs.ashbyhq.com/tldr.tech)[Advertise](/data/advertise)

Timestamp: 1751588867