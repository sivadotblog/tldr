MetricFlow now Open Source üß©, Postgres is not Kafka üöß, Intelligent Flink scaling ‚öôÔ∏è

[TLDR](/)

[Newsletters](/newsletters)

[Advertise](https://advertise.tldr.tech/)

[TLDR](/)

# TLDR Data 2025-11-03

## MetricFlow now Open Source üß©, Postgres is not Kafka üöß, Intelligent Flink scaling ‚öôÔ∏è

üì±

### Deep Dives

[### From Outages to Order: Netflix's Approach to Database Resilience with WAL (2 minute read)](https://www.infoq.com/news/2025/10/netflix-wal-resilience/?utm_source=tldrdata)

Netflix deployed a modular Write-Ahead Log (WAL) system to address data loss, replication entropy, and cross-region consistency, capturing all database mutations in a durable log for robust recoverability. The pluggable design separates producers and consumers, leverages SQS/Kafka with dead-letter queues for reliability, and enables flexible routing to multiple storage backends without code changes. It is capable of handling millions of writes per second with sub-second tail latencies.

[### From JSON to AVRO in the CDC Pipeline (8 minute read)](https://medium.com/fresha-data-engineering/from-json-to-avro-in-the-cdc-pipeline-ff24ac9c9abc?utm_source=tldrdata)

Fresha upgraded its Change Data Capture (CDC) pipeline from JSON to AVRO format to address scalability issues as the business grew. Originally using Postgres, Debezium, Kafka with JSON, Snowpipe Streaming, and Snowflake, the pipeline was simple but inefficient for handling increased schema changes, storage, and query performance. The transition to AVRO, enabled by Snowflake's schema evolution support, introduced automated schema management via a Schema Registry, reducing manual work and improving efficiency.

[### Machine-Learning Predictive Autoscaling for Flink (8 minute read)](https://engineering.grab.com/ml-predictive-autoscaling-for-flink?utm_source=tldrdata)

Grab increased Flink usage 2.5x in a year, driving the need for efficient, self-service stream processing. Traditional reactive autoscaling proved inefficient due to restart spikes, parallelism constraints, and manual tuning overhead, often causing resource waste and latency spikes. They prototyped a predictive autoscaler leveraging time-series forecasting and regression models to dynamically align CPU provisioning with real-time Kafka workload patterns, achieving over 35% average cloud cost savings and simplifying deployment by automating scaling decisions.

üöÄ

### Opinions & Advice

[### "You Don't Need Kafka, Just Use Postgres" Considered Harmful (16 minute read)](https://www.morling.dev/blog/you-dont-need-kafka-just-use-postgres-considered-harmful/?utm_source=tldrdata)

Many ‚Äújust use Postgres‚Äù arguments ignore that Kafka and Postgres solve fundamentally different problems. Kafka provides persistent ordered logs, consumer groups, low latency streaming, fault tolerance, and a massive connector ecosystem, which are difficult and costly to recreate on top of Postgres. Postgres can handle simple queuing at small scale, but using it as an event streaming platform eventually leads to complexity and performance issues, so the right approach is to let Postgres manage state and Kafka handle event streaming.

[### Why You'll Never Have a FAANG Data Infrastructure and That's the Point (9 minute read)](https://moderndata101.substack.com/p/why-youll-never-have-a-faang-infrastructure?utm_source=tldrdata)

Enterprises seeking FAANG-level data capabilities often try to replicate decades-old, custom-built infrastructure. Instead, they should emulate the core design philosophies: abstraction, automation, and accountability. A hybrid ‚Äúbuy + build‚Äù approach using managed/SaaS platforms layered with organizational domain models and robust data governance, organizations can achieve 90% the outcomes (scalable ingestion, analytics, ML, and self-service) without FAANG-scale budgets or engineering headcount. The key is treating data artefacts as products, enforcing standards, and leveraging internal developer platforms to maximize visibility, reliability, and business alignment.

[### Simple, Battle-Tested Algorithms Still Outperform AI (5 minute read)](https://hackernoon.com/simple-battle-tested-algorithms-still-outperform-ai?utm_source=tldrdata)

Companies are losing over $200 billion annually (experiencing negative 45% ROI) by deploying AI systems instead of proven, simpler algorithms that routinely deliver off the charts ROI. Executive fascination with AI and vendor pressures drive costly, inefficient implementations, while well-established methods like EOQ and Little's Law, managed by skilled programmers, remain dramatically more effective for most operational decisions.

[### Ladder of Evidence in Understanding Effectiveness of New Products (7 minute read)](https://medium.com/@AnalyticsAtMeta/ladder-of-evidence-in-understanding-effectiveness-of-new-products-part-i-ad8dee70906c?utm_source=tldrdata)

Meta data scientists use a "ladder of evidence" framework to evaluate the effectiveness of new products. Randomized Controlled Trials (RCTs) are used as the gold standard for establishing causality, but causal inference methods are essential alternatives when RCTs are not feasible due to constraints like cost, sample size, or launch requirements.

üíª

### Launches & Tools

[### PyTorch Foundation Welcomes Ray to Deliver a Unified Open Source AI Compute Stack (4 minute read)](https://pytorch.org/blog/pytorch-foundation-welcomes-ray-to-deliver-a-unified-open-source-ai-compute-stack/?utm_source=tldrdata)

The PyTorch Foundation announced at the PyTorch Conference in San Francisco on October 22 that Ray is joining as its newest foundation-hosted project. This move aims to create a unified open-source AI compute stack by integrating Ray with existing projects like PyTorch and vLLM, simplifying AI development and accelerating production workflows.

[### dbt Labs Open Sources MetricFlow: An Independent Schema for Data Interoperability (4 minute read)](https://thenewstack.io/dbt-labs-open-sources-metricflow-an-independent-schema-for-data-interoperability/?utm_source=tldrdata)

dbt Labs has open-sourced MetricFlow (a Semantic Layer SQL generation tool and its JSON-based metadata schema) to drive semantic interoperability across the data ecosystem under the Apache 2.0 license. The universal metadata layer enables common metric definitions and lineage tracing between tools and data platforms. This approach facilitates data product transparency and audit, particularly relevant for agentic and LLM-powered workflows, by standardizing data and metric exchange across platforms.

[### FlinkSketch (GitHub Repo)](https://github.com/ProjectASAP/FlinkSketch?utm_source=tldrdata)

FlinkSketch is a library offering various sketching algorithms compatible with Apache Flink. Developers can integrate these algorithms into their applications using Flink's DataStream API.

üéÅ

### Miscellaneous

[### The Architectural Shift: AI Agents Become Execution Engines While Backends Retreat to Governance (3 minute read)](https://www.infoq.com/news/2025/10/ai-agent-orchestration/?utm_source=tldrdata)

Enterprise architecture is rapidly evolving as AI agents transition into primary execution engines, directly orchestrating workflows and CRUD operations via protocols like Model Context Protocol (MCP), while backends focus on governance and permissions. Gartner forecasts that 40% of enterprise applications will feature autonomous agents by 2026, up from less than 5% today, with agent-driven systems poised to deliver up to $6 trillion in economic value by 2028.

[### Anonymous Credentials: Rate-Limiting Bots and Agents Without Compromising Privacy (21 minute read)](https://blog.cloudflare.com/private-rate-limiting/?utm_source=tldrdata)

The rapid adoption of agentic AI is shifting web traffic from individual users to high-frequency, platform-driven requests, challenging existing rate-limiting and security mechanisms, especially as traditional fingerprinting becomes ineffective and risks unjustly blocking large user cohorts. Cloudflare uses cryptographically robust anonymous credentials (ARC and ACT), enabling fine-grained, privacy-preserving rate limiting and per-user controls with sublinear communication costs and support for multi-use tokens and late origin-binding. These protocols, under IETF standardization, offer actionable primitives (algebraic MACs and zero-knowledge proofs) that seamlessly integrate with modern automation frameworks (e.g., MCP Tools), effectively balancing resource management, security, and user privacy.

‚ö°Ô∏è

### Quick Links

[### Exploring how PostgreSQL 18 Conquered Time with Temporal Constraints (4 minute read)](https://aiven.io/blog/exploring-how-postgresql-18-conquered-time-with-temporal-constraints?utm_source=tldrdata)

Postgres 18 introduces native temporal constraints, leveraging GiST indexes and the WITHOUT OVERLAPS clause to enforce non-overlapping time ranges and PERIOD-based foreign keys for robust temporal referential integrity.

[### Building a Unified Hybrid Cloud with Infrastructure as Code at RBC (3 minute read)](https://www.cncf.io/blog/2025/10/31/building-a-unified-hybrid-cloud-with-infrastructure-as-code-at-rbc/?utm_source=tldrdata)

How a major bank approaches policy-as-code and GitOps to unify thousands of hybrid cloud workloads.

## Curated deep dives, tools and trends in big data, data science and data engineering üìä

Subscribe

Join 308,008 readers for [one daily email](/api/latest/data)

[Privacy](/privacy)[Careers](https://jobs.ashbyhq.com/tldr.tech)[Advertise](/data/advertise)

Timestamp: 1762216035