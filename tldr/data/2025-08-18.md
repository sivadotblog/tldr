Query to Structured Report üß©, Cursor Can‚Äôt Do Data ‚ö†Ô∏è, Kafka Adds Iceberg Topics üßä

[TLDR](/)

[Newsletters](/newsletters)

[Advertise](https://advertise.tldr.tech/)

[TLDR](/)

# Improve your data knowledge in 5 min 2025-08-18

## Query to Structured Report üß©, Cursor Can‚Äôt Do Data ‚ö†Ô∏è, Kafka Adds Iceberg Topics üßä

üì±

### Deep Dives

[### How Rovo Deep Research works (7 minute read)](https://www.atlassian.com/blog/atlassian-engineering/how-rovo-deep-research-works?utm_source=tldrdata)

Rovo Deep Research takes a user's complex query, splits it into targeted sub-questions, and then searches across Atlassian's Teamwork Graph, connected tools, and external sources to draft well-structured report sections in parallel with citations and proofreading. It combines retrieval-augmented generation, multi-path searches, iterative refinement, and multiple LLMs to ensure the final output is both accurate and fast to produce.

[### You Can't UPDATE What You Can't Find: ClickHouse vs PostgreSQL (14 minute read)](https://clickhouse.com/blog/update-performance-clickhouse-vs-postgresql?utm_source=tldrdata)

ClickHouse's enhanced SQL UPDATE functionality achieves comparable performance to PostgreSQL for single-row updates and outperforms it by up to 4,000x in bulk updates, leveraging its columnar architecture and optimized merge processes. While ClickHouse excels in analytics-heavy workloads with infrequent updates, PostgreSQL remains better suited for transactional systems requiring immediate consistency.

[### Using External Indexes, Metadata Stores, Catalogs, and Caches to Accelerate Queries on Apache Parquet (12 minute read)](https://datafusion.apache.org/blog/2025/08/15/external-parquet-indexes?utm_source=tldrdata)

Efficiently combining Apache Parquet's hierarchical data layout with external indexes and metadata caching can dramatically reduce query latency, especially when using the Apache DataFusion toolkit. By leveraging custom external indexes for file- and page-level pruning and in-memory metadata caches, teams can achieve performance within 2x of proprietary formats while retaining Parquet's strong compression, ecosystem interoperability, and ease of integration.

üöÄ

### Opinions & Advice

[### Why Cursor Doesn't Work for Data Teams (5 minute read)](https://thenewaiorder.substack.com/p/why-cursor-doesnt-work-for-data-teams?utm_source=tldrdata)

Cursor and similar AI coding tools fall short for data teams because they lack essential data context, leading to inaccurate outputs and cumbersome workflows. Traditional tools like MCPs offer limited functionality without addressing the unique needs of data engineers, who prioritize data quality and outputs over mere code generation. nao Labs aims to address these shortcomings by integrating seamlessly with data stack tools and focusing on data-centric user interfaces.

[### Bridging Data and Decision-Making: AI's Role in Modern Analytics (71 minute podcast)](https://www.youtube.com/watch?v=ooI71WBgJnU&amp;utm_source=tldrdata)

Lucas Thelosen and Drew Gilson from Gravity discuss Orion, an AI-powered autonomous data analyst and multi-agent system that democratizes data insights for businesses by automating analysis, delivering actionable insights, and integrating into workflows with accuracy and trustworthiness. Automating routine tasks frees analysts for strategic decision-making and stakeholder management.

[### Will AI Replace Data and Analytics Engineers? (3 minute read)](https://sqlpatterns.com/p/will-ai-replace-data-and-analytics?utm_source=tldrdata)

Despite rapid advances in GenAI and code generation tools, core data engineering and analytics roles remain essential, with responsibilities diversifying across analytics, engineering, and software domains. AI tools like Copilot shift the required expertise toward system design, architecture, and the ability to read and verify AI-generated code, rather than manual code writing. Production-grade systems still demand strong human oversight to ensure maintainability and reliability, underpinning the continued criticality of skilled data professionals.

üíª

### Launches & Tools

[### PgHook (GitHub Repo)](https://github.com/PgHookCom/PgHook?utm_source=tldrdata)

PgHook is a tool that listens to PostgreSQL logical replication events and sends changed row data as JSON to specified webhooks. Key features include real-time change data capture and integration capabilities, making it highly relevant for data engineers looking to streamline data synchronization and event-driven architectures.

[### Firecrawl (GitHub Repo)](https://github.com/mendableai/firecrawl?utm_source=tldrdata)

Firecrawl is an API-first platform that transforms entire websites into LLM-ready data (clean Markdown, structured JSON, HTML, screenshots, and metadata) without needing sitemaps or manual configuration. It handles complexities like JavaScript rendering, proxies, rate limits, pagination, media parsing, and anti-bot measures. Firecrawl has SDKs in Python, Node.js, and Go, and integrations with tools like LangChain and LlamaIndex. It is licensed under AGPL-3.0 and also available as a managed cloud service.

[### Iceberg Topics for Apache Kafka: Zero ETL, Zero Copy (13 minute read)](https://aiven.io/blog/iceberg-topics-for-apache-kafka-zero-etl-zero-copy?utm_source=tldrdata)

Apache Kafka now supports Iceberg Topics, allowing users to ingest and process data as Apache Iceberg tables without the need for ETL or data copies. This enhancement significantly reduces costs, streamlines data workflows, and eliminates the redundancy of around 60% of Kafka sink connectors, enabling native SQL access for analytics and improving data engineering efficiency.

[### What is SQLMesh and How is it Different from dbt? (5 minute read)](https://www.confessionsofadataguy.com/what-is-sqlmesh-and-how-is-it-different-from-dbt/#google_vignette?utm_source=tldrdata)

SQLMesh, an open-source DataOps framework, addresses complex data transformation workflows by using virtual data environments with views, enabling faster development and eliminating physical schema duplication. Compared to dbt's static, schema-based model, SQLMesh's semantic understanding via Abstract Syntax Trees validates SQL at compile time, tracks dependencies for efficient updates, and leverages Python-native macros, automated data contracts, and a CI/CD bot to enhance developer experience and reduce compute costs.

üéÅ

### Miscellaneous

[### Creating AI Agent Solutions for Warehouse Data Access and Security (10 minute read)](https://engineering.fb.com/2025/08/13/data-infrastructure/agentic-solution-for-warehouse-data-access/?utm_source=tldrdata)

Meta's data warehouse struggles with complex access patterns and manual permissions, exacerbated by generative AI demands, as traditional role-based access control falls short on efficiency and security. The warehouse's hierarchical structure is converted into a text-based format for AI agents, using context and intention management to streamline access for human and AI users.

[### How to Deploy Vertex AI Workbench with Terraform - Without UI Pain (5 minute read)](https://hackernoon.com/how-to-deploy-vertex-ai-workbench-with-terraform-without-ui-pain?utm_source=tldrdata)

Implementing Vertex AI Workbench with a Terraform module and CI/CD pipeline in Google Cloud streamlines Jupyter environment provisioning, enforces consistent security and configuration standards, and drastically reduces manual errors and cloud waste. Automating instance deployment ensures reproducibility, transparent cost attribution, and audit readiness while minimizing the operational overhead for data teams.

‚ö°Ô∏è

### Quick Links

[### Stochastic DuckDB Extension (4 minute read)](https://query.farm/duckdb_extension_stochastic.html?utm_source=tldrdata)

The Stochastic DuckDB Extension enhances DuckDB with a wide range of statistical distribution functions, enabling advanced statistical analysis directly in SQL.

[### Handling Fuzzy Matching of Transactions Cleanly (3 minute read)](https://performancede.substack.com/p/handling-fuzzy-matching-of-transactions?utm_source=tldrdata)

How to use lookup tables and DuckDB's ILIKE operator to handle string matching and categorization statements instead of unwieldy CASE statements.

## Curated deep dives, tools and trends in big data, data science and data engineering üìä

Subscribe

Join 308,008 readers for [one daily email](/api/latest/data)

[Privacy](/privacy)[Careers](https://jobs.ashbyhq.com/tldr.tech)[Advertise](/data/advertise)

Timestamp: 1755563254