Trust Through Lineage üîó, Iceberg Data Loss üí•, Pinterest‚Äôs Kubernetes Edge Case üîç

[TLDR](/)

[Newsletters](/newsletters)

[Advertise](https://advertise.tldr.tech/)

[TLDR](/)

# Improve your data knowledge in 5 min 2025-07-21

## Trust Through Lineage üîó, Iceberg Data Loss üí•, Pinterest‚Äôs Kubernetes Edge Case üîç

üì±

### Deep Dives

[### Debugging the One-in-a-Million Failure: Migrating Pinterest's Search Infrastructure to Kubernetes (9 minute read)](https://medium.com/pinterest-engineering/debugging-the-one-in-a-million-failure-migrating-pinterests-search-infrastructure-to-kubernetes-bef9af9dabf4?utm_source=tldrdata)

Pinterest's migration of its search infrastructure to Kubernetes revealed a rare issue where one in a million search requests took 100 times longer due to an interaction between the memory-intensive search system and a container monitoring process. Disabling cAdvisor's working set size estimation with a one-line change resolved the performance regression.

[### Build a Data Lakehouse with Apache Iceberg, Polaris, Trino, and MinIO (9 minute read)](https://medium.com/@gilles.philippart/build-a-data-lakehouse-with-apache-iceberg-polaris-trino-minio-349c534ecd98?utm_source=tldrdata)

You can build a local, self-managed LakeHouse by pairing MinIO for S3-compatible storage, Apache Iceberg for open table formats, Polaris as the Iceberg REST catalog, and Trino for distributed SQL queries. Running entirely in Docker, this stack requires no cloud accounts or Hadoop dependencies, just open-source components wired together via Polaris's REST interface. Iceberg ensures transactional reliability and schema evolution, Polaris centralizes metadata, Trino delivers sub-second analytics, and MinIO provides cost-free, local object storage.

[### Delta Lake Transaction Logs Explained (7 minute read)](https://medium.com/capital-one-tech/delta-lake-transaction-logs-explained-6b5f036e64e2?utm_source=tldrdata)

Delta Lake's transaction log, stored in the \_delta\_log directory as JSON files, records every operation (insert, update, and delete) on a Delta table, ensuring ACID compliance, schema enforcement, and time travel capabilities. Each transaction log entry details changes like added or removed Parquet files to enable efficient metadata handling for large-scale data lakes.

[### Leveraging Multimodal LLMs for Shopify's Global Catalogue: Recap of Expo Talk at ICLR 2025 (8 minute read)](https://shopifyengineering.myshopify.com/blogs/engineering/leveraging-multimodal-llms?utm_source=tldrdata)

Shopify's Global Catalogue uses multimodal Large Language Models (LLMs) to unify and standardize product data across millions of merchants, processing 40 million daily inferences to enhance search, recommendations, and conversational commerce. By fine-tuning open-source models like LlaVA 1.5 7B, LLaMA 3.2 11B, and Qwen2VL 7B, Shopify achieves high accuracy and cost efficiency while addressing challenges like data fragmentation and multilingual product listings.

üöÄ

### Opinions & Advice

[### Embedding User-Defined Parquet Indexes (6 minute read)](https://datafusion.apache.org/blog/2025/07/14/user-defined-parquet-indexes/?utm_source=tldrdata)

Standard Parquet statistics (min/max, null counts, and Bloom filters) often fail to prune files when predicates target high-cardinality columns. By leveraging Parquet's extensible footer metadata and offset-based addressing, you can embed arbitrary index structures (such as per-file distinct-value lists, HyperLogLog sketches, or precomputed aggregates) directly into the file body without breaking compatibility with existing readers.

[### Data Lineage for MLOps (OpenLineage, DataHub, Metaflow, Airflow) (70 minute video)](https://www.youtube.com/watch?v=OF7AoOC7W6c&amp;utm_source=tldrdata)

Data lineage is evolving from a ‚Äúnice to have‚Äù into a critical component of modern data infrastructure. It is vital for trust, compliance, debugging, and reliable AI/ML workflows. Data outages often stem from silent data quality issues, not model drift. Lineage is increasingly essential for both operational resilience and regulatory pressure.

[### Iceberg Table Corruption and Data Loss in the Wild: Part 1 (9 minute read)](https://www.ryft.io/blog/iceberg-table-corruption-and-data-loss-in-the-wild-part-1?utm_source=tldrdata)

A bug in Apache Iceberg caused silent data loss and table corruption by allowing data files to be overwritten during ingestion without indication. Critical exceptions arose when querying, revealing issues with the expected Parquet file signature and invalid S3 file ranges, highlighting the importance of continuous monitoring for data integrity in streaming pipelines.

üíª

### Launches & Tools

[### Cocoindex (GitHub Repo)](https://github.com/cocoindex-io/cocoindex?utm_source=tldrdata)

CocoIndex introduces a pure data flow programming model where data is immutable, transformations are fully traceable, and side effects are eliminated. Unlike in traditional workflow orchestrators that obscure data and increase pipeline complexity, data is the primary unit of composition, enabling transparent auditing, simplified debugging, and automatic downstream updates on source changes.

[### DuckDB GSheets (GitHub Repo)](https://github.com/evidence-dev/duckdb_gsheets?utm_source=tldrdata)

This GitHub repository offers a DuckDB extension that enables data engineers to read from and write to Google Sheets using SQL, streamlining data manipulation and integration workflows. Key features include seamless SQL access to Google Sheets, facilitating data extraction and analysis directly within DuckDB. This tool is particularly relevant for data engineers seeking to enhance their ETL processes by leveraging spreadsheet data efficiently.

[### Flink HTTP Full Cache Connector (6 minute read)](https://datanutshell.com/posts/flink_http_full_cache_connector?utm_source=tldrdata)

The Flink HTTP Full Cache Connector preloads infrequently changing reference data from external HTTP endpoints into Flink state at startup, then serves asynchronous lookup joins with sub-millisecond latency in both Table/SQL and DataStream APIs. By caching entire datasets up front and refreshing only on configuration changes, it eliminates per-event HTTP calls, reducing external dependencies and ensuring consistent, low-latency enrichments for streaming applications.

üéÅ

### Miscellaneous

[### From Linear Regression to XGBoost: A Side-by-Side Performance Comparison (6 minute read)](https://machinelearningmastery.com/from-linear-regression-to-xgboost-a-side-by-side-performance-comparison/?utm_source=tldrdata)

Linear regression provides a simple, interpretable baseline for regression tasks but struggles with non-linear patterns, while XGBoost, using decision tree ensembles, significantly improves prediction accuracy by modeling complex relationships. On the housing dataset, XGBoost reduced RMSE by 30% and increased R¬≤ from 0.64 to 0.83 compared to linear regression.

[### Using Generative AI to Accelerate Data Product Development (6 minute read)](https://medium.com/quantumblack/using-generative-ai-to-accelerate-data-product-development-9816e2f3ad2c?utm_source=tldrdata)

AI for Data Products (AI4DP) by QuantumBlack leverages generative AI to accelerate and automate the creation of high-quality, reusable data products, reducing engineering time by up to 80%. It eliminates delays and ensures rigorous governance by grounding outputs in source schemas and automating critical tasks like schema design, pipeline construction, and quality enforcement.

‚ö°Ô∏è

### Quick Links

[### IBM Tackles Shadow AI: An Enterprise Blind Spot (6 minute read)](https://thenewstack.io/ibm-tackles-shadow-ai-an-enterprise-blind-spot/?utm_source=tldrdata)

IBM has launched unified agentic governance and security tools.

[### Caching (10 minute read)](https://planetscale.com/blog/caching?utm_source=tldrdata)

A guided tour of caching techniques, explained with animations.

## Curated deep dives, tools and trends in big data, data science and data engineering üìä

Subscribe

Join 308,008 readers for [one daily email](/api/latest/data)

[Privacy](/privacy)[Careers](https://jobs.ashbyhq.com/tldr.tech)[Advertise](/data/advertise)

Timestamp: 1753144169