Apache Parquet Security Vulnerability ‚ö†Ô∏è, Zillow‚Äôs Graph-Powered Search üè†, Devin 2.0 Launched üöÄ

[TLDR](/)

[Newsletters](/newsletters)

[Advertise](https://advertise.tldr.tech/)

[TLDR](/)

# TLDR Data 2025-04-10

## Apache Parquet Security Vulnerability ‚ö†Ô∏è, Zillow‚Äôs Graph-Powered Search üè†, Devin 2.0 Launched üöÄ

üì±

### Deep Dives

[### Revenue Attribution Report: how we used homomorphic encryption to enhance privacy and cut network congestion by 99% (7 minute read)](https://www.linkedin.com/blog/engineering/data/how-we-used-homomorphic-encryption-to-enhance-privacy-and-cut-network-congestion?utm_source=tldrdata)

Homomorphic encryption allows LinkedIn to sum encrypted user data‚Äîlike job search stats on the cloud without decrypting it, slashing network load by 99%. The ASHE scheme enables summing encrypted data by computing new ciphertexts from existing ones without the secret key, outperforming slower public-key systems. It ties encryption to a tag, requiring this identifier for summation or decryption, enhancing efficiency and security.

[### PostgreSQL Index-Only Scan and Covering Index ‚Äî Backend Devs Must Know (5 minute read)](https://levelup.gitconnected.com/postgresql-index-only-scan-and-covering-index-backend-devs-must-know-0b5804ad10b4?utm_source=tldrdata)

PostgreSQL's index-only scans improve performance by retrieving data directly from indexes, avoiding access to the table heap. To take advantage of this, you can either create a multi-column index that includes all the required fields or use the INCLUDE clause to add non-searchable fields needed in the results. While multi-column indexes are sorted using a B-Tree, indexes with INCLUDE require scanning and filtering results to extract the necessary data.

[### Leveraging Knowledge Graphs in Real Estate Search (8 minute read)](https://www.zillow.com/tech/leveraging-knowledge-graphs-in-real-estate-search/?utm_source=tldrdata)

How do you use knowledge graphs to organize messy real estate data‚Äîlike listings, neighborhoods, and schools‚Äîinto a structured, searchable network? Zillow shares key steps including knowledge extraction, defining ontologies, normalization, and resolving entity ambiguities to build meaningful connections. However, the process doesn't stop once the graph is built‚Äîongoing updates and versioning are essential to keep it accurate and useful.

üöÄ

### Opinions & Advice

[### The Rise of the Open Lakehouse made Databricks. It could bring it down (8 minute read)](https://www.getorchestra.io/blog/the-rise-of-the-open-lakehouse-made-databricks-it-could-bring-it-down?utm_source=tldrdata)

The open lakehouse model is pushing data engineers to consider alternative compute engines‚Äîlike DuckDB for lightweight analytics, Ray for scalable AI/ML workloads, and Apache DataFusion for high-performance SQL queries‚Äîover traditional platforms like Databricks. This highlights the evolving balance between platform integration and best-in-class features, forcing organizations to increasingly rely on hybrid architectures to optimize cost, performance, and operational simplicity.

[### How AI Will Disrupt BI As We Know It (11 minute read)](https://roundup.getdbt.com/p/how-ai-will-disrupt-bi-as-we-know?utm_source=tldrdata)

AI is poised to transform business intelligence, particularly enhancing exploratory data analysis (EDA), traditionally the domain of BI tools. With advanced setups, like a dbt+context protocol+Claude combo, EDA becomes vastly faster, more efficient, and fundamentally challenging the current BI workflows. This shift empowers data practitioners by offloading code-intensive tasks, allowing them to concentrate on analytical problem-solving. As AI-driven interfaces outpace traditional BI tools in EDA, BI's role may evolve into governance and presentation, presenting a significant headwind for the current BI tools and workflows vendors and tremendous opportunities for innovation.

[### From Data Tyranny to Data Democracy (9 minute read)](https://moderndata101.substack.com/p/from-data-tyranny-to-data-democratization?utm_source=tldrdata)

Successful data management decentralization strategies hinge on establishing appropriate governance levels for each data product. Implementing a risk-based model enables organizations to tailor governance mechanisms to the specific sensitivity and criticality of each data asset, ensuring both security and accessibility. Designating Data Product Owners is crucial in this framework, as they oversee the quality, compliance, and lifecycle of their respective data products. This approach fosters agile and scalable data democratization, transforming rigid data control structures into more flexible, user-empowered environments.

üíª

### Launches & Tools

[### Devin 2.0 (2 minute read)](https://cognition.ai/blog/devin-2?utm_source=tldrdata)

Cognition AI's Devin 2.0 is an upgraded, agent-native IDE that boosts collaboration between developers and an AI engineer, Devin. It supports running multiple Devins in parallel, each with its own interactive, cloud-based workspace. New features include Interactive Planning for task breakdowns, Devin Search for codebase queries with cited answers, and Devin Wiki for auto-generated documentation.

[### dbt in Action #3: Analyses, Materialisations, and Incremental Models (6 minute read)](https://substack.com/home/post/p-160769018?utm_source=tldrdata)

dbt's analyses enable materializations (views, tables, incremental) and incremental models to handle only new rows. Settings like a reliable updated\_at field and unique\_key optimize updates, perfect for large datasets where full refreshes would be inefficient. Incremental models help cut compute costs, but could introduce risks of errors from flawed keys or filters.

[### How to run dlt with Airflow (Or any other Python thing) (10 minute read)](https://dlthub.com/blog/dlt-with-airflow?utm_source=tldrdata)

An overview of three primary methods for running dlt pipelines in Airflow using PythonOperator, PythonVirtualenvOperator, and KubernetesPodOperator. While each has its benefits, the KubernetesPodOperator best decouples scheduling from execution to mitigate module and resource conflicts. External compute services can serve as alternatives when Kubernetes isn't an option. Each approach involves trade-offs in complexity, resource management, and team skills.

üéÅ

### Miscellaneous

[### Max severity RCE flaw discovered in widely used Apache Parquet (3 minute read)](https://www.bleepingcomputer.com/news/security/max-severity-rce-flaw-discovered-in-widely-used-apache-parquet/?utm_source=tldrdata)

A critical security flaw in Apache Parquet versions up to 1.15.0 allows attackers to execute arbitrary code by tricking systems into processing malicious Parquet files, putting big data pipelines at severe risk. Immediate upgrade to Parquet 1.15.1 is recommended, along with enhanced safeguards when handling untrusted files.

[### The Evolution of Modern RAG Architectures (10 minute read)](https://www.newsletter.swirlai.com/p/the-evolution-of-modern-rag-architectures?utm_source=tldrdata)

Modern RAG architectures have evolved from simple retrieval systems that mitigate LLM hallucinations using basic text chunking and vector searches to advanced setups incorporating query rewriting, contextual retrieval, and even agentic approaches that dynamically route data sources and reflect on output. These innovations aim to improve accuracy and efficiency by fusing retrieval with generation, helping data teams address inherent challenges of LLM limitations in real-world applications.

‚ö°Ô∏è

### Quick Links

[### 5 Reasons Why Traditional Machine Learning is Alive and Well in the Age of LLMs (2 minute read)](https://machinelearningmastery.com/5-reasons-why-traditional-machine-learning-is-alive-and-well-in-the-age-of-llms/?utm_source=tldrdata)

Traditional ML still shines over LLMs with its edge in structured data tasks (think fraud detection with XGBoost), lower compute costs, and easier explainability.

[### Kafka Consumer Lag (5 minute read)](https://dattell.com/data-architecture-blog/kafka-consumer-lag-explained/?utm_source=tldrdata)

Kafka consumer lag is the gap between the newest message offset and the last processed offset - it indicates how far a consumer is behind.

## Curated deep dives, tools and trends in big data, data science and data engineering üìä

Subscribe

Join 308,008 readers for [one daily email](/api/latest/data)

[Privacy](/privacy)[Careers](https://jobs.ashbyhq.com/tldr.tech)[Advertise](/data/advertise)

Timestamp: 1744589391