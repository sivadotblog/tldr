Mining Datasets from LLMs ‚õèÔ∏è, Data is Political üèõÔ∏è, Juniors Left Behind by AI üë®‚Äçüíª

[TLDR](/)

[Newsletters](/newsletters)

[Advertise](https://advertise.tldr.tech/)

[TLDR](/)

# Improve your data knowledge in 5 min 2025-09-22

## Mining Datasets from LLMs ‚õèÔ∏è, Data is Political üèõÔ∏è, Juniors Left Behind by AI üë®‚Äçüíª

üì±

### Deep Dives

[### Building an Anomaly Detection Platform at DoorDash to Catch Fraud Trends Early (10 minute read)](https://careersatdoordash.com/blog/doordash-anomaly-detection-platform-to-catch-fraud-trends/?utm_source=tldrdata)

DoorDash built a large-scale anomaly detection platform to surface subtle fraud trends early by scanning hundreds of millions of metric‚Äìdimension segments daily. Using DuckDB for fast aggregation, a z-score time-series detector, and hierarchical clustering, the system reduces thousands of anomalies into manageable clusters for investigation. This cut fraud detection time from 100+ days to under 3, saving tens of millions annually and showing how data engineering enables proactive fraud defense.

[### Data Lakehouse: Infrastructure (5 minute read)](https://medium.com/fresha-data-engineering/data-lakehouse-infrastructure-218d1c0776aa?utm_source=tldrdata)

Fresha engineered a scalable Data Lakehouse that unified both real-time streaming and batch analytics and enabled clear workload separation and cost attribution. Deployed on AWS, the platform leverages EKS, Apache Paimon, StarRocks, and Apache Flink. The multi-account cloud setup uses cross-account VPC peering and IAM roles for secure, seamless access to secrets, while a single internal ALB optimizes cost and operational complexity. Terraform and ArgoCD automate infrastructure provisioning and GitOps deployments, enabling rapid experimentation and enterprise-grade security.

[### The Kaggle Grandmasters Playbook: 7 Battle-Tested Modeling Techniques for Tabular Data (12 minute read)](https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/?utm_source=tldrdata)

Kaggle Grandmasters reveal a proven playbook: iterate fast with smart EDA and model training, use k-fold/group CV tailored to data, and build diverse baselines early. Key modeling techniques include extensive feature engineering, model stacking & hill climbing ensembles, pseudo-labeling with soft predictions, and additional final model training (seed averaging or full-data retraining). The article contains links to detailed writeups and prior Kaggle competitions that illustrate these techniques.

[### Viaduct, Five Years on: Modernizing the Data-Oriented Service Mesh (9 minute read)](https://medium.com/airbnb-engineering/viaduct-five-years-on-modernizing-the-data-oriented-service-mesh-e66397c9e9a9?utm_source=tldrdata)

Viaduct is a unified, decentralized GraphQL schema connecting all domains, serving as a "one-stop" internal mesh (75%+ of requests are intra-Airbnb), allowing teams to deploy logic serverlessly in Viaduct, bypassing thin GraphQL layers over microservices to reduce ops burden. A year-long "Viaduct Modern" overhaul addressed previous pitfalls like fragmented APIs and weak boundaries via a streamlined developer API and modular engine.

üöÄ

### Opinions & Advice

[### Data is Political (3 minute read)](https://practicaldatamodeling.substack.com/p/data-is-political?utm_source=tldrdata)

Data modeling is inherently political, with negotiations, departmental turf wars, and conflicting definitions shaping both the technical process and organizational trust. Success requires data professionals to embrace three roles: technical practitioner, persuasive salesperson, and empathetic servant, mediating conflict and translating value. High-trust, psychologically safe environments foster effective data models and alignment, while dysfunctional organizations risk eroding trust and amplifying poor decision-making.

[### Full vs Incremental Data Loads Explained (9 minute video)](https://www.confessionsofadataguy.com/full-vs-incremental-data-loads-explained/?utm_source=tldrdata)

Full data loads excel in simplicity and reliability for small datasets or high-integrity needs, but become inefficient for large-scale systems due to high resource use. Incremental loads offer efficiency and scalability for bigger data volumes, though they demand strong change-tracking to prevent errors, with hybrid methods often blending both for optimal balance based on size, frequency, resources, and integrity requirements.

üíª

### Launches & Tools

[### Data privacy as a competitive advantage (Sponsor)](https://www.onetrust.com/resources/tldr-csyn/the-strategic-guide-to-consent-and-preferences-ebook/?utm_source=tldrdata)

Honda recently discovered that failing to honor TCPA rules can cost $632,500 ‚Äì Ouch‚Ä¶ But smart data teams are discovering something else: [properly configured consent infrastructure](https://www.onetrust.com/resources/tldr-csyn/the-strategic-guide-to-consent-and-preferences-ebook/) vastly improves data quality. Customers who understand your value proposition share better data, leading to more accurate analytics and segmentation. Ready for no risk, all reward? [Get the eBook from OneTrust](https://www.onetrust.com/resources/tldr-csyn/the-strategic-guide-to-consent-and-preferences-ebook/) üìò

[### LLM-Deflate: Extracting LLMs Into Datasets (9 minute read)](https://www.scalarlm.com/blog/llm-deflate-extracting-llms-into-datasets/?utm_source=tldrdata)

LLM-Deflate introduces a method for extracting structured datasets from large language models (LLMs) by reversing their data compression process. This technique has successfully generated substantial training datasets from popular models, showcasing the potential for systematic knowledge extraction. The approach can enhance synthetic data generation, as evidenced by advancements from Stanford and NVIDIA, emphasizing the importance of infrastructure and quality control in scaling data generation effectively.

[### The Case for an Iceberg-Native Database: Why Spark Jobs and Zero-Copy Kafka Won't Cut It (15 minute read)](https://www.warpstream.com/blog/the-case-for-an-iceberg-native-database-why-spark-jobs-and-zero-copy-kafka-wont-cut-it?utm_source=tldrdata)

WarpStream's TableFlow is an Iceberg-native database that ingests Kafka topics directly into Iceberg tables (‚ÄúKafka in, Iceberg out‚Äù), akin to Confluent's Tableflow. It addresses the shortcomings of Spark-based ingestion, such as expensive batch jobs, high write amplification, small-file and snapshot bloat, and schema/concurrency conflicts, while sidestepping the operational fragility of zero-copy Kafka tiers. TableFlow handles ingestion, compaction, and upserts natively, delivering lower latency and more reliable pipelines for Iceberg-based analytics.

[### Honest Review of Polars Cloud (9 minute read)](https://dataengineeringcentral.substack.com/p/honest-review-of-polars-cloud?utm_source=tldrdata)

Polars Cloud, an SaaS implementation of the high-performance Rust-based Polars engine, aims to undercut Databricks, Snowflake, and AWS Glue with significantly lower costs ($0.0005 per vCPU hour) and faster processing. Initial hands-on experience highlighted onboarding friction, unintuitive UI, sparse documentation, and opaque error handling, especially for production-scale orchestration and distributed workloads. While Polars' core engine remains powerful, the cloud service's lackluster developer experience and limited visibility into job execution may present significant hurdles for data engineering teams in the near term.

üéÅ

### Miscellaneous

[### AI Was Supposed to Help Juniors Shine. Why Does It Mostly Make Seniors Stronger? (4 minute read)](https://elma.dev/notes/ai-makes-seniors-stronger/?utm_source=tldrdata)

AI currently enhances the productivity of senior developers more than juniors, as it excels in automating repetitive tasks and fast prototyping, but struggles with code quality, architecture, and security. The expectation that AI would empower juniors has proven unrealistic, highlighting the need for experienced engineers to guide and interpret AI outputs to avoid potential pitfalls in software development.

[### A Case for Computing on Unstructured Data (25 minute read)](https://arxiv.org/pdf/2509.14601?utm_source=tldrdata)

Most of the world's data is unstructured, yet today's systems only query it after forced structuring. Unstructured data deserves first-class computational models. Extract‚ÄìTransform‚ÄìProject (XTP) pipelines convert unstructured inputs into structured forms, compute, then project back into the original format. MXFlow is a neurosymbolic dataflow system that mixes LLMs with deterministic operators. It can handle complex multimodal queries with provenance and cleaning.

[### A Gentle Introduction to vLLM for Serving (6 minute read)](https://www.kdnuggets.com/a-gentle-introduction-to-vllm-for-serving?utm_source=tldrdata)

vLLM is an open-source engine for serving large language models that boosts throughput, lowers latency, and reduces memory waste. It does this via innovations like PagedAttention, which virtualizes key/value cache memory into pages and dynamically reuses them, especially helpful for handling long input sequences and many users at once.

‚ö°Ô∏è

### Quick Links

[### 2025 PostGIS & GEOS Release (11 minute read)](https://www.crunchydata.com/blog/2025-postgis-and-geos-release?utm_source=tldrdata)

PostGIS 3.6 and GEOS 3.14 introduce automated polygonal coverage cleaning via the new ST\_CoverageClean function, eliminating gaps and overlaps in spatial datasets directly within PostgreSQL.

[### UUIDv7 Comes to PostgreSQL 18 (8 minute read)](https://www.thenile.dev/blog/uuidv7?utm_source=tldrdata)

PostgreSQL 18 adds UUIDv7, a timestamp-based variant that improves btree indexing, sorting, and index locality.

## Curated deep dives, tools and trends in big data, data science and data engineering üìä

Subscribe

Join 308,008 readers for [one daily email](/api/latest/data)

[Privacy](/privacy)[Careers](https://jobs.ashbyhq.com/tldr.tech)[Advertise](/data/advertise)

Timestamp: 1758587144