How S3 is Built ‚òÅÔ∏è, Fixing Vendor SQL ü¶Ü, Notebooks Without Setup üêç

[TLDR](/)

[Newsletters](/newsletters)

[Advertise](https://advertise.tldr.tech/)

[TLDR](/)

# TLDR Data 2026-01-26

## How S3 is Built ‚òÅÔ∏è, Fixing Vendor SQL ü¶Ü, Notebooks Without Setup üêç

üì±

### Deep Dives

[### How AWS S3 is built (78 minute podcast)](https://newsletter.pragmaticengineer.com/p/how-aws-s3-is-built?utm_source=tldrdata)

Amazon S3 processes hundreds of millions of transactions per second, manages over 500 trillion objects, and operates across hundreds of exabytes with 11 nines of durability, achieved through auditor microservices and automated repair systems. Recent architectural advances include a near-total Rust rewrite for core pathways, rigorous formal methods for correctness, and rollout of new primitives like S3 Vectors supporting 20 trillion vectors per bucket with sub-100ms queries. S3's design emphasizes simplicity at scale, crash consistency, proactive defense against correlated failures, and engineering practices where increased scale enhances reliability and performance.

[### High-Risk, High-Scale: Guaranteeing Ad Budget Precision at 1 Million Events/Second (5 minute read)](https://blog.flipkart.tech/high-risk-high-scale-guaranteeing-ad-budget-precision-at-1-million-events-second-cc23977796d7?utm_source=tldrdata)

Flipkart Ads processes over 1 million events per second with a horizontally scalable, stateful stream-processing architecture that prioritizes rapid spend enforcement while ensuring strict budget control. The system employs Flink-managed state for distributed deduplication and key-based idempotency, watermarking to manage temporal skew due to delayed mobile events, and a Lambda architecture that separates sub-second enforcement from batch reconciliation for financial accuracy.

[### How I Reduced AI Token Costs by 91% with Semantic Tool Selection and Redis (11 minute read)](https://mlops.community/how-i-reduced-ai-token-costs-by-91-with-semantic-tool-selection-and-redis/?utm_source=tldrdata)

An enterprise AI platform eliminated the need to include all 70+ tool definitions in every LLM prompt by using multi-component embeddings stored in Redis and selecting only the 2‚Äì5 most relevant tools per query. This reduced input tokens by 91.5%, cut total costs by ~49%, boosted precision@3 to 95%, and improved response times by 31%.

üöÄ

### Opinions & Advice

[### The Desperate Need For An ‚ÄúAgent Contract‚Äù (6 minute read)](https://www.montecarlodata.com/ai-agent-contract/?utm_source=tldrdata)

The AI Agent Contract is a framework that establishes explicit, enforceable agreements between data producers, engineers, and AI agents to ensure reliable inputs, consistent definitions, and controlled changes, preventing common failures from schema drifts, tool breakage, or mismatched expectations.

[### MCP is Not the Problem, It's your Server: Best Practices for Building MCP Servers (10 minute read)](https://www.philschmid.de/mcp-best-practices?utm_source=tldrdata)

Many MCP servers fail not because of the protocol itself but due to poor design, treating MCP as mere REST API wrappers instead of thoughtful user interfaces for AI agents. To succeed, focus on high-level outcome-oriented tools, ruthlessly curate tools, use clear prefixed names for discoverability, and paginate large results to prevent context overload.

[### Path forward for Data Governance: Existence Over Essence (8 minute read)](https://moderndata101.substack.com/p/path-forward-for-data-governance?utm_source=tldrdata)

Traditional compliance and documentation models fail as automation and autonomous agents scale, making embedded, computable governance essential for observable, auditable, and context-sensitive oversight. Data governance must evolve from rigid, essence-based frameworks to a dynamic, socio-technical discipline that enables real-time negotiation of meaning, strategic direction, and continuous accountability within distributed, AI-driven environments.

üíª

### Launches & Tools

[### Your pipelines work great for 3 data sources. Will they work with 30? (Sponsor)](https://go.fivetran.com/signups/smb?utm_source=TLDR%20data&amp;utm_medium=sponsored-ad&amp;utm_campaign=SC-PLG-brand%20Q4&amp;utm_content=46048)

[Fivetran](https://go.fivetran.com/signups/smb?utm_source=TLDR%20data&utm_medium=sponsored-ad&utm_campaign=SC-PLG-brand%20Q4&utm_content=46048) helps you manage your data stack, no matter how much it grows. With 700+ prebuilt connectors, you can set up fully managed, self-healing pipelines in just a few steps. Fivetran handles: schema drift and pipeline maintenance. You handle: strategy, architecture, and AI. [Try it free today ‚Äî no credit card needed](https://go.fivetran.com/signups/smb?utm_source=TLDR%20data&utm_medium=sponsored-ad&utm_campaign=SC-PLG-brand%20Q4&utm_content=46048).

[### pynb (Tool)](https://pynb.app/?utm_source=tldrdata)

pynb is a macOS tool for running Python notebooks without kernels or environment setup, using plain .py files that are git friendly and scalable to large datasets. It supports SQL alongside Python, works with your existing ChatGPT subscription and agents, and keeps all data local.

[### Benchmarking 1B Vectors with Low Latency and High Throughput (5 minute read)](https://hackernoon.com/benchmarking-1b-vectors-with-low-latency-and-high-throughput?utm_source=tldrdata)

ScyllaDB Vector Search achieves industry-leading p99 latencies as low as 1.7 ms and throughput up to 252,000 QPS on billion-scale datasets, demonstrated using the yandex-deep\_1b benchmark with 96-dimensional vectors. The architecture co-locates structured and vector data, supporting hybrid queries. Now generally available, upcoming enhancements include native filtering, quantization, and optimized hybrid retrieval.

[### Sirius (GitHub Repo)](https://github.com/sirius-db/sirius?utm_source=tldrdata)

Sirius is a GPU-native SQL engine that plugs into existing query engines (currently DuckDB, with Doris ‚Äúcoming soon‚Äù) via Substrait, so teams can offload execution to GPUs without rewriting SQL or rebuilding the whole platform. Built on NVIDIA CUDA-X, it reports ~10x TPC-H (on SF=100) speedup at similar on-demand cost. Today, it accelerates a limited operator/type set (joins, group-bys, etc.; common scalar types) and falls back to CPU for unsupported features.

[### Feast Joins the PyTorch Ecosystem: Bridging Feature Stores and Deep Learning (5 minute read)](https://pytorch.org/blog/feast-joins-the-pytorch-ecosystem/?utm_source=tldrdata)

Feast, an open-source feature store, has joined the PyTorch Ecosystem, taming the training-serving skew by ensuring models receive identical feature transformations in both development and production. With unified APIs, point-in-time joins, support for Spark/Snowflake/Flink, OpenTelemetry observability, and RBAC-based governance, Feast empowers teams to maintain data consistency and lineage for large-scale AI deployments.

üéÅ

### Miscellaneous

[### The Joy of SQL - If Properly Implemented (18 minute read)](https://blobs.duckdb.org/data-day-texas-2026-joy-of-sql-slides.txt?utm_source=tldrdata)

SQL's frustrations mostly come from poor vendor implementations rather than the language itself. DuckDB shows how better defaults, simpler syntax, and improved tooling can make SQL feel enjoyable and productive.

[### Optimizing Data Transfer in Distributed AI/ML Training Workloads (15 minute read)](https://towardsdatascience.com/optimizing-data-transfer-in-distributed-ai-ml-training-workloads/?utm_source=tldrdata)

Profiling GPU-to-GPU data transfer in distributed training reveals stark differences in performance between PCIe and NVLink interconnects. NVLink impacts throughput by only 8% versus over 6x slowdown for PCIe. Techniques like gradient compression, optimized memory usage, and parallelized reduction drive substantial performance and cost gains, illustrating the value of regular profiling with NVIDIA Nsight‚Ñ¢ Systems for all AI/ML teams.

‚ö°Ô∏è

### Quick Links

[### Anthropic Economic Index report: economic primitives (43 minute read)](https://www.anthropic.com/research/anthropic-economic-index-january-2026-report?utm_source=tldrdata)

Anthropic's latest Economic Index reveals that Claude AI usage is increasingly concentrated in high-value software, administrative, and educational tasks, with enterprise API requests now 74% work-related and 52% focused on coding/data workflows.

[### Small Kafka: Tansu + SQLite on a free t3.micro (8 minute read)](https://blog.tansu.io/articles/broker-aws-free-tier?utm_source=tldrdata)

You can run a Kafka-compatible Tansu broker cheaply on AWS free tier using SQLite.

## Curated deep dives, tools and trends in big data, data science and data engineering üìä

Subscribe

Join 400,000 readers for [one daily email](/api/latest/data)

[Privacy](/privacy)[Careers](https://jobs.ashbyhq.com/tldr.tech)[Advertise](/data/advertise)

Timestamp: 1769440247