First Mile Data Quality üèÉüèª, Similarity-Based Joins üîó, Why OLTP Indexes Fail ‚úÇÔ∏è

[TLDR](/)

[Newsletters](/newsletters)

[Advertise](https://advertise.tldr.tech/)

[TLDR](/)

# TLDR Data 2025-10-13

## First Mile Data Quality üèÉüèª, Similarity-Based Joins üîó, Why OLTP Indexes Fail ‚úÇÔ∏è

üì±

### Deep Dives

[### Beyond Indexes: How Open Table Formats Optimize Query Performance (20 minute read)](https://jack-vanlightly.com/blog/2025/10/8/beyond-indexes-how-open-table-formats-optimize-query-performance?utm_source=tldrdata)

Traditional secondary indexes in OLTP databases drive efficient point lookups but are fundamentally misaligned with the large-scale, columnar, append-only architecture of open table formats like Apache Iceberg, Delta Lake, and Hudi. Read performance boils down to minimizing IO via data layout optimization (partitioning, sorting, and compaction) and leveraging auxiliary structures such as metadata-based column statistics, Bloom filters, and materialized views for efficient pruning. Analytical query speed hinges on how well physical data organization supports common access patterns, fundamentally diverging from the "index" paradigm of the OLTP world.

[### Why We Bet on Rust to Supercharge Feature Store at Agoda (8 minute read)](https://medium.com/agoda-engineering/why-we-bet-on-rust-to-supercharge-feature-store-at-agoda-ed4a70d2efb7?utm_source=tldrdata)

To overcome unpredictable latency and scaling limitations in its JVM/Scala-based Feature Store, Agoda rewrote its Feature Store Serving component in Rust. The switch slashed CPU and memory use, enabled handling 5x more traffic, and cut projected compute costs by ~84 %. A careful migration strategy (‚Äúshadow testing,‚Äù incremental proof of concept, and using Copilot + the Rust compiler) ensured correctness while onboarding a team without prior Rust experience.

[### How Facebook's Distributed Priority Queue Handles Trillions of Items (13 minute read)](https://blog.bytebytego.com/p/how-facebooks-distributed-priority?utm_source=tldrdata)

Facebook's FOQS is a fully managed, horizontally scalable, multi-tenant distributed priority queue processing over one trillion items per day. Built on sharded MySQL, FOQS utilizes namespaces, topics, and item-level metadata to ensure strict isolation and high throughput. Features include in-memory buffering, batching, adaptive prefetching, and demand-aware routing. The pull-based delivery model, robust disaster recovery, and idempotent ack/nack operations enable resilient, low-latency task processing suitable for massive workloads and varied enterprise use cases.

üöÄ

### Opinions & Advice

[### Not Another Workflow Builder (4 minute read)](https://blog.langchain.com/not-another-workflow-builder/?utm_source=tldrdata)

Visual workflow builders limit scalable AI development. They are too complex for nontechnical users and too limited for advanced use cases. LangChain sees a future where no-code agents handle simple tasks, and code-based workflows with AI-generated code manage complex ones, combining flexibility, maintainability, and practical usability for enterprises.

[### No More Swamps: Building a Better-Governed Data Lake Architecture (8 minute read)](https://www.confluent.io/blog/data-lake-governance-tableflow/?utm_source=tldrdata)

TableFlow bridges Kafka topics/schemas to open formats like Iceberg/Delta Lake, automating preprocessing, validation, and transfer to lakes/warehouses. It uses Confluent Cloud for Flink-based cleansing near sources, enforces schemas via Registry for evolution (e.g., suspending flows on breaks), and pushes read-only metadata directly to catalogs (AWS Glue, Polaris, and Databricks) for real-time sync and consistency across catalogs.

[### Your Data Contracts Are in the Wrong Spot (6 minute read)](https://dataproducts.substack.com/p/your-data-contracts-are-in-the-wrong?utm_source=tldrdata)

Data contracts are most effective when implemented at the "first mile": directly at the data source within software code, rather than downstream in analytical warehouses where they only address symptoms. Teams often misplace contracts, leading to persistent data quality bottlenecks or undetected upstream issues. Successful enforcement requires coordinated cultural change, seamless onboarding, management of tech sprawl, contract versioning, and contextual alerting. Target mission-critical data products with end-to-end "steel threads," ensuring visible, organization-wide quality improvements.

üíª

### Launches & Tools

[### How has AI impacted your productivity? See how others have answered (Sponsor)](https://multitudes.typeform.com/to/uHFHmXrj?utm_source=tldr)

Want to see how other tech leaders are rolling out AI and measuring impact? Fill out the [AI impact survey](https://multitudes.typeform.com/to/uHFHmXrj?utm_source=tldr) to get early access to insights from eng, data, and product leaders like you. It's anonymous and takes 15 minutes. [Take the survey and get free access](https://multitudes.typeform.com/to/uHFHmXrj?utm_source=tldr)

[### Jellyjoin: Join Dataframes or Lists Based on Semantic Similarity (GitHub Repo)](https://github.com/olooney/jellyjoin?utm_source=tldrdata)

The Jellyjoin Python package facilitates soft joins using embedding vectors, allowing data engineers to efficiently merge datasets based on similarity rather than exact matches. Key features include handling high-dimensional data and optimizing join operations, making it relevant for tasks involving machine learning and data integration.

[### Introducing Variant: A New Open Standard for Semi-Structured Data in Apache Parquet, Delta Lake, and Apache Iceberg (5 minute read)](https://www.databricks.com/blog/introducing-variant-new-open-standard-semi-structured-data-apache-parquettm-delta-lake?utm_source=tldrdata)

Variant is now a native Parquet type, with Delta Lake support for over a year, Iceberg v3 approval in May 2025, and over 9,600 lines of code contributed by Databricks to Parquet-java. Variant employs a compact binary format with offsets for fast field navigation (e.g., accessing "order.item.name" without full parsing). Shredding extracts common fields into typed Parquet columns for pruned I/O, data skipping, and compression, supported via SQL table creation and automatic optimization in Delta and Iceberg tables.

[### Lakekeeper (GitHub Repo)](https://github.com/lakekeeper/lakekeeper?utm_source=tldrdata)

Lakekeeper is an Apache-licensed, secure, fast, and user-friendly implementation of the Apache Iceberg REST Catalog spec, written in Rust (based on apache/iceberg-rust). Integrated with Spark, PyIceberg, Trino, and Starrocks, it acts as a catalog for Apache Iceberg tables in open lakehouses, supporting multi-table commits. To start using it, use a Docker image from the catalog or deploy via Helm on K8s.

[### Measuring ETL Price-Performance On Cloud Data Platforms (22 minute read)](https://www.onehouse.ai/blog/measuring-etl-price-performance-on-cloud-data-platforms?utm_source=tldrdata)

ETL benchmarks typically overlook the load stage, which can account for 20-50 % of pipeline time and significantly distort cost-performance results. Lake Loader, a newly open source tool by OneHouse, can be combined with TPC-DS to measure realistic incremental loads across dimensions, facts, and event tables. This method enables ETL price-performance comparisons across data platforms and cost attribution to different phases (ET vs L) and target object types. A simulator with EMR, Databricks, and Snowflake is included in the article.

üéÅ

### Miscellaneous

[### Engineering Real-Time Multimodal AI Pipelines: Scaling File Processing to 50M Daily Uploads (5 minute read)](https://engineering.salesforce.com/building-real-time-multimodal-ai-pipelines-scaling-file-processing-to-50m-daily-uploads/?utm_source=tldrdata)

Salesforce's Prompt Builder now delivers real-time multimodal AI integration, enabling large language models to process and extract structured data from unindexed files (such as PDFs, images, and policy documents) across both Data Cloud and non-Data Cloud environments. The platform's new pipeline manages up to 50 million daily file uploads, validates diverse file types dynamically, and supports seamless interoperability with major LLMs (OpenAI, Gemini, and Anthropic) through a compatibility abstraction layer.

[### From Single-Node to Multi-GPU Clusters: How Discord Made Distributed Compute Easy for ML Engineers (5 minute read)](https://discord.com/blog/from-single-node-to-multi-gpu-clusters-how-discord-made-distributed-compute-easy-for-ml-engineers?utm_source=tldrdata)

Discord's ML systems scaled from simple classifiers to complex models serving millions, facing hurdles like multi-GPU training needs, datasets exceeding single-node capacity, inconsistent manual Ray cluster setups, non-standardized resource management, and siloed custom solutions across teams that hindered reproducibility and efficiency. The team built a Ray-centric platform with a custom CLI for simplified cluster lifecycle management, Dagster-KubeRay orchestration for automated provisioning and workflows, and X-Ray for real-time observability.

‚ö°Ô∏è

### Quick Links

[### Benchmark Results for DuckDB v1.4 LTS (1 minute read)](https://duckdb.org/2025/10/09/benchmark-results-14-lts.html?utm_source=tldrdata)

DuckDB scales to 100 TB TPC-H on a single node, proving the in-process engine's viability for massive analytic workloads.

[### Pipelining in psql (PostgreSQL 18) (4 minute read)](https://postgresql.verite.pro/blog/2025/10/01/psql-pipeline.html?utm_source=tldrdata)

PostgreSQL 18 introduces enhanced pipelining capabilities in the psql command-line client, allowing clients to send multiple queries without waiting for previous results, which significantly boosts query throughput.

## Curated deep dives, tools and trends in big data, data science and data engineering üìä

Subscribe

Join 308,008 readers for [one daily email](/api/latest/data)

[Privacy](/privacy)[Careers](https://jobs.ashbyhq.com/tldr.tech)[Advertise](/data/advertise)

Timestamp: 1760401528