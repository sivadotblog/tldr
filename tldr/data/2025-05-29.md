Spark 4 Is Out ‚ú®, DuckDB‚Äôs Lakehouse Format ü¶Ü, Salesforce Buys Informatica üíµ

[TLDR](/)

[Newsletters](/newsletters)

[Advertise](https://advertise.tldr.tech/)

[TLDR](/)

# TLDR Data 2025-05-29

## Spark 4 Is Out ‚ú®, DuckDB‚Äôs Lakehouse Format ü¶Ü, Salesforce Buys Informatica üíµ

üì±

### Deep Dives

[### How to Reduce Your Power BI Model Size by 90% (21 minute read)](https://towardsdatascience.com/how-to-reduce-your-power-bi-model-size-by-90/?utm_source=tldrdata)

Power BI models often balloon in-memory due to unused fields, high-cardinality columns, and raw transaction tables. You can dramatically improve VertiPaq compression by dropping irrelevant tables/columns, grouping rare values to lower cardinality, choosing compact data types, and materializing early aggregations in a star schema. Implementing composite models with DirectQuery, partitioning with incremental refresh, and disabling Auto Date/Time further caps memory growth. Combined, these tactics can shrink PBIX file sizes by up to 90%, slash refresh windows, and deliver sub-second query performance

[### Backfilling Postgres TOAST Columns in Debezium Data Change Events (7 minute read)](https://www.morling.dev/blog/backfilling-postgres-toast-columns-debezium-change-events/?utm_source=tldrdata)

Backfilling TOAST columns in PostgreSQL for Debezium change events captures large data fields like text or binary data in change data capture (CDC) processes. Debezium's snapshotting feature generates change events for existing data, including TOAST column data, without manual table alterations. This ensures data consistency and supports analytics and data integration with minimal downtime.

[### Lakeflow Connect: Efficient and Easy Data Ingestion Using the SQL Server Connector (15 minute read)](https://www.databricks.com/blog/lakeflow-connect-efficient-and-easy-data-ingestion-using-sql-server-connector?utm_source=tldrdata)

The Databricks LakeFlow Connect SQL Server connector simplifies data ingestion from SQL Server databases into the Databricks Data Intelligence Platform, enabling efficient and scalable data pipelines with minimal setup. It uses change data capture (CDC) and change tracking to incrementally ingest data, integrating with Unity Catalog for governance.

üöÄ

### Opinions & Advice

[### 20 Pandas One-Liners That Can Save You Hours of Work (6 minute read)](https://www.nb-data.com/p/20-pandas-one-liners-that-can-save?utm_source=tldrdata)

Essential one-liner Pandas techniques that boost data processing efficiency, memory usage, and automation for managing large datasets or optimizing resource-intensive pipelines. Examples include using pyarrow backend memory optimization, window functions for group-based calculations, and vectorized timestamp operations.

[### Snowflake Is Cheaper & Faster Than Databricks Serverless SQL with Real Data & by a Lot (7 minute read)](https://medium.com/@nickakincilar/snowflake-is-cheaper-faster-than-databricks-serverless-with-real-data-by-a-lot-dc2fe021838a?utm_source=tldrdata)

Benchmarking with realistic, variable data and proper dimensional modeling shows Snowflake Gen1 (16-node XL) runs common analytics queries about 58% faster and 28% cheaper than Databricks Large, and even an 8-node Snowflake Large beats Databricks on both speed and cost. Earlier results favoring Databricks were skewed by uniform synthetic data, poor schema design, and caching. Fixing these reveals Snowflake's clear performance edge for real-world workloads. Always validate benchmarks with your own data and best-practice SQL patterns for an accurate comparison.

üíª

### Launches & Tools

[### DuckLake: SQL as a Lakehouse Format (20 minute read)](https://duckdb.org/2025/05/27/ducklake.html?utm_source=tldrdata)

DuckLake is a new open lakehouse format, officially released as part of DuckDB version 1.3.0, that uses a standard SQL database to manage metadata and stores data in Parquet files, avoiding vendor lock-in. It supports full ACID transactions, time travel for querying data snapshots, and integration with any SQL database that supports ACID operations and primary keys.

[### Introducing Apache Spark 4.0 (10 minute read)](https://www.databricks.com/blog/introducing-apache-spark-40?utm_source=tldrdata)

Apache Spark 4.0 delivers a major upgrade with richer SQL capabilities (scripting, reusable UDFs, PIPE syntax, and ANSI compliance), expanded Spark Connect support across multiple languages, and tighter data integrity via default ANSI mode and a new VARIANT type for JSON. PySpark users gain built-in plotting and a Python DataSource API, while Structured Streaming adds a powerful transformWithState API and state store improvements for more reliable, observable stream processing.

[### Meet the‚ÄØdbt‚ÄØFusion Engine: the New Rust-based, Industrial-grade Engine for dbt (10 minute read)](https://docs.getdbt.com/blog/dbt-fusion-engine?utm_source=tldrdata)

dbt Labs just announced the dbt Fusion Engine, which reimplements dbt's runtime in Rust, replacing the Python-based core to deliver up to 10x faster model compilation and execution with lower memory overhead without altering existing SQL or Jinja code. Exposed via CLI, Language Server (for real-time error detection in editors like VS Code), and integrated into dbt Studio, it enables real-time error detection, cost-efficient builds, and is positioned as the foundation for emerging AI-driven analytics workflows.

[### Python Pandas Ditches NumPy for Speedier PyArrow (3 minute read)](https://thenewstack.io/python-pandas-ditches-numpy-for-speedier-pyarrow/?utm_source=tldrdata)

Pandas 3.0 replaces NumPy with Apache PyArrow as its default backend, significantly boosting performance for loading and reading columnar data. PyArrow, which uses the Apache Arrow columnar format, offers faster data processing and better memory efficiency, allowing users to maintain the familiar Pandas API while gaining substantial speed improvements.

üéÅ

### Miscellaneous

[### How to Generate Synthetic Data: A Comprehensive Guide Using Bayesian Sampling and Univariate Distributions (31 minute read)](https://towardsdatascience.com/how-to-generate-synthetic-data-a-comprehensive-guide-using-bayesian-sampling-and-univariate-distributions/?utm_source=tldrdata)

Generating realistic data for ML often risks exposing sensitive records or lacks fidelity to original distributions. By modeling each feature's empirical distribution with Probability Density Functions (best for continuous distributions) or Bayesian priors (best for categorical data) and drawing posterior samples, you can produce synthetic values that match real-world behavior without leaking private information.

[### Salesforce Takeover of Informatica is on for $8 Billion (1 minute read)](https://www.theregister.com/2025/05/27/salesforce_snaps_up_informatica_for/?utm_source=tldrdata)

Salesforce is acquiring Informatica, an enterprise data management and analytics provider, in an $8 billion deal. This will be Salesforce's fourth significant acquisition in the data management space, after Data Cloud, MuleSoft, and Tableau. It is aiming to expand its capabilities in data integration, governance, and analytics, directly within its platform.

‚ö°Ô∏è

### Quick Links

[### Spark 4.0 Release Notes (5 minute read)](https://spark.apache.org/releases/spark-release-4-0-0.html?utm_source=tldrdata)

Spark 4.0.0 fixes over 5,100 issues and adds lightweight Spark Connect clients (1.5 MB Python, Go, Swift, and Rust) with a configurable spark.api.mode.

[### I Am A Staff Data Scientist At A Big Tech Company -- AMA (5 minute read)](https://www.reddit.com/r/datascience/comments/1kjjb32/i_am_a_staff_data_scientist_at_a_big_tech_company/?utm_source=tldrdata)

A staff data scientist with a PhD in Statistics working at a big tech company shares insights on whether to get a PhD, their role, and work experiences.

## Curated deep dives, tools and trends in big data, data science and data engineering üìä

Subscribe

Join 308,008 readers for [one daily email](/api/latest/data)

[Privacy](/privacy)[Careers](https://jobs.ashbyhq.com/tldr.tech)[Advertise](/data/advertise)

Timestamp: 1748564838