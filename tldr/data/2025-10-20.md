Warehouse-Native Application Layers üöÄ, Kafka Goes Diskless ‚òÅÔ∏è, Smarter Agentic BI üß©

[TLDR](/)

[Newsletters](/newsletters)

[Advertise](https://advertise.tldr.tech/)

[TLDR](/)

# TLDR Data 2025-10-20

## Warehouse-Native Application Layers üöÄ, Kafka Goes Diskless ‚òÅÔ∏è, Smarter Agentic BI üß©

üì±

### Deep Dives

[### Advanced RAG Techniques for High-Performance LLM Applications (8 minute read)](https://neo4j.com/blog/genai/advanced-rag-techniques/?utm_source=tldrdata)

Advanced Retrieval-Augmented Generation (RAG) architectures address LLM production challenges by integrating hybrid retrieval (semantic + lexical), metadata filtering, reranking, and structured chunking with knowledge graph-driven context. Implementing stepwise agentic planning, query expansion, and context distillation further improves precision, recall, and grounding. Knowledge graphs enable multi-hop reasoning, higher explainability, and traceable provenance, supporting scalable, trustworthy GenAI applications.

[### Emerging Architectures for Modern Data Infrastructure (10 minute read)](https://a16z.com/emerging-architectures-for-modern-data-infrastructure/?utm_source=tldrdata)

While core data systems (warehouses, ingest, transformation) remain stable, a Cambrian explosion of supporting tools (metric layers, observability, data apps) is reshaping how teams build on top of data. a16z introduces the data platform hypothesis: vendors are becoming platforms with common backends and APIs, letting ‚Äúfrontend‚Äù developers build apps without redoing plumbing. As logic and apps move into the data tier, it may shift the balance from ETL and BI toward warehouse-native application layers.

[### How and Why Netflix Built a Real-Time Distributed Graph: Part 1 ‚Äî Ingesting and Processing Data Streams at Internet Scale (7 minute read)](https://netflixtechblog.com/how-and-why-netflix-built-a-real-time-distributed-graph-part-1-ingesting-and-processing-data-80113e124acc?utm_source=tldrdata)

As Netflix's ecosystem expands, traditional data warehouses struggle with integrating cross-device, cross-vertical events in real-time. They built Real-Time Distributed Graph (RDG), a system designed to unify siloed member interaction data across streaming, ads, live events, and gaming. RDG uses graph structures for efficient relationship queries, pattern detection, and schema flexibility, enabling faster insights for personalization and operations.

[### The Good, The Bad, and The AutoMQ (7 minute read)](https://medium.com/fresha-data-engineering/the-good-the-bad-and-the-automq-5aa7a8748e71?utm_source=tldrdata)

Kafka is undergoing a major architectural shift toward diskless, shared-storage design driven by cloud-native architecture maturation, as indicated by the proposals led by Slack, Aiven, and AutoMQ. Fast Tiering (Slack), which offloads active logs to fast object stores, could optimize replication costs by cutting broker footprint and cross-AZ traffic, while Diskless Topics (Aiven) redesigns Kafka altogether with brokers as compute nodes and object storage as the source of truth. Unified Shared Storage (AutoMQ) proposes a pluggable storage engine, seamlessly supporting both disk-backed and object-store backends (already implemented). Together, these efforts mark an irreversible shift where streaming and storage converge, redefining what Kafka can mean.

[### Deletion Vectors and Puffin Files Merge in the New v3 Iceberg Format (3 minute read)](https://medium.com/@vincent_daniel/deletion-vectors-and-puffin-files-merge-in-the-new-v3-iceberg-format-565188036d0c?utm_source=tldrdata)

Apache Iceberg v3 introduces deletion vectors, enabling efficient row-level deletes without file rewrites and offering significant CDC MERGE performance gains compared to v2. Benchmarks on Amazon EMR with Spark 3.5.5 show 2x (filtered read) to 4x (merge) performance improvements from Iceberg v2 over v3. Deletion vectors simultaneously optimize storage, S3 Access, and compute costs, as well as operational ones, by changing file compaction from a routine necessity to an occasional task.

üöÄ

### Opinions & Advice

[### Why I'm Not a Fan of Zero-copy Apache Kafka-Apache Iceberg (8 minute read)](https://jack-vanlightly.com/blog/2025/10/15/why-im-not-a-fan-of-zero-copy-apache-kafka-apache-iceberg?utm_source=tldrdata)

Zero-copy, Kafka brokers tier log segments directly to Iceberg tables for dual use by streaming consumers and analytics, eroding Kafka's strengths. It shifts costs from storage to compute, creates hybrid systems prone to conflicts, and favors logical unification (e.g., via schemas and reliable data movement) over physical sharing. Alternatively, materialization's duplication is already standard and preferable for decoupling.

[### Is Postgres Read Heavy or Write Heavy? (And Why Should You Care) (10 minute read)](https://www.crunchydata.com/blog/is-postgres-read-heavy-or-write-heavy-and-why-should-you-care?utm_source=tldrdata)

Classifying Postgres workloads as read-heavy or write-heavy is crucial for targeted performance tuning, resource allocation, and scaling. Since reads are often cheaper (via caching) than writes (which involve WAL logging, index updates, and flushes), misclassification leads to inefficiencies, e.g., over-indexing hurts writes, while under-caching slows reads. Most databases lean read-heavy (10:1 ratio), but monitor for proactive optimization.

[### Data Modeling for the Agentic Era: Semantics, Speed, and Stewardship (27 minute read)](https://www.ssp.sh/blog/agentic-data-modeling/?utm_source=tldrdata)

AI agents can generate queries or dashboards in seconds, but without structure, they're just guessing. Agentic workflows should build on three essential pillars: Semantics, a curated metrics layer where agents query well-defined business entities and measures; Speed, sub-second analytics so humans can instantly verify AI outputs; and Stewardship - guardrails, human oversight, and versions that keep agents reliable and compliant. These principles create safe and effective systems where humans stay in the driver's seat and AI acts like a BI pair-programmer.

üíª

### Launches & Tools

[### Guide to Data Science on Google Cloud: From Demand Forecasting to Agentic Flows (Sponsor)](https://cloud.google.com/resources/content/data-science-guide?e=48754805&amp;hl=en&amp;utm_source=cloud_sfdc&amp;utm_medium=email&amp;utm_campaign=FY25-Q3-GLOBAL-ENT36892-website-dl-datasciencebook-99286&amp;utm_content=tldr&amp;utm_term=oct_20)

This guide helps you get started with data science workflows on Google Cloud and offers a range of [real-world use cases (with code)](https://cloud.google.com/resources/content/data-science-guide?e=48754805&hl=en&utm_source=cloud_sfdc&utm_medium=email&utm_campaign=FY25-Q3-GLOBAL-ENT36892-website-dl-datasciencebook-99286&utm_content=tldr&utm_term=oct_20) for you to explore. See how you can tackle challenges, ranging from "traditional" ML problems such as segmentation to agentic AI automation. [Read the guide](https://cloud.google.com/resources/content/data-science-guide?e=48754805&hl=en&utm_source=cloud_sfdc&utm_medium=email&utm_campaign=FY25-Q3-GLOBAL-ENT36892-website-dl-datasciencebook-99286&utm_content=tldr&utm_term=oct_20)

[### Apache Flink Agents 0.1.0 Release Announcement (5 minute read)](https://flink.apache.org/2025/10/15/apache-flink-agents-0.1.0-release-announcement/?utm_source=tldrdata)

Apache Flink Agents 0.1.0 introduces a unified framework for integrating event-driven AI agents that incorporate LLMs, tools, memory, and orchestration directly into Flink's streaming runtime. It provides massive scalability, millisecond latency, exactly-once processing, and state management, with native support for Python and Java and seamless integration with Flink DataStream and Table APIs.

[### MinerU ‚Äî Open-Source Document Parser for Agentic Workflows (GitHub Repo)](https://github.com/opendatalab/MinerU?utm_source=tldrdata)

MinerU transforms complex documents (e.g., PDFs) into LLM-ready Markdown/JSON, preserving layout, tables, formulas, and reading order. Built for agents and enterprise use, it supports OCR in 84 languages, GPU/CPU/NPU acceleration, and outputs richly structured representations interpreted by downstream AI systems. Trade-offs: sophisticated deployment and resource requirements; edge cases (handwritten docs, comics) still limited.

üéÅ

### Miscellaneous

[### Scaling LLM Inference: Innovations in Tensor Parallelism, Context Parallelism, and Expert Parallelism (3 minute read)](https://engineering.fb.com/2025/10/17/ai-research/scaling-llm-inference-innovations-tensor-parallelism-context-parallelism-expert-parallelism/?utm_source=tldrdata)

Meta has introduced advanced parallelism techniques, including tensor, context, and expert parallelism, to optimize LLM inference at scale, reducing end-to-end latency by 10‚Äì50% for key workloads compared with baseline libraries. Innovations include direct data access (DDA) algorithms for communication efficiency, fast-attention kernels for context parallelism enabling under one-minute inference of 10 million tokens, and near-linear scaling on multi-node clusters.

[### Effective Human Supervision: the M-AI-nority Report (11 minute read)](https://www.zeropartydata.es/p/effective-human-supervision-the-minority?utm_source=tldrdata)

‚ÄúHuman in the loop‚Äù AI compliance is often superficial, with organizations implementing nominal oversight (such as perfunctory human rubber-stamping and inadequate training) while real automated decision risks and mandated legal safeguards (GDPR, AI Act) are ignored. Documented failures (e.g., Amazon hiring, Dutch SyRI, Wells Fargo) illustrate how unchecked automation perpetuates bias, fails meaningful review, and exposes organizations to multimillion penalties and reputational risk. Ensure rigorous, context-aware human supervisory processes and robust metrics targeting both system performance and legal compliance, as mere technical ‚Äúoverrides‚Äù or compliance theater are insufficient under regulatory scrutiny.

‚ö°Ô∏è

### Quick Links

[### The ‚Äúdbt Fear Index‚Äù Just Spiked, and You Should Be Worried (2 minute read)](https://www.linkedin.com/posts/danthelion_the-dbt-fear-index-just-spiked-and-you-share-7384229937823707136-PzCO?utm_source=tldrdata)

The surge in ‚Äúdbt Fear Index‚Äù shows the community's concern that the Fivetran + dbt Labs merger could lead to higher costs and tighter vendor control.

[### What's new for Python in 2025? (12 minute read)](https://www.jumpingrivers.com/blog/whats-new-py314/?utm_source=tldrdata)

Python 3.14 adds free-threading, allowing genuine multi-core execution for faster, CPU-bound data workflows.

## Curated deep dives, tools and trends in big data, data science and data engineering üìä

Subscribe

Join 308,008 readers for [one daily email](/api/latest/data)

[Privacy](/privacy)[Careers](https://jobs.ashbyhq.com/tldr.tech)[Advertise](/data/advertise)

Timestamp: 1761006408