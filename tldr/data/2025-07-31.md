Postgres Tops Dev Charts üêò, Agents Tackle dbt Tasks ü§ñ, Airbnb SQL via Kubernetes üè†

[TLDR](/)

[Newsletters](/newsletters)

[Advertise](https://advertise.tldr.tech/)

[TLDR](/)

# Improve your data knowledge in 5 min 2025-07-31

## Postgres Tops Dev Charts üêò, Agents Tackle dbt Tasks ü§ñ, Airbnb SQL via Kubernetes üè†

üì±

### Deep Dives

[### Achieving High Availability with Distributed Database on Kubernetes at Airbnb (6 minute read)](https://medium.com/airbnb-engineering/achieving-high-availability-with-distributed-database-on-kubernetes-at-airbnb-58cc2e9856f4?utm_source=tldrdata)

Airbnb integrated an open-source, horizontally scalable SQL database into its Kubernetes infrastructure. To avoid quorum loss and service disruption, it used AWS EBS for quick storage volume reattachment, enabled by Kubernetes' Persistent Volume Claims, and a custom k8s operator to manage node replacements, ensuring that new nodes synced with the cluster's state before proceeding.

[### The Evolution of Grab's Machine Learning Feature Store (11 minute read)](https://engineering.grab.com/evolution-of-grab-machine-learning-feature-store?utm_source=tldrdata)

Grab started revamping its ML feature store in November 2023, upgrading to a feature-table-centric architecture built on Amazon Aurora PostgreSQL and S3-backed Parquet tables. This design tackles high-cardinality and high-dimensional data via composite keys, atomic batch updates, and strict read-write isolation across three AZs. A reverse-ETL ingestion pipeline ensures reproducible, versioned feature deployments with serverless writers and scaled readers. Decentralised SDKs and atomic deployments reduce operational complexity and boost model consistency.

[### Building Jetflow: A Framework for Flexible, Performant Data Pipelines at Cloudflare (9 minute read)](https://blog.cloudflare.com/building-jetflow-a-framework-for-flexible-performant-data-pipelines-at-cloudflare/?utm_source=tldrdata)

Cloudflare's Jetflow is a modular Consumers-Transformers-Loaders data ingestion architecture that leverages Apache Arrow and YAML-based configuration/build for high ingestion efficiency and throughput per connection. Its idempotent RunInstance/Partition/Batch model and optimized Go drivers ensure low garbage collection overhead and high parallelism. Deployed for 141 billion daily rows, Jetflow cuts runtime and resource costs, simplifies end-to-end testing, and scales flexibly across diverse sources.

[### Create Event-Driven Airflow DAGs with Kafka (14 minute video)](https://www.youtube.com/watch?v=KgEGrKbfBbM&amp;utm_source=tldrdata)

Airflow 3 now supports event-driven data pipelines triggered by Kafka messages, enabling workflows to react instantly to external events like user actions. The setup involves using Astro CLI for a local Airflow environment, configuring Kafka via Docker, and creating a producer-consumer pipeline where messages on a Kafka topic automatically trigger a DAG. This approach simplifies real-time processing without relying solely on time-based scheduling.

üöÄ

### Opinions & Advice

[### What Are Deletion Vectors (DLV)? (5 minute read)](https://www.confessionsofadataguy.com/what-are-deletion-vectors-dlv/?utm_source=tldrdata)

Deletion Vectors in Delta Lake enable efficient Merge-on-Read operations by marking rows for deletion in metadata without rewriting Parquet files, improving write performance for DELETE, UPDATE, and MERGE operations. While they preserve ACID semantics and are ideal for high-write tables, they introduce read-time overhead and require regular maintenance like OPTIMIZE and REORG to manage accumulated deletion markers.

[### Build vs. Buy Data Pipeline: How to Decide (5 minute read)](https://www.rudderstack.com/blog/build-vs-buy-data-pipeline/?utm_source=tldrdata)

Deciding whether to build or buy a data pipeline involves weighing the customization and control of in-house solutions against the speed, reduced maintenance, and scalability of managed vendor tools. Building suits teams with unique needs and strong engineering capacity, while buying is ideal for rapid deployment, limited resources, or when offloading maintenance and updates to vendors.

[### Developing a dbt Project with Cursor: My #Profound\_Moment with MCP (5 minute read)](https://alexiospanos.com/posts/2025-07-04-cursor-mcp-dbt/?utm_source=tldrdata)

A recent experience with Cursor demonstrated significant advancements in software development workflows, particularly in automating dbt project tasks using MCP servers. By leveraging AI agents to autonomously handle tasks from project management in Linear to SQL model creation in Supabase, the author noted both the potential and limitations of current technology, highlighting the need for oversight despite the efficiency gains. The evolution of these tools signals a transformative shift in data engineering practices, emphasizing the importance of adapting to emerging workflows and automation capabilities.

üíª

### Launches & Tools

[### Apache DataFusion 49.0.0 Released (5 minute read)](https://datafusion.apache.org/blog/2025/07/28/datafusion-49.0.0?utm_source=tldrdata)

DataFusion 49.0.0 introduces major performance upgrades, notably a redesigned equivalence system for faster planning on high-column queries; dynamic filtering and TopK pushdown, which yield over 1.5x speedups for certain TPC-H workloads; and async User-Defined Functions, which enable integration with external services like LLMs. Additional enhancements include support for user-defined types, column-level Parquet encryption, ordered-set aggregates via WITHIN GROUP, disk spill file compression, and new regex functions.

[### 2025 Stack Overflow Developer Survey: Admired and Desired Databases (2 minute read)](https://survey.stackoverflow.co/2025/technology#admired-and-desired-database-desire-admire?utm_source=tldrdata)

PostgreSQL leads as both the most admired (65.5%) and most desired (46.5%) database among developers, with strong usage (28.3%) and continued interest (59%) from the community. Other databases like Supabase, SQLite, and Redis also show high desire-to-usage ratios, indicating growing momentum. DuckDB and Databricks SQL, while still niche, are gaining attention in modern data workflows.

[### Turso (GitHub Repo)](https://github.com/tursodatabase/turso?utm_source=tldrdata)

Turso Database is an in-process OLTP engine library written in Rust, fully SQLite-compatible with C API and file formats, that enables embedded high-performance workloads. It supports real-time change data capture, async I/O on Linux via io\_uring, and polyglot drivers for Go, JavaScript, Java, Python, Rust, and WebAssembly. Turso is still a work in progress - you can get up to $1,000 by spotting data corruption bugs during Alpha.

üéÅ

### Miscellaneous

[### Dreaming of Graphs in the Open Lakehouse (13 minute read)](https://semyonsinchenko.github.io/ssinchenko/post/dreams-about-graph-in-lakehouse/?utm_source=tldrdata)

The Open Lakehouse supports various data types with open storage standards like Apache Iceberg and GeoParquet, but lacks native support for property graphs, which are increasingly vital for AI and Graph RAG applications. Apache GraphAr aims to provide a storage standard for graphs, while tools like GraphFrames, KuzuDB, and Apache HugeGraph offer scalable batch processing, fast in-memory analytics, and interactive querying, respectively.

[### Data Privacy Challenges in Open MCP Architectures (25 minute read)](https://hackernoon.com/data-privacy-challenges-in-open-mcp-architectures?source=rss&amp;utm_source=tldrdata)

Model Context Protocol standardizes dynamic LLM integrations via a uniform interface to external systems, but broadens the attack surface across interception/tampering, endpoint compromise, malicious tool servers, prompt injection, data exfiltration, and over-privileged scopes. Key mitigations include strict OAuth 2.1 with least-privilege, TLS 1.3 end-to-end encryption, ephemeral data retention policies, cryptographic signing plus immutable audit logs, and rigorous third-party vetting and sandboxing. Adoption of confidential compute, homomorphic encryption, and dynamic consent frameworks enhance privacy and resilience.

‚ö°Ô∏è

### Quick Links

[### Stream and Batch Processing Convergence in Apache Flink (25 minute read)](https://www.infoq.com/presentations/stream-finch/?utm_source=tldrdata)

The latest Apache Flink advancements deliver near-Spark batch performance on a unified batch and streaming engine, enabling identical code to run seamlessly on both real-time and finite datasets.

[### Giving Benchmarks a Boat (5 minute read)](https://buttondown.com/jaffray/archive/giving-benchmarks-a-boat/?utm_source=tldrdata)

Accurate benchmarks like TPC-C matter because they enforce a standard workload-to-data ratio, enabling fair comparisons.

## Curated deep dives, tools and trends in big data, data science and data engineering üìä

Subscribe

Join 308,008 readers for [one daily email](/api/latest/data)

[Privacy](/privacy)[Careers](https://jobs.ashbyhq.com/tldr.tech)[Advertise](/data/advertise)

Timestamp: 1754008435