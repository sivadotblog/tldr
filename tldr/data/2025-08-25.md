Agent Data Access at Meta ü§ñ, Tiering Tradeoffs in Storage ‚öñÔ∏è, dbt MCP Servers ‚öôÔ∏è

[TLDR](/)

[Newsletters](/newsletters)

[Advertise](https://advertise.tldr.tech/)

[TLDR](/)

# Improve your data knowledge in 5 min 2025-08-25

## Agent Data Access at Meta ü§ñ, Tiering Tradeoffs in Storage ‚öñÔ∏è, dbt MCP Servers ‚öôÔ∏è

üì±

### Deep Dives

[### A Conceptual Model for Storage Unification (16 minute read)](https://jack-vanlightly.com/blog/2025/8/21/a-conceptual-model-for-storage-unification?utm_source=tldrdata)

Storage unification is becoming increasingly important as object storage dominates, though hot data still requires low-latency solutions. Architectural choices in the tiering approach, virtualization layer, and access method all involve tradeoffs. Shared tiering can reduce duplication (and system cost) but requires strict coordination, ownership, and lifecycle governance to avoid becoming a liability. Materialization, grounded in decades of proven practice, remains the simpler and more reliable approach.

[### Datadog's Rust-Based Timeseries Engine: Throttling Under Heavy Load (10 minute read)](https://www.datadoghq.com/blog/engineering/rust-timeseries-engine/#throttling-under-heavy-load?utm_source=tldrdata)

Datadog's new timeseries engine, built in Rust, achieves major performance gains versus its previous Go and RocksDB-based implementation (60x faster ingestion and 5x faster queries) using a shard‚Äëper‚Äëcore architecture and modular LSM-tree storage. Despite perfect sharding, surge traffic or complex queries can push nodes to their limits. To stay robust, Datadog implemented permit-based throttling, shedding or rejecting workloads when metrics like ingestion lag, memory usage, and concurrent query count cross thresholds.

[### How Fresha Accidentally Became one of UK's First StarRocks Production Pioneer (11 minute read)](https://medium.com/fresha-data-engineering/how-we-accidentally-became-one-of-uks-first-starrocks-production-pioneers-7db249f10010?utm_source=tldrdata)

Fresha's data stack relied on Postgres and Snowflake for ad-hoc analytics, but performance bottlenecks and unpredictable dashboard latency under load prompted a shift. It adopted StarRocks, valued for MySQL protocol connectivity, federated querying over open-formats (Iceberg/Paimon), and sub-second real-time analytics via internal tables. Engineering integrated Flink and Kafka pipelines into a hybrid model (real-time, historical, and search lanes), all unified under a single SQL surface. Post-migration, critical dashboards' p95 latency dropped from 20 s to ~200 ms.

üöÄ

### Opinions & Advice

[### Creating AI Agent Solutions for Warehouse Data Access and Security (12 minute read)](https://engineering.fb.com/2025/08/13/data-infrastructure/agentic-solution-for-warehouse-data-access/?utm_source=tldrdata)

Meta is introducing a multi-agent system to streamline and secure data warehouse access, with user agents helping request data and owner agents managing permissions. The system uses LLMs for context- and task-specific decision-making combined with guardrails like query-level controls, data-access budgets, and rule-based risk checks. This approach reduces friction in access requests while maintaining strong security and auditability.

[### The Database Has a New User‚ÄîLLMs‚Äîand They Need a Different Database (6 minute read)](https://www.tigerdata.com/blog/the-database-new-user-llms-need-a-different-database?utm_source=tldrdata)

Embedding natural language semantic descriptions within PostgreSQL schemas enables self-describing databases, significantly enhancing LLM-driven SQL query generation. TigerData's experiments show up to a 27% boost in SQL accuracy (58% to 86% on certain schemas) using an LLM-generated semantic catalog. Storing and version-controlling these YAML-based semantic annotations tightens context, mitigates misinterpretation, and streamlines agentic data interaction. An implementation is shared in a linked GitHub repository.

[### Enterpriseland and Productland (6 minute read)](https://practicaldatamodeling.substack.com/p/enterpriseland-and-productland?utm_source=tldrdata)

Data teams operate in two distinct models: Enterpriseland, where data is a cost center focused on internal reporting, and Productland, where data is integral to revenue-driving products and user experiences. In Productland, modular, domain-driven data models directly power product features and have clear, measurable ROI, while Enterpriseland struggles to quantify value and justify investment. Understanding where your organization stands clarifies expectations and optimizes team focus and impact as the shift toward "data-as-a-product" and AI adoption is redefining data's role from operational support to strategic value creation.

[### Knowledge, Metrics, and AI: Rethinking the Semantic Layer with David Jayatillake (41 minute podcast)](https://www.dataengineeringweekly.com/p/knowledge-metrics-and-ai-rethinking?utm_source=tldrdata)

Semantic layers are shifting from BI lock-in to dynamic, AI-maintained infrastructure. Semantic layers are critical when teams face inconsistent metrics (e.g., revenue and churn) or slow query responses that AI and semantics could resolve instantly. AI enhances semantic layers by generating new metrics, ensuring governance, and enabling natural language queries, making semantics ‚Äúinvisible‚Äù and responsive to executive demands for instant answers.

üíª

### Launches & Tools

[### Unifying ecomm and operational data with CData and BigQuery (Sponsor)](https://www.cdata.com/resources/inside-medik8s-data-driven-makeover-with-cdata-and-bigquery/?utm_source=tldr&amp;utm_medium=&amp;utm_campaign=25Q3_Sync_Google_Medik9)

Join global ecommerce brand Medik8, Google Cloud, and CData for a [live webinar on August 27](https://www.cdata.com/resources/inside-medik8s-data-driven-makeover-with-cdata-and-bigquery/?utm_source=tldr&utm_medium=&utm_campaign=25Q3_Sync_Google_Medik9). In the session, you'll see how a modern data stack powered by BigQuery + CData Sync helped Medik8 unify ecommerce data, scale analytics, and fuel global growth. [Sign up for free](https://www.cdata.com/resources/inside-medik8s-data-driven-makeover-with-cdata-and-bigquery/?utm_source=tldr&utm_medium=&utm_campaign=25Q3_Sync_Google_Medik9)

[### Build Reliable AI Agents with the dbt MCP Server (6 minute read)](https://www.getdbt.com/blog/build-reliable-ai-agents-with-the-dbt-mcp-server?utm_source=tldrdata)

The dbt Model Context Protocol (MCP) server provides a standardized, open interface that bridges AI agents with governed, structured metadata, lineage, and execution context from dbt projects. It supports metadata discovery, semantic-layer querying, and executing dbt commands such as build, run, compile, and test. This enables automation of workflows from answering business questions via natural language to running and validating dbt migrations.

[### From Rainfall to Rows - The Butterfly Effect in PostgreSQL (8 minute read)](https://www.cybertec-postgresql.com/en/from-rainfall-to-rows-the-butterfly-effect-in-postgresql/?utm_source=tldrdata)

Small changes in a PostgreSQL database, like updating a single row, can trigger significant downstream impacts, such as altering dashboards or influencing decisions. Unlike a static tool, PostgreSQL behaves like a responsive system with features like triggers, notify events, and dynamic query planning, allowing it to intelligently adapt to data changes.

[### Apache Paimon: Real-Time Lake Storage with Iceberg Compatibility (20 minute read)](https://www.alibabacloud.com/blog/apache-paimon-real-time-lake-storage-with-iceberg-compatibility-2025_602485?utm_source=tldrdata)

Apache Paimon is a streaming-optimized table format built on a Log‚ÄëStructured Merge‚Äëtree (LSM) architecture that seamlessly integrates with Apache Flink for low-latency ingestion and merging. A recent key feature, Iceberg compatibility via deletion vectors, allows real-time data to be queried alongside batch data within Iceberg ecosystems, enabling minute‚Äëlevel freshness. Production deployments at Alibaba, ByteDance, Vivo, and Shopee witness Paimon's increasing maturity and adoption.

üéÅ

### Miscellaneous

[### From Facts & Metrics to Media Machine Learning: Evolving the Data Engineering Function at Netflix (6 minute read)](https://netflixtechblog.com/from-facts-metrics-to-media-machine-learning-evolving-the-data-engineering-function-at-netflix-6dcc91058d8d?utm_source=tldrdata)

Netflix is evolving data engineering into Media ML Data Engineering, creating a Media Data Lake to standardize and serve multi-modal media data for ML. This enables faster experimentation, richer insights, and tighter integration between creative workflows and machine learning.

[### Securing private data at scale with differentially private partition selection (6 minute read)](https://research.google/blog/securing-private-data-at-scale-with-differentially-private-partition-selection/?utm_source=tldrdata)

Differentially private (DP) partition selection is the task of safely selecting frequently appearing items across users while preserving privacy by adding noise and thresholding. Google introduced a new algorithm family, MAD, with a two-round variant, MAD2R, that adaptively redistributes ‚Äúweight‚Äù from overly common items to less frequent ones, boosting utility. It achieves state-of-the-art performance across diverse datasets, including the massive Common Crawl corpus (~800 billion entries), covering 99.9% of partitions and 97% of database records, all while maintaining rigorous DP guarantees.

‚ö°Ô∏è

### Quick Links

[### Tecton is Joining Databricks to Power Real-Time Data for Personalized AI Agents (3 minute read)](https://www.databricks.com/blog/tecton-joining-databricks-power-real-time-data-personalized-ai-agents?utm_source=tldrdata)

Tecton is joining Databricks to enhance real-time data capabilities for AI agents, integrating Tecton's leading enterprise feature store with Databricks' Agent Bricks.

[### Debunking myths about Airflow's architecture and performance (7 minute read)](https://www.astronomer.io/blog/debunking-myths-about-airflows-architecture-and-performance/?utm_source=tldrdata)

Concerns that Airflow has an unreliable scheduler, is hard to scale, cannot process data in tasks, or lacks versioning are outdated.

## Curated deep dives, tools and trends in big data, data science and data engineering üìä

Subscribe

Join 308,008 readers for [one daily email](/api/latest/data)

[Privacy](/privacy)[Careers](https://jobs.ashbyhq.com/tldr.tech)[Advertise](/data/advertise)

Timestamp: 1756168004