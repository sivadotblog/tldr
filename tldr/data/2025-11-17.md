Smarter Select-Driven Grouping üß†, Uber‚Äôs Query Telemetry üîç, Single-Node Engines Win ‚öôÔ∏è

[TLDR](/)

[Newsletters](/newsletters)

[Advertise](https://advertise.tldr.tech/)

[TLDR](/)

# TLDR Data 2025-11-17

## Smarter Select-Driven Grouping üß†, Uber‚Äôs Query Telemetry üîç, Single-Node Engines Win ‚öôÔ∏è

üì±

### Deep Dives

[### I/O Observability for Uber's Massive Petabyte-Scale Data Lake (7 minute read)](https://www.uber.com/blog/i-o-observability-for-ubers-massive-petabyte-scale-data-lake/?utm_source=tldrdata)

Uber engineered a unified, zero-touch I/O observability system to monitor read/write patterns across its petabyte-scale data lake, spanning on-premises and cloud storage. By intercepting file system operations, it efficiently monitors 400K daily Spark apps, 2M Presto queries, and 6.7M YARN containers with less than 5-minute delay, enabling precise network egress attribution, cross-zone congestion detection, and dataset heat-maps for tiering.

[### How Yelp modernized its data infrastructure with a streaming lakehouse on AWS (12 minute read)](https://aws.amazon.com/blogs/big-data/how-yelp-modernized-its-data-infrastructure-with-a-streaming-lakehouse-on-aws/?utm_source=tldrdata)

Yelp overhauled its legacy data pipelines by adopting a streaming lakehouse architecture built with Apache Flink, Apache Paimon, Amazon MSK, and S3, slashing analytics data latency from 18 hours to minutes and reducing storage costs by over 80%. The migration replaced bespoke CDC formats and complex Kafka chains with SQL-accessible, versioned tables and community-supported standards, enabling flexible scaling, real-time CDC, and built-in data management features like schema evolution.

[### How We Made Our Internal Data Warehouse AI-first (13 minute read)](https://clickhouse.com/blog/ai-first-data-warehouse?utm_source=tldrdata)

ClickHouse transformed its internal data warehouse from a traditional BI-focused system to an AI-first setup, allowing users to gain insights via natural language queries without SQL. This addresses analytics bottlenecks, handling ~70% of use cases, driven by advanced LLMs (e.g., Anthropic's Claude models) and the Model Context Protocol (MCP). The result, DWAINE (Data Warehouse AI Natural Expert), democratizes data access, slashing query times from 30-45 minutes to near-instant, while reducing analyst workload by 50-70%.

üöÄ

### Opinions & Advice

[### 650GB of Data (Delta Lake on S3). Polars vs DuckDB vs Daft vs Spark (7 minute read)](https://dataengineeringcentral.substack.com/p/650gb-of-data-delta-lake-on-s3-polars?utm_source=tldrdata)

Testing DuckDB, Polars, and Daft on a 32GB EC2 node against a 650GB Delta Lake table demonstrated that single-node engines can efficiently process large lakehouse datasets: Polars completed a full aggregation in 12 minutes, DuckDB in 16, Daft in 50, and PySpark in over an hour. Distributed clusters, while still relevant, are no longer essential for such workloads thanks to impressive Larger-Than-Memory support and simple integration. Single-node frameworks deliver substantial cost savings, reduced operational complexity, and straightforward code, challenging conventional lakehouse architecture assumptions.

[### Organizing Code, Experiments, and Research for Kaggle Competitions (14 minute read)](https://towardsdatascience.com/organizing-code-experiments-and-research-for-kaggle-competitions/?utm_source=tldrdata)

Robust code organization, meticulous experiment tracking, and reproducible environments are non-negotiable for successful data science projects. Using modular repo structures, version control, and tools like wandb and Hydra streamlines experimentation and collaboration across platforms.

[### Batch Vs Real-Time Data Pipelines ‚Äì Do We Still Need To Pick? (5 minute read)](https://www.theseattledataguy.com/batch-vs-real-time-data-pipelines-do-we-still-need-to-pick/?utm_source=tldrdata)

While the debate between batch and real-time/streaming data pipelines persists, modern tools like Estuary allow seamless toggling between modes, letting teams run real-time for high-stakes needs and batch/micro-batch elsewhere to cut costs. This enables dynamic pipeline adjustments based on business value and use cases, with hybrid or "right-time" approaches offering flexibility to balance cost, complexity, and timeliness.

[### Scaling Data Products Starts With Fixing the Foundation: Five Lessons We've Learned (7 minute read)](http://rudderstack.com/blog/scaling-data-products-fix-the-foundation/?utm_source=tldrdata)

Most failures in scaling data products stem from weak foundational pipelines rather than flawed models or analytics. By treating pipelines like products that come with clear ownership, versioned changes, SLOs, and standardized plumbing, teams can build reliable, observable data foundations that could scale.

üíª

### Launches & Tools

[### O'Reilly + dbt Labs report: Building platforms designed for AI success (Sponsor)](https://www.getdbt.com/resources/structured-for-intelligence-why-al-needs-governed-discoverable-and-provisioned-data?utm_medium=paid-email&amp;utm_source=tldr&amp;utm_campaign=q4-2026_tldr-newsletters_cv&amp;utm_content=_newsletter2___&amp;utm_term=all_all__)

Without structured, trusted data, your AI investments stall or stumble. This [early release eBook](https://www.getdbt.com/resources/structured-for-intelligence-why-al-needs-governed-discoverable-and-provisioned-data?utm_medium=paid-email&utm_source=tldr&utm_campaign=q4-2026_tldr-newsletters_cv&utm_content=_newsletter2___&utm_term=all_all__) from O'Reilly and dbt Labs lays out the tools, governance, and strategy needed to structure data for AI. Learn how to turn fragmented systems into AI-ready platforms, preempt compliance issues, and position your enterprise ahead of competitors. Read the early chapters and get the full report sent to you when it's released. [Get your copy](https://www.getdbt.com/resources/structured-for-intelligence-why-al-needs-governed-discoverable-and-provisioned-data?utm_medium=paid-email&utm_source=tldr&utm_campaign=q4-2026_tldr-newsletters_cv&utm_content=_newsletter2___&utm_term=all_all__)

[### Waiting for SQL:202y: GROUP BY ALL (4 minute read)](https://peter.eisentraut.org/blog/2025/11/11/waiting-for-sql-202y-group-by-all?utm_source=tldrdata)

GROUP BY ALL is a new SQL feature that automatically groups by all non-aggregate expressions in the SELECT list, removing the need to repeat columns manually. It works well for simple queries but intentionally avoids handling ambiguous cases where expressions mix aggregates and non-aggregates. Changes to the SELECT list implicitly change the grouping, so it should be used with care, especially in complex queries.

[### Announcing BigQuery-managed AI functions for better SQL (4 minute read)](https://cloud.google.com/blog/products/data-analytics/sql-reimagined-for-the-ai-era-with-bigquery-ai-functions/?utm_source=tldrdata)

BigQuery now supports AI.IF, AI.CLASSIFY, and AI.SCORE functions, enabling direct semantic filtering, classification, and ranking of unstructured data in SQL without manual prompt tuning or model selection. These managed AI functions use Gemini LLMs and integrate with WHERE, JOIN, GROUP BY, and ORDER BY clauses.

üéÅ

### Miscellaneous

[### Capital One at EMNLP 2025: Trust and efficiency in AI (7 minute read)](https://medium.com/capital-one-tech/capital-one-at-emnlp-2025-trust-and-efficiency-in-ai-d87afc16a0cd?utm_source=tldrdata)

Capital One showcased several peer-reviewed advances at EMNLP 2025, including a multi-agent LLM framework for complex financial workflows (MACAW) and a data augmentation framework (GRAID), which boosts guardrail model F1 scores by 12%. Additional key contributions feature a merged-embedding approach delivering 47.5% greater RAG consistency, TruthTorchLM for multi-method truthfulness evaluation, and activation-based confidence estimation enabling fast and trustworthy LLM deployment.

[### Reflections from JupyterCon 2025 ‚Äî A Week of Ideas, Code, and Community (4 minute read)](https://medium.com/womenintechnology/reflections-from-jupytercon-2025-8ace9e6b27ab?utm_source=tldrdata)

The Jupyter ecosystem has evolved far beyond notebooks: it's powering enterprise AI pipelines, reproducible research, and data-science applications at scale. From extension workshops to community sprints, what stood out most at JupyterCon 2025 was the human side - showing that thriving open-source isn't just about code, it's about people.

[### Effective context engineering for AI agents (12 minute read)](https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents?utm_source=tldrdata)

Context engineering has become critical for building reliable, long-horizon LLM agents, shifting focus from prompt optimization to actively managing the set of information (‚Äòtokens') sent to models at inference time. Effective strategies, such as compaction, structured note-taking, dynamic just-in-time retrieval, and sub-agent architectures, address context window constraints, minimize context rot, and improve agent coherence across complex workflows.

‚ö°Ô∏è

### Quick Links

[### Beyond Quantization: Bringing Sparse Inference to PyTorch (6 minute read)](https://pytorch.org/blog/beyond-quantization-bringing-sparse-inference-to-pytorch/?utm_source=tldrdata)

Sparse inference is emerging as a transformative optimization for LLM deployment.

[### Race Condition in DynamoDB DNS System: Analyzing the AWS US-EAST-1 Outage (3 minute read)](https://www.infoq.com/news/2025/11/aws-dynamodb-outage-postmortem/?utm_source=tldrdata)

The prolonged AWS US-EAST-1 outage of October 19 was caused by a latent race condition in DynamoDB's automated DNS management system.

## Curated deep dives, tools and trends in big data, data science and data engineering üìä

Subscribe

Join 400,000 readers for [one daily email](/api/latest/data)

[Privacy](/privacy)[Careers](https://jobs.ashbyhq.com/tldr.tech)[Advertise](/data/advertise)

Timestamp: 1763393909