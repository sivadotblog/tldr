Airflow 3.0 GA üå™Ô∏è, Super Charged Pinot ingestion üç∑, Kafka to Flink ‚û°Ô∏è

[TLDR](/)

[Newsletters](/newsletters)

[Advertise](https://advertise.tldr.tech/)

[TLDR](/)

# TLDR Data 2025-04-24

## Airflow 3.0 GA üå™Ô∏è, Super Charged Pinot ingestion üç∑, Kafka to Flink ‚û°Ô∏è

üì±

### Deep Dives

[### 2025 State of Analytics Engineering Report (10 minute read)](https://www.getdbt.com/resources/reports/state-of-analytics-engineering-2025?utm_source=tldrdata)

dbt Labs' 2025 State of Analytics Engineering Report highlights the transformative impact of AI on data teams. 70% of analytics professionals now use AI for code development and 50% for documentation, indicating AI's role in augmenting rather than replacing human expertise. Data budgets are expanding, with AI tooling being the primary investment focus. Despite technological advancements, data quality remains a critical concern - unreliable data still leads to unreliable outputs. The report also notes a diversification in the analytics engineering field, with significant growth in regulated industries like finance and healthcare.

[### A Deep Dive Into Ingesting Debezium Events From Kafka With Flink SQL (15 minute read)](https://www.morling.dev/blog/ingesting-debezium-events-from-kafka-with-flink-sql/?utm_source=tldrdata)

Ingesting Debezium change events from Kafka into Flink SQL can be achieved using various connector and format combinations, each offering distinct semantics. The Apache Kafka SQL Connector, when paired with JSON or Avro formats, treats events as an append-only stream, requiring explicit modeling of Debezium's event structure. Alternatively, using Debezium-specific formats like debezium-json or debezium-avro-confluent enables Flink to interpret events as a changelog stream, facilitating incremental materialized views. The Upsert Kafka SQL Connector processes flattened events as changelogs, suitable for scenarios where only the current row state is needed. Selecting the appropriate combination depends on the desired processing semantics and the structure of the Debezium events.

[### Decomposing Transactional Systems (24 minute read)](https://transactional.blog/blog/2025-decomposing-transactional-systems?utm_source=tldrdata)

Transactional systems execute, order, validate, and persist transactions, with each step's design impacting system properties like consistency or scalability. Execution evaluates transaction logic to produce reads and writes, ordering assigns a transaction's temporal position (e.g., timestamp or log sequence), validation ensures conflict-free outcomes, and persistence ensures durability, often via replication logs. Different orderings of these steps yield unique database types, as seen in systems like Calvin's deterministic sequencing or Aurora DSQL's MVCC-based approach.

üöÄ

### Opinions & Advice

[### From Data Engineer to YAML Engineer (Part II) (4 minute read)](https://juhache.substack.com/p/from-data-engineer-to-yaml-engineer?utm_source=tldrdata)

Data engineering is increasingly embracing declarative paradigms, reducing the need for manual coding in building data pipelines. Tools like dlt for ingestion, SQLMesh for transformation, and Rill for BI offer declarative interfaces that simplify complex workflows while allowing for imperative customizations when needed. Solutions like Starlake propose to integrate both ingestion and transformation in a seamless declarative framework, enhancing the efficiency and consistency of operations across longer segments of the data lifecycle.

[### The Case for Dagster: Moving Beyond Airflow in the Modern Data Stack (5 minute read)](https://www.linkedin.com/pulse/case-dagster-moving-beyond-airflow-modern-data-stack-dagsterlabs-ksovc?utm_source=tldrdata)

Dagster represents a shift in data orchestration that focuses on data assets rather than tasks, offering a more modern, scalable approach for data platforms. Unlike Airflow's task-centric model, Dagster provides better developer productivity, observability, self-service capabilities, and resource optimization. It also facilitates easier migrations and integration with existing Airflow systems, which makes it an effective choice for building future-proof data ecosystems.

[### Approaching Data Quality in Today's Complex Data World (13 minute read)](https://thedataecosystem.substack.com/p/issue-43-data-quality-today?utm_source=tldrdata)

Data quality issues continue to plague organizations due to complex data ecosystems and a lack of cohesive strategies, resulting in costly inconsistencies and mistrust. Despite advances in data tools, the divide between business and data professionals perpetuates inefficiencies. A comprehensive approach combining detection, enforcement, and ownership via platform-based solutions is crucial. Leveraging AI can improve data quality by automating detection and correction tasks, yet human oversight remains vital to address contextual and interpretative challenges. Engaging cross-functional collaboration and enhancing data literacy are essential for sustainable data quality improvements.

üíª

### Launches & Tools

[### Diskless Kafka: 80% Leaner, 100% Open (8 minute read)](https://aiven.io/blog/diskless-apache-kafka-kip-1150?utm_source=tldrdata)

Apache Kafka's KIP-1150 introduces Diskless Topics, which replicate data directly to object storage, bypassing local disk usage to reduce costs by up to 80% and enable instant autoscaling and low-cost geo-replication. Unlike Tiered Storage (KIP-405), which retains active segments on local disks, Diskless Topics eliminate replication-related expenses like inter-zone data transfers, modifying just 1.7% of Kafka's codebase without altering client APIs. The leaderless architecture uses a pluggable BatchCoordinator (e.g., SQLite or DynamoDB-backed) to assign offsets, with brokers writing to shared log segments in S3.

[### SQLFlow (GitHub Repo)](https://github.com/turbolytics/sql-flow?utm_source=tldrdata)

SQLFlow is an open-source stream processing engine that leverages DuckDB and Apache Arrow to enable high-performance data pipelines defined entirely in SQL. Designed as a lightweight alternative to systems like Flink, it supports ingesting data from sources such as Kafka and WebSockets and writing outputs to destinations including PostgreSQL, Kafka topics, and cloud storage in formats like Parquet and Iceberg. With capabilities like tumbling window aggregations, stream enrichment, and support for user-defined functions, SQLFlow aims to simplify real-time data processing workflows. Its architecture facilitates efficient processing of tens of thousands of messages per second on a single machine with low memory overhead.

[### Apache Airflow 3 is Generally Available! (5 minute read)](https://airflow.apache.org/blog/airflow-three-point-oh-is-here/?utm_source=tldrdata)

Apache Airflow 3.0 introduces significant enhancements that include an upgraded UI, DAG versioning, improved backfill support, and event-driven scheduling for better integration with external systems. With a focus on ease of use, multi-cloud compatibility, and scalability, this release also brings improvements for ML/AI workflows, security, and task execution across environments. It's designed to better meet the growing needs of data teams managing complex workflows.

[### Powering Apache Pinot ingestion with Hoptimator (6 minute read)](https://www.linkedin.com/blog/engineering/infrastructure/powering-apache-pinot-ingestion-with-hoptimator?utm_source=tldrdata)

Apache Pinot's original ingestion, reliant on storage nodes consuming Kafka streams, faced inefficiencies like excessive I/O and CPU usage due to unoptimized data formats, partitioning, and lack of pre-processing, requiring manual producer-side logic and new Kafka topics. Hoptimator addresses these pain points by enabling consumer-driven, self-service data pipelines, dynamically creating optimized Flink SQL jobs and Kafka topics via a Subscription API to deliver pre-processed, Pinot-specific streams with minimal fields and ideal partitioning. It automates schema evolution, partition adjustments, and resource management, reducing operational toil and server resource demands.

üéÅ

### Miscellaneous

[### Type alignment and padding bytes: how to not waste space in PostgreSQL tables (4 minute read)](https://www.cybertec-postgresql.com/en/type-alignment-padding-bytes-no-space-waste-in-postgresql/?utm_source=tldrdata)

PostgreSQL's type alignment and padding ensure efficient memory and CPU access by aligning data types (e.g., integer and bigint) to specific byte boundaries, such as 4-byte or 8-byte, to match hardware requirements. Misaligned data incurs performance penalties due to extra CPU cycles, while padding bytes fill gaps to maintain alignment, potentially increasing storage size. Techniques like reordering table columns (e.g., placing bigint before boolean) or using composite types can minimize padding waste, optimizing storage and query performance.

[### All in the Data: Data Governance in the Not-for-Profit World (6 minute read)](https://tdan.com/all-in-the-data-data-governance-in-the-not-for-profit-world/32572?utm_source=tldrdata)

Non-Invasive Data Governance (NIDG) is a tailored solution for not-for-profit organizations that integrates data governance within existing roles without adding bureaucratic overhead. It eschews complex, enterprise-style frameworks for a flexible approach that delineates clear, role-specific expectations, ensuring compliance is part of everyday tasks. NIDG enhances mission-driven work by empowering staff to manage data efficiently, aiding in regulatory compliance, and improving operational sustainability despite limited resources. This model aligns governance with the not-for-profits' dynamic environment by utilizing current tools and promoting engagement without disrupting their core mission.

‚ö°Ô∏è

### Quick Links

[### Apache Airflow 3.0.0 Example DAGs (GitHub Repo)](https://github.com/apache/airflow/tree/3.0.0/airflow-core/src/airflow/example_dags?utm_source=tldrdata)

This repository contains Apache Airflow's example DAGs, which showcase various improvements and adjustments made to the codebase.

[### SIGMOD Programming Contest Archive (2 minute read)](https://transactional.blog/sigmod-contest/?utm_source=tldrdata)

An archive that preserves annual SIGMOD Programming Contest tasks and frameworks to ensure ongoing access to valuable database performance challenges.

## Curated deep dives, tools and trends in big data, data science and data engineering üìä

Subscribe

Join 308,008 readers for [one daily email](/api/latest/data)

[Privacy](/privacy)[Careers](https://jobs.ashbyhq.com/tldr.tech)[Advertise](/data/advertise)

Timestamp: 1745540783