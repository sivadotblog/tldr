Automated KPI storytelling ü§ñ, Fast Search on Postgres üîç, Database Fixture Frameworks üõ†

[TLDR](/)

[Newsletters](/newsletters)

[Advertise](https://advertise.tldr.tech/)

[TLDR](/)

# TLDR Data 2025-06-02

## Automated KPI storytelling ü§ñ, Fast Search on Postgres üîç, Database Fixture Frameworks üõ†

üì±

### Deep Dives

[### How Instacart Built a Modern Search Infrastructure on Postgres (11 minute read)](https://tech.instacart.com/how-instacart-built-a-modern-search-infrastructure-on-postgres-c528fa601d54?utm_source=tldrdata)

Instacart's modern search infrastructure on Postgres combines traditional full-text search with semantic retrieval using pgvector, unifying both mechanisms into a single, scalable system. This hybrid approach minimizes document overfetching, enhances precision and recall through fine-grained dataset control, and reduces operational overhead by consolidating datastores.

[### Building a Distributed Cache for S3 (25 minute read)](https://clickhouse.com/blog/building-a-distributed-cache-for-s3?utm_source=tldrdata)

ClickHouse Cloud's distributed cache for S3 provides a shared, low-latency network service that speeds up data access by allowing compute nodes to share hot data, outperforming local SSDs in many cases. It eliminates the need for local cache rebuilding, reduces S3 reads, and supports elastic scaling with performance improvements.

[### How we Brought Multimedia Search to Dropbox Dash (5 minute read)](https://dropbox.tech/infrastructure/multimedia-search-dropbox-dash-evolution?utm_source=tldrdata)

Dropbox Dash's multimedia search enables fast, accurate retrieval of images, videos, and audio by indexing lightweight metadata and using just-in-time previews, reducing latency and compute costs. The system leverages existing Dropbox infrastructure for cleaner code and incorporates location-aware queries and fuzzy file naming for improved relevance.

[### From Data to Stories: Code Agents for KPI Narratives (10 minute read)](https://towardsdatascience.com/from-data-to-stories-code-agents-for-kpi-narratives/?utm_source=tldrdata)

Hugging Face's smolagents framework streamlines the creation of code agents and can be used to transform raw data into narrative-rich KPI reports. We can use smolagents to automate data analysis and construct meaningful business reports directly from structured data. This empowers data teams to deliver executive-ready narratives with minimal manual effort, accelerating analytics workflows.

üöÄ

### Opinions & Advice

[### Don't Mock the Database: Data Fixtures are Parallel Safe, and Plenty Fast (6 minute read)](https://www.crunchydata.com/blog/dont-mock-the-database-data-fixtures-are-parallel-safe-and-plenty-fast?utm_source=tldrdata)

Data fixtures in PostgreSQL testing are faster and more reliable than database mocks, as they leverage real database operations, inserting complex objects in about 100 microseconds and supporting parallel test execution. Unlike mocks, fixtures accurately reflect database constraints like unique keys and foreign keys, ensuring realistic test coverage.

[### Kafka: The End of the Beginning (10 minute read)](https://materializedview.io/p/kafka-end-of-beginning?utm_source=tldrdata)

Apache Kafka and Flink have dominated streaming for over a decade with little fundamental change, as broad adoption has outpaced innovation. Entrenched protocols like Kafka's partition-and-leadership model now constrain evolution even as object-store integrations (e.g., Redpanda and WarpStream) emerge, and novel systems like S2 rethink per-user streams free from Kafka's legacy. Evolution opportunities lie in compact libraries (D2) and next-generation compute layers (Feldera) that promise true cloud-native scalability. As batch processing saw a renaissance with lakehouses, the next decade in streaming could similarly shift toward flexible, high-performance architectures that transcend on-prem protocols.

[### Scaling Data Operations With Platform Engineering (42 minute podcast)](https://www.dataengineeringpodcast.com/database-platform-engineering-episode-466?utm_source=tldrdata)

Delivering reliable, scalable database services across diverse systems presents challenges in consistency, automation, and legacy migrations. The Platform Engineering approach proposes to centralize database expertise, offers databases-as-a-service through tools like AWS Service Catalog, and integrates AI/ML for task automation. This standardized platform model enhances efficiency and ultimately drives organizational buy-in.

üíª

### Launches & Tools

[### Google Cloud's Open Lakehouse: Architected for AI, Open Data, and Unrivaled Performance (5 minute read)](https://cloud.google.com/blog/products/data-analytics/extending-the-google-data-cloud-lakehouse-architecture/?utm_source=tldrdata)

Google Cloud's lakehouse architecture introduces BigLake Iceberg native storage, utilizing Google Cloud Storage for enterprise-grade management of Apache Iceberg data. It integrates unified operational and analytical engines for seamless data processing with BigQuery and AlloyDB.

[### How to Query Apache Kafka¬Æ Topics With Natural Language (5 minute read)](https://www.confluent.io/blog/querying-kafka-natural-language/?utm_source=tldrdata)

Confluent's Stream Processing with natural language, powered by ksqlDB, enables users to query Kafka data streams using simple English-like SQL statements, simplifying real-time data processing. It supports tasks like filtering, aggregating, and joining streaming data, making it accessible for users without deep programming knowledge.

[### Where We're Headed with the dbt Fusion Engine (6 minute read)](https://www.getdbt.com/blog/where-we-re-headed-with-the-dbt-fusion-engine?utm_source=tldrdata)

The roadmap for the recently released dbt Fusion Engine focuses on expanding Rust‚Äêbased performance and developer ergonomics without altering SQL authoring. Upcoming priorities include seamless support for incremental models and ephemeral tables, improved dependency graph visualizations, and richer IDE integrations that catch errors in real time. Future work also targets AI‚Äêassisted features (like automated test generation), deeper observability into runtime metrics, and pluggable adapters for non‚ÄêSQL data sources.

[### Designing Pareto-optimal GenAI Workflows with syftr (10 minute read)](https://www.datarobot.com/blog/pareto-optimized-ai-workflows-syftr/?utm_source=tldrdata)

Building effective GenAI workflows involves exploring an enormous combination of components. syftr automates this via multi-objective Bayesian optimization with a Pareto Pruner to balance cost, accuracy, and latency. This open-source, modular framework helps data engineers quickly pinpoint cost-effective, high-accuracy RAG pipelines (e.g., on the CRAG Sports benchmark) without manual trial-and-error.

üéÅ

### Miscellaneous

[### Current London 2025 (5 minute read)](https://www.streamingdata.tech/p/current-london-2025?utm_source=tldrdata)

The Current London 2025 conference was last week. Confluent proposed Flink jobs as AI agents and unveiled ‚Äúsnapshot‚Äù queries that hybridize batch and streaming with Iceberg and Kafka. OpenAI and Pinterest highlighted PyFlink's adoption and the real-world needs for backfill and unified CDC ingestion. Despite deep technical innovation, the data streaming sector faces slow growth and consolidation risks, with competitive tensions rising as vendors reposition in a maturing space.

[### Data and AI Work ‚Ä¶ is it Engineering? (8 minute read)](https://martinchesbrough.net/data-and-ai-work-is-it-engineering-a7dea64c672a?utm_source=tldrdata)

Data and AI engineering should be recognized as genuine engineering disciplines, not merely technical "building" tasks, due to their reliance on scientific principles, mathematical models, and ongoing innovation. Yet, as other software engineering disciplines, it is less about traditional engineering's defined process models, and more influenced by empirical and iterative processes. Adopting agile methods (notably Extreme Programming) and practices like data contracts, pattern recognition, and ‚Äúdo the simplest thing that could possibly work‚Äù can drive higher-quality, more maintainable, and cost-effective data systems.

‚ö°Ô∏è

### Quick Links

[### Google Releases MedGemma: Open AI Models for Medical Text and Image Analysis (4 minute read)](https://www.infoq.com/news/2025/05/google-medgemma?utm_source=tldrdata)

Google has open-sourced MedGemma, a pair of generative AI models for medical text and image analysis.

[### Delta Live Tables recipes: Consuming from Azure Event Hubs using Unity Catalog Service Credentials (3 minute read)](https://alexott.blogspot.com/2025/05/delta-live-tables-recipes-consuming.html?utm_source=tldrdata)

Delta Live Tables can now use Unity Catalog Service Credentials to obtain short-lived tokens for Azure Event Hubs, eliminating the need for long-lived secrets like service principal passwords or SAS tokens and enhancing security.

## Curated deep dives, tools and trends in big data, data science and data engineering üìä

Subscribe

Join 308,008 readers for [one daily email](/api/latest/data)

[Privacy](/privacy)[Careers](https://jobs.ashbyhq.com/tldr.tech)[Advertise](/data/advertise)

Timestamp: 1748910474