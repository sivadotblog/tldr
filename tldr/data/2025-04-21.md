GPT SQL Optimization ü§ñ, Iceberg S3 Event Ingestion ‚ùÑÔ∏è, Faster Postgres Queries üöÄ

[TLDR](/)

[Newsletters](/newsletters)

[Advertise](https://advertise.tldr.tech/)

[TLDR](/)

# TLDR Data 2025-04-21

## GPT SQL Optimization ü§ñ, Iceberg S3 Event Ingestion ‚ùÑÔ∏è, Faster Postgres Queries üöÄ

üì±

### Deep Dives

[### How Agoda Uses GPT to Optimize SQL Stored Procedures in CI/CD (6 minute read)](https://medium.com/agoda-engineering/how-agoda-uses-gpt-to-optimize-sql-stored-procedures-in-ci-cd-29caf730c46c?utm_source=tldrdata)

Agoda has integrated GPT into its CI/CD pipeline to streamline SQL stored procedure (SP) optimization, significantly cutting down manual efforts by enabling self-service tools for developers. The integration has reduced the time spent on optimization, with developers previously dedicating 366 man-days annually, predominantly on failed performance tests. GPT's process involves detailed input analysis, query refinement, indexing suggestions, and provides side-by-side performance comparisons, leading to more informed decision-making. Since implementation, GPT's accuracy has reached approximately 25%, and further developments are underway to enhance its scope and accuracy in SP optimization.

[### Hacking the Postgres Statistics Tables for Faster Queries (15 minute read)](https://www.crunchydata.com/blog/hacking-the-postgres-statistics-tables-for-faster-queries?utm_source=tldrdata)

PostgreSQL's query planner relies on internal statistics to optimize execution plans, but it assumes column independence, which can lead to inefficient queries when columns are correlated. To address this, PostgreSQL offers extended statistics that capture relationships between columns, improving the planner's estimations. Users can provide the planner with more accurate data distributions by creating multivariate statistics using the CREATE STATISTICS command and running ANALYZE. This approach enhances query performance by enabling the planner to make better decisions based on the actual data relationships within the database.

[### Cost-efficient event ingestion into Iceberg S3 Tables on AWS (13 minute read)](https://tobilg.com/cost-efficient-event-ingestion-into-iceberg-s3-tables-on-aws#heading-verdict?utm_source=tldrdata)

Routing CloudFront logs through CloudWatch Logs into Kinesis Data Firehose with a Lambda processor enables pay‚Äëper‚Äëuse ingestion directly into Apache Iceberg S3 Tables on AWS, avoiding fixed Kinesis Data Streams fees. Deployment uses three Serverless Framework stacks plus manual S3 Table and LakeFormation setup because CloudFormation support is incomplete and permission race conditions exist. Emerging solutions like Cloudflare Pipelines with R2 Data Catalog promise simpler, lower‚Äëcomplexity lakehouse stacks.

[### The internal of BigQuery, Snowflake, Databricks, and Redshift (10 minute read)](https://vutr.substack.com/p/the-internal-of-bigquery-snowflake?utm_source=tldrdata)

BigQuery, Snowflake, and Databricks each offer distinct architectures for data processing. BigQuery's serverless, columnar storage model uses the Dremel query engine and Colossus for scalable storage, separating compute and storage with data in immutable Capacitor files to support features like Time Travel, while Vortex enables streaming and batch analytics. Snowflake's hybrid shared-nothing architecture, which focuses on isolation and structured/semi-structured data, decouples compute (Virtual Warehouses scaled via ‚ÄúT-shirt sizes‚Äù) and storage (on S3) using FoundationDB for metadata and min-max pruning for query optimization. Databricks, built on Apache Spark, implements a Lakehouse model with Delta Lake for ACID-compliant storage that supports unified SQL, Python, and real-time ML workflows on scalable compute clusters.

üöÄ

### Opinions & Advice

[### Data Engineering: Now with 30% More Bullshit (4 minute read)](https://luminousmen.com/post/data-engineering-now-with-30-more-bullshit?utm_source=tldrdata)

Data engineering is mired in overhyped trends like the Medallion Architecture, a useful but oversold model marketed as revolutionary, and Zero ETL, which shifts complexity to real-time integrations (e.g., Kafka, Kinesis, and Pub/Sub) without eliminating data cleaning, logic writing, or debugging issues like schema drift and deduplication. The Modern Data Stack‚Ñ¢, a vendor-coined term for cloud-native tools like Snowflake, dbt, and Airflow, has become a hollow marketing phrase for selling the same six tools to every company.

[### dbt Isn't Declarative ‚Äî And That's a Problem (5 minute read)](https://jennykwan.org/posts/dbt-isnt-declarative/?utm_source=tldrdata)

dbt's ‚Äúdeclarative‚Äù model really runs an imperative DAG of side‚Äëeffecting SQL that mutates state, leading to inconsistent inputs, slow failures, and unreproducible pipelines. AprioriDB offers an append‚Äëonly, cell‚Äëversioned, artifact‚Äëbased database foundation to deliver true declarative workflows, safe rollbacks, and faster, more reliable data engineering.

üíª

### Launches & Tools

[### Introducing MAI-DS-R1 (19 minute read)](https://techcommunity.microsoft.com/blog/machinelearningblog/introducing-mai-ds-r1/4405076?utm_source=tldrdata)

MAI‚ÄëDS‚ÄëR1 boosts DeepSeek R1's ability to answer previously blocked queries to 99.3%, outpacing the original model by 2.2√ó while matching top competitors on reasoning benchmarks and halving harmful outputs. It was post‚Äëtrained on 460,000 safety and non‚Äëcompliance examples and scored significantly higher on internal satisfaction and harm‚Äëmitigation evaluations.

[### SpacetimeDB‚ÄîA Yak Shaving Success Story? (8 minute read)](https://blog.slamdunk.software/spacetimedb-a-yak-shaving-success-story/?utm_source=tldrdata)

SpacetimeDB is a source-available backend platform developed by Clockwork Labs originally to support its MMORPG, BitCraft. It integrates server compute, transactional storage, and real-time networking into a single system, allowing developers to write backend logic as reducers‚Äîstateful, transactional cloud functions‚Äîalongside relational tables and subscription queries. This architecture, which aims to simplify development and reduce infrastructure complexity, eliminates the traditional separation between application servers and databases. While designed for gaming, SpacetimeDB also powers applications like Pogly, a real-time stream overlay used by Twitch streamers. These radical design choices and the ambiguous implications of its Business Source License (BSL 1.1) choice certainly set SpacetimeDB apart from other data platforms.

[### An Intro to DeepSeek's Distributed File System (5 minute read)](https://maknee.github.io/blog/2025/3FS-Performance-Journal-1/?utm_source=tldrdata)

DeepSeek's 3FS, a distributed filesystem designed for AI workloads, features four components: Meta servers handle file metadata (paths, permissions, and sizes) in distributed key-value stores; Mgmtd centrally manages node liveness, replication factors, and chain configurations; Storage nodes employ the Rust ChunkEngine to break data into chunks, storing metadata (ID, size, and checksums) in LevelDB, with workers managing allocation, reclamation, and async reads, while forwarding writes through CRAQ chains; and Client coordinates file operations and data transfers across components. CRAQ guarantees strong consistency and linearizability via sequential write propagation and load-balanced read queries.

üéÅ

### Miscellaneous

[### So you want to use Object Storage (8 minute read)](https://spiraldb.com/post/so-you-want-to-use-object-storage?utm_source=tldrdata)

Running databases directly on object storage offers scalability and cost benefits, but introduces challenges like unpredictable tail latencies. These latencies, often due to the distributed nature of object storage systems, can degrade performance. The three strategies typically implemented by cloud data applications to maintain consistent performance are hedging (sending multiple parallel requests), caching (on block storage), and horizontal scaling (read across multiple connections).

[### Parquet is a streaming data format (1 minute read)](https://www.linkedin.com/posts/danforsberg_parquet-is-a-streaming-data-format-i-activity-7319055651689631744-M64r?utm_source=tldrdata)

This post discusses a Rust-based agent that ingests Arrow FlightRPC streams into Parquet files on S3 Iceberg by streaming concurrent multipart uploads, offering zero‚Äëcopy memory, SIMD‚Äëoptimized deduplication, exactly‚Äëonce delivery, and true horizontal scaling without compaction. It hits up to 4 GB/s on a laptop and supports tunable persistence and sequence‚Äëbased ordering.

[### SeaTunnel + Bedrock + OpenSearch = AI That Gets What You're Saying (10 minute read)](https://hackernoon.com/seatunnel-bedrock-opensearch-ai-that-gets-what-youre-saying?utm_source=tldrdata)

Apache SeaTunnel orchestrates real-time ingestion and transformation of text data by calling Amazon Bedrock for embeddings and loading semantic vectors into Amazon OpenSearch to power hybrid semantic and structured search. Its plugin-based, loosely coupled design lets you swap AI models or downstream systems easily and includes best practices for embedding caching, vector dimension tuning, and OpenSearch index lifecycle management.

‚ö°Ô∏è

### Quick Links

[### Data Modeling is Dead. All AI Needs in OBT! (2 minute read)](https://practicaldatamodeling.substack.com/p/data-modeling-is-dead-all-ai-needs?utm_source=tldrdata)

Data modeling remains crucial - despite claims that One Big Table (OBT) suffices for AI applications, AI and BI deployment alike require structured, well-modeled data to ensure reliable outcomes.

[### Vanna (GitHub Repo)](https://github.com/vanna-ai/vanna?utm_source=tldrdata)

Vanna is a Python package that uses retrieval augmentation to help you generate accurate SQL queries for your database using LLMs.

## Curated deep dives, tools and trends in big data, data science and data engineering üìä

Subscribe

Join 308,008 readers for [one daily email](/api/latest/data)

[Privacy](/privacy)[Careers](https://jobs.ashbyhq.com/tldr.tech)[Advertise](/data/advertise)

Timestamp: 1745281555