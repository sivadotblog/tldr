Scaling Crawlers Smartly üï∏Ô∏è, CDC Boosts Parquet Efficiency üì¶, DIME Sharpens Dense Search üìâ

[TLDR](/)

[Newsletters](/newsletters)

[Advertise](https://advertise.tldr.tech/)

[TLDR](/)

# Improve your data knowledge in 5 min 2025-07-28

## Scaling Crawlers Smartly üï∏Ô∏è, CDC Boosts Parquet Efficiency üì¶, DIME Sharpens Dense Search üìâ

üì±

### Deep Dives

[### How We Built Fast UPDATEs for the ClickHouse Column Store ‚Äì Part 1: Purpose-built Engines (15 minute read)](https://clickhouse.com/blog/updates-in-clickhouse-1-purpose-built-engines?utm_source=tldrdata)

ClickHouse speeds up row-level updates by treating them as inserts, using specialized engines like ReplacingMergeTree, CoalescingMergeTree, and CollapsingMergeTree. These engines rely on high insert throughput and background merges to efficiently handle updates, deletes, and upserts without rewriting data in place. The FINAL keyword enables query-time consolidation for up-to-date results before merges complete.

[### Crawling a Billion Web Pages in Just Over 24 Hours, in 2025 (17 minute read)](https://andrewkchan.dev/posts/crawler.html?utm_source=tldrdata)

This guide outlines the process and challenges of building a web crawler capable of crawling a billion web pages in 24 hours, focusing on a budget-friendly approach. Key insights include the importance of efficient parsing, the need for fault tolerance, and adherence to web politeness standards, while highlighting the shift in web content dynamics since earlier crawls. The final design involved a cluster of optimized nodes rather than a disaggregated system, reflecting a modernized approach to web crawling.

[### Policy Zones: How Meta Enforces Purpose Limitation at Scale in Batch Processing Systems (15 minute read)](https://engineering.fb.com/2025/07/23/security/policy-zones-meta-purpose-limitation-batch-processing-systems/?utm_source=tldrdata)

Meta's Privacy-Aware Infrastructure (PAI) and Policy Zones enable scalable, fine-grained purpose limitation and transparency across exabyte-scale batch and stream data processing systems by integrating runtime information flow control and Governable Data Annotations. Engineers can efficiently annotate, track, and enforce privacy policies through unified toolchains while maintaining high developer velocity. This framework minimizes data siloing by automating compliance for complex dependencies and ML workflows.

üöÄ

### Opinions & Advice

[### Understanding Dimension Importance Estimation (DIME) for Dense Information Retrieval through Code and Experimentation (18 minute read)](https://medium.com/@kswastik29/understanding-dimension-importance-estimation-dime-for-dense-information-retrieval-through-code-ce2a1996ef48?utm_source=tldrdata)

DIME (Dimension Importance Estimation) improves dense retrieval by removing low-signal dimensions from query embeddings without requiring retraining or reindexing. It supports four techniques: Magnitude (based on value size), PRF (centroid from top-k results), LLM (synthetic doc embedding), and Active Feedback (user interactions). PRF and LLM methods showed the best performance in tests.

[### Data Engineering in the Age of AI: Skills To Master Now (3 minute read)](https://thenewstack.io/data-engineering-in-the-age-of-ai-skills-to-master-now/?utm_source=tldrdata)

Data engineers must master real-time, event-driven streaming pipelines to support agentic AI systems, moving beyond traditional batch ETL and ML workflows. Key skills include designing low-latency architectures with Kafka and Flink, precise retrieval for vector search, robust feedback loops, and scalable, secure pipelines to ensure reliable AI performance.

[### The Semantic Layer, Where Subjectivity and Objectivity Meet (4 minute read)](https://www.datamanagementblog.com/the-semantic-layer-where-subjectivity-and-objectivity-meet/?utm_source=tldrdata)

The semantic layer in logical data management platforms bridges objectivity and subjectivity by decoupling data meaning from its technical structure, enabling shared, organization-wide definitions of concepts like "customer" at an objective level while supporting subjective interpretations tailored to roles like Sales or Marketing. This multilevel approach respects diverse perspectives and ensures clear, agreed-upon data representations.

üíª

### Launches & Tools

[### FastLanes (GitHub Repo)](https://github.com/cwida/FastLanes?utm_source=tldrdata)

This repository introduces a next-gen big data file format designed to enhance data storage and processing efficiency. Key features include optimized data compression, schema evolution support, and compatibility with popular big data tools, making it highly relevant for data engineers seeking to improve data handling and performance in large-scale systems.

[### Introducing MCP Server for Apache Spark History Server for AI-powered Debugging and Optimization (5 minute read)](https://aws.amazon.com/blogs/big-data/introducing-mcp-server-for-apache-spark-history-server-for-ai-powered-debugging-and-optimization/?utm_source=tldrdata)

The Spark History Server MCP, an open-source AWS tool, lets AI assistants analyze Spark data, turning complex debugging into conversational insights. It integrates with AWS EMR and Glue, enabling engineers to optimize Spark applications using natural language queries.

[### Parquet Content-Defined Chunking (16 minute read)](https://huggingface.co/blog/parquet-cdc?utm_source=tldrdata)

Parquet Content-Defined Chunking (CDC) is now integrated into PyArrow and Pandas, allowing data engineers to optimize file uploads and downloads on platforms like Hugging Face Hub. By leveraging CDC with the new Xet storage layer, users can achieve significant reductions in data transfer and storage costs, only handling changed data chunks, enhancing efficiency in data workflows.

[### FoundationDB: A Distributed Database That Can't Be Killed (7 minute read)](https://thenewstack.io/foundationdb-a-distributed-database-that-cant-be-killed/?utm_source=tldrdata)

FoundationDB, a NoSQL, open-source database, demonstrates exceptional resilience and self-healing capabilities when deployed on Kubernetes. Its microservices architecture, strict five-second transaction limits, and small key/value size caps inherently prevent disruptive queries and data bloat at scale. FoundationDB can be leveraged to reliably serve billions of global S3-compatible object storage metadata requests daily.

üéÅ

### Miscellaneous

[### Synthetic and Federated: Privacy-Preserving Domain Adaptation with LLMs for Mobile Applications (7 minute read)](https://research.google/blog/synthetic-and-federated-privacy-preserving-domain-adaptation-with-llms-for-mobile-applications/?utm_source=tldrdata)

Google has integrated privacy-preserving synthetic data techniques with federated learning and differential privacy to enhance both small and large language models for Gboard, delivering up to 22.8% NWP accuracy gains and 3%-13% improvements in key production metrics. Synthetic datasets, generated via public LLMs like Gemini and refined by privacy-tuned modules, enable domain adaptation without exposing user data. This approach streamlines model training pipelines, strengthens privacy guarantees, and boosts model quality and deployability for mobile applications at scale.

[### Why Python Pros Avoid Loops: A Gentle Guide to Vectorized Thinking (4 minute read)](https://www.kdnuggets.com/why-python-pros-avoid-loops-a-gentle-guide-to-vectorized-thinking?utm_source=tldrdata)

Vectorized thinking in Python enables faster and more readable code by processing entire data arrays simultaneously, avoiding slow, sequential "for" loops. For large datasets, vectorization is significantly more efficient. However, loops remain better for complex logic or small datasets.

‚ö°Ô∏è

### Quick Links

[### Atlassian's Inference Engine, Our Self-hosted AI Inference Service (7 minute read)](https://www.atlassian.com/blog/atlassian-engineering/inference-engine?utm_source=tldrdata)

Atlassian's AI platform runs LLMs in the cloud using Kubernetes and Triton, cutting LLM latency by 40%, non-LLM latency by 63%, and costs by up to 80%.

[### Optimizing Vector Search for Indexing and Real-Time Retrieval with NVIDIA cuVS (7 minute read)](https://developer.nvidia.com/blog/optimizing-vector-search-for-indexing-and-real-time-retrieval-with-nvidia-cuvs/?utm_source=tldrdata)

The latest NVIDIA cuVS release brings GPU-accelerated vector indexing and real-time retrieval, delivering up to 40x faster than CPU index builds and low-latency search with out-of-the-box integrations for FAISS, AlloyDB, Lucene, and more.

## Curated deep dives, tools and trends in big data, data science and data engineering üìä

Subscribe

Join 308,008 readers for [one daily email](/api/latest/data)

[Privacy](/privacy)[Careers](https://jobs.ashbyhq.com/tldr.tech)[Advertise](/data/advertise)

Timestamp: 1753749138