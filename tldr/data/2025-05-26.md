DuckDB 1.3 Release ü¶Ü, JSON-based Iceberg üßä, 100 ms real-time SLA ‚ö°

[TLDR](/)

[Newsletters](/newsletters)

[Advertise](https://advertise.tldr.tech/)

[TLDR](/)

# TLDR Data 2025-05-26

## DuckDB 1.3 Release ü¶Ü, JSON-based Iceberg üßä, 100 ms real-time SLA ‚ö°

üì±

### Deep Dives

[### Journey to 1,000 models: Scaling Instagram's recommendation system (8 minute read)](https://engineering.fb.com/2025/05/21/production-engineering/journey-to-1000-models-scaling-instagrams-recommendation-system/?utm_source=tldrdata)

Instagram scaled its recommendation system to over 1,000 machine learning models. It developed a portable model health definition and generic alerting to detect and fix issues faster. Capacity planning, offline performance evaluation, and an automated launching platform helped balance metric improvements with costs.

[### Showing Delivery Date at 10x Scale with 1/10th Latency (7 minute read)](https://blog.flipkart.tech/showing-delivery-sla-at-10x-scale-with-1-10th-latency-28b17b198cc8?utm_source=tldrdata)

Flipkart optimized delivery date calculations by decoupling Source and Vendor Capacity stores, caching source capacity in Aerospike with bit-encoded integers to reduce data size by 90%, and caching vendor capacity at the Vendor-Pincode level. These changes enabled real-time delivery date computation within 100ms latency on the Search and Browse page, achieving over 90% accuracy and supporting 10x query-per-second throughput.

[### Building a Local Data Platform with Terraform and Docker (16 minute read)](https://p-munhoz.github.io/blog/data-engineering/building-local-data-platform-terraform-docker?utm_source=tldrdata)

A guide to building a local data platform using Terraform and Docker, replicating cloud services like S3, Lambda, and Redshift at no cost. The setup involves containerized components like MinIO, Airflow, LocalStack, and DuckDB, enabling a local environment for learning, prototyping, and testing data engineering workflows. This solution offers an affordable and scalable way to experiment with modern data architecture and infrastructure as code principles.

[### Boring Iceberg Catalog ‚Äî 1 JSON file, 0 Setup (5 minute read)](https://juhache.substack.com/p/boring-iceberg-catalog?utm_source=tldrdata)

The Boring Catalog is a simple, serverless Apache Iceberg catalog implemented as a JSON file. Leveraging S3 conditional writes, it supports basic operations like appending Parquet files and integrates with DuckDB for querying. Designed for experimentation, it simplifies Iceberg catalog setup and aims to enhance CLI functionality for partition specs and schema evolution, with potential future support for a REST interface.

üöÄ

### Opinions & Advice

[### Transaction Log in Open Table Formats (3 minute read)](https://www.ssp.sh/brain/transaction-log-open-table-formats/?utm_source=tldrdata)

Open table formats like Delta Lake, Apache Iceberg, and Apache Hudi simplify data lake management by consolidating distributed files into structured tables with features like ACID transactions and schema evolution. The transaction log, such as Delta Lake's \_delta\_log, records all table transactions, enabling time travel to access historical data versions and supporting GDPR compliance. These formats optimize metadata handling and data compaction, using techniques like bin-packing and sorting to enhance query performance and scalability.

[### Archive Postgres Partitions to Iceberg (5 minute read)](https://www.crunchydata.com/blog/archive-postgres-partitions-to-iceberg?utm_source=tldrdata)

PostgreSQL's built-in partitioning, enhanced by pg\_partman, simplifies data management by retaining recent data for performance and dropping older data for cost efficiency. Seamlessly migrating older partitions to Apache Iceberg retains all data indefinitely, enabling long-term querying in a data warehouse while Postgres maintains the most recent 30 days as an operational database.

[### From Simple Requirement to Scalable Solution: Why ‚ÄúAsking Why‚Äù Matters in Software Engineering (4 minute read)](https://medium.com/@prashant.raghav/from-just-save-to-s3-to-scalable-search-how-asking-why-leads-to-better-engineering-9d96d37b2275?utm_source=tldrdata)

This article describes how to evolve a simple "save to S3" data storage approach into a scalable search system by questioning inefficiencies and implementing structured solutions like Apache Iceberg for better query performance. By addressing challenges like data organization and retrieval with open table formats, the system achieved scalability and efficiency for large-scale data processing.

üíª

### Launches & Tools

[### MindsDB (GitHub Repo)](https://github.com/mindsdb/mindsdb?utm_source=tldrdata)

MindsDB is an open-source platform designed to deliver answers from data scattered across disparate data sources. It handles query federation over structured and unstructured data sources, definition of unified views of knowledge bases, and natural language data queries. The server can be deployed via Docker or PyPI. It is available on the AWS Marketplace. A demo playground is available.

[### Announcing DuckDB 1.3.0 (7 minute read)](https://duckdb.org/2025/05/21/announcing-duckdb-130.html?utm_source=tldrdata)

DuckDB 1.3.0 "Ossivalis'" major features include a remote file cache that accelerates repeated queries on Parquet/CSV/JSON stored in cloud or HTTP sources, Python-style lambda syntax for clearer SQL, and direct CLI querying of external files with a temporary in-memory database. Key enhancements also include spatial join performance, unified multi-file reader APIs, improved Parquet support and compression, more robust UUIDv7 support, and enhanced column unpacking.

[### AlloyDB AI's ScaNN Index Boosts Vector and Hybrid Search Performance (5 minute read)](https://cloud.google.com/blog/products/databases/alloydb-ais-scann-index-improves-search-on-all-kinds-of-data/?utm_source=tldrdata)

AlloyDB AI now integrates Google Research's ScaNN index to accelerate vector similarity searches by up to 3x and enable hybrid queries combining keyword and embedding-based retrieval. By supporting both dense and sparse data types, as well as custom distance metrics, ScaNN delivers sub-millisecond lookup times at scale. This enhancement sharpens relevance and recall across use cases while preserving AlloyDB's managed, serverless simplicity and strong consistency guarantees.

üéÅ

### Miscellaneous

[### JUDE: LLM-based representation learning for LinkedIn job recommendations (7 minute read)](https://www.linkedin.com/blog/engineering/ai/jude-llm-based-representation-learning-for-linkedin-job-recommendations?utm_source=tldrdata)

LinkedIn's JUDE platform uses a two-tower model architecture and distilled models to generate high-quality embeddings for job recommendations. Real-time processing with Kafka and Samza pipelines handles trillions of events daily, enabling low-latency job matching. GPU-accelerated inference serves 7B-parameter LLMs, with outputs delivered to online Kafka sinks for real-time recommendations and offline Venice sinks for analytics.

[### The Role of the Data Architect in AI Enablement (15 minute read)](https://moderndata101.substack.com/p/the-role-of-the-data-architect?utm_source=tldrdata)

Data architects are pivotal to successful AI initiatives, ensuring robust foundations by aligning data strategy, governance, and domain knowledge with business needs. They oversee the delivery of targeted, business solutions integrating data governance, quality, and architecture for specific high-value use cases, rather than perfecting foundational layers in isolation. Prioritizing actionable impact over technical hype, data architects bridge business and technology, enabling sustainable AI systems that quantify business outcomes and adapt to ongoing change.

[### Irish privacy watchdog OKs Meta to train AI on EU folks' posts (2 minute read)](https://www.theregister.com/2025/05/22/irish_data_protection_commission_gives/?utm_source=tldrdata)

The Irish Data Protection Commission has authorized Meta to start using European user data for AI model training beginning next week, despite pending legal objections. This decision enables Meta to leverage vast amounts of regional data to enhance its AI capabilities, potentially accelerating model improvements.

‚ö°Ô∏è

### Quick Links

[### USING KEY in Recursive CTEs (8 minute read)](https://duckdb.org/2025/05/23/using-key.html?utm_source=tldrdata)

DuckDB 1.3 introduces key-based recursive CTEs via the USING KEY clause, substantially improving performance and memory efficiency for iterative SQL algorithms.

[### Making the Right Choice: Flink or Kafka Streams? (8 minute read)](https://medium.com/@getindatatechteam/making-the-right-choice-flink-or-kafka-streams-a84979cc3646?utm_source=tldrdata)

Choosing between Apache Flink and Kafka Streams hinges on workload complexity and operational priorities.

## Curated deep dives, tools and trends in big data, data science and data engineering üìä

Subscribe

Join 308,008 readers for [one daily email](/api/latest/data)

[Privacy](/privacy)[Careers](https://jobs.ashbyhq.com/tldr.tech)[Advertise](/data/advertise)

Timestamp: 1748305591