Netflix‚Äôs LLM Personalization Model üé¨, LinkedIn‚Äôs Rust-Based DB üêü, Postgres Health Check üè•

[TLDR](/)

[Newsletters](/newsletters)

[Advertise](https://advertise.tldr.tech/)

[TLDR](/)

# TLDR Data 2025-11-20

## Netflix‚Äôs LLM Personalization Model üé¨, LinkedIn‚Äôs Rust-Based DB üêü, Postgres Health Check üè•

üì±

### Deep Dives

[### FishDB: a Generic Retrieval Engine for Scaling LinkedIn's Feed (12 minute read)](https://www.linkedin.com/blog/engineering/infrastructure/fishdb-a-generic-retrieval-engine-for-scaling-linkedins-feed?utm_source=tldrdata)

LinkedIn's FishDB is a Rust-based, generic retrieval engine optimized for recommender systems like feeds. It employs a scatter-gather architecture with a broker distributing queries across 48 sharded partitions (16 replicas each) powered by a lambda-architecture ingestion pipeline, in-memory inverted/forward/reference indexes optimized for low indirection and copy-on-write updates, RocksDB-backed attribute stores, and a Volcano-style query engine with tree-walk interpretation for complex expressions.

[### Integrating Netflix's Foundation Model into Personalization applications (7 minute read)](https://netflixtechblog.medium.com/integrating-netflixs-foundation-model-into-personalization-applications-cf176b5860eb?utm_source=tldrdata)

Netflix has centralized its personalization efforts with a large Foundation Model, streamlining user preference learning and supporting three production integration patterns: batch-refreshed embeddings via an Embedding Store, subgraph integration for real-time inference, and customized fine-tuned model deployments. The embeddings approach offers scalable, low-latency access but can suffer from staleness, while subgraph integration unlocks deeper personalization at higher complexity and compute cost. The modular framework enables data teams to tailor recommendations to diverse application constraints.

[### How Dash uses context engineering for smarter AI (5 minute read)](https://dropbox.tech/machine-learning/how-dash-uses-context-engineering-for-smarter-ai?utm_source=tldrdata)

Dropbox improved Dash's agentic performance by consolidating many retrieval tools into a single unified ‚ÄúDash Search‚Äù tool, filtering results at runtime using a knowledge graph to deliver only highly relevant context, and delegating complex subtasks like query construction to a specialized search agent. These three context-engineering strategies reduce noise and tool sprawl, prevent context overload, and balance token usage, cost, latency, and reliability.

üöÄ

### Opinions & Advice

[### The Network is the Product: Data Network Flywheel, Compound Through Connection (7 minute read)](https://moderndata101.substack.com/p/the-data-network-flywheel?utm_source=tldrdata)

Data value compounds not through isolated products, but via interconnected data ecosystems where feedback loops drive continual learning and intelligence. Transitioning from siloed models to a networked ‚ÄúData Flywheel‚Äù amplifies value, speed, and trust, as every new data product, user context, and global quality protocol reinforce system-wide outcomes. Prioritizing connection density, context-driven design, and distributed quality assurance turns data platforms into self-accelerating engines of innovation and actionable insight.

[### Tips for Building Knowledge Graphs (15 minute read)](https://ontologist.substack.com/p/tips-for-building-knowledge-graphs?utm_source=tldrdata)

Knowledge graphs offer distinct advantages over traditional relational databases for modeling highly interconnected and complex domains, especially beyond the 30-table threshold. They simplify schema evolution, enable advanced inferencing via standards like OWL and SHACL, and streamline business logic by embedding process knowledge directly into the data layer. Integrating knowledge graphs with LLMs via structured APIs enhances security and query expressivity. However, the primary challenge (and cost driver) remains acquiring, structuring, and maintaining high-quality, domain-relevant data.

[### Why Strong Consistency? (6 minute read)](https://brooker.co.za/blog/2025/11/18/consistency.html?utm_source=tldrdata)

Eventual consistency, while useful for rare low-latency trade-offs, complicates high-availability services by demanding sophisticated routing, error-handling, and testing. Aurora DSQL delivers strong consistency across all replicas by combining monotonic journal updates with timestamp-based queries, where replicas simply wait for all prior writes to be applied, so developers can write straightforward code rather than consistency hacks.

üíª

### Launches & Tools

[### dbt's new Fusion Engine for smarter, cost-effective data ops (Sponsor)](https://www.getdbt.com/resources/webinars/how-the-dbt-fusion-engine-optimizes-data-work/?utm_medium=paid-email&amp;utm_source=tldr&amp;utm_campaign=q4-2026_tldr-newsletters_cv&amp;utm_content=_newsletter3___&amp;utm_term=all_all__)

Data teams face an impossible choice: move fast and explode cloud costs, or manage spend and sacrifice quality. The new dbt Fusion engine eliminates this tradeoff with state-aware orchestration that skips unchanged models and tests automatically, achieving [29% efficiency gains](https://www.getdbt.com/resources/webinars/how-the-dbt-fusion-engine-optimizes-data-work/?utm_medium=paid-email&utm_source=tldr&utm_campaign=q4-2026_tldr-newsletters_cv&utm_content=_newsletter3___&utm_term=all_all__) while maintaining data freshness. For a closer look, [join the live session (December 3 / 4)](https://www.getdbt.com/resources/webinars/how-the-dbt-fusion-engine-optimizes-data-work/?utm_medium=paid-email&utm_source=tldr&utm_campaign=q4-2026_tldr-newsletters_cv&utm_content=_newsletter3___&utm_term=all_all__) and hear how Fusion is helping Obie Insurance and Analytics8 move toward faster pipelines and reduced waste.

[### sqlmap (GitHub Repo)](https://github.com/sqlmapproject/sqlmap?utm_source=tldrdata)

sqlmap is an open-source tool for automating SQL-injection discovery and exploitation across all major databases. It supports six injection techniques (boolean-based blind, time-based blind, error-based, UNION query-based, stacked queries, and out-of-band) while testing full DB fingerprinting, data extraction, file operations, and OS-level command execution when privileges allow.

[### DuckDB Internals - Part 4: Optimizer Overview (21 minute read)](https://www.alibabacloud.com/blog/duckdb-internals---part-4-optimizer-overview_602677?utm_source=tldrdata)

DuckDB's optimizer is a sophisticated, extensible component central to its OLAP performance, transforming unoptimized logical plans into efficient ones via rule-based transformations. Encapsulated in the Optimizer class, it applies 26 built-in rules to simplify expressions, reorder operations, and push down filters. The optimizer supports for plugins via OptimizerExtension for custom pre/post-optimization hooks.

[### Ax 1.0: Efficient Optimization With Adaptive Experimentation (5 minute read)](https://engineering.fb.com/2025/11/18/open-source/efficient-optimization-ax-open-platform-adaptive-experimentation/?utm_source=tldrdata)

The open-source platform Ax powers adaptive optimization for large-scale ML systems at Meta, replacing brute-force searches (grid/random) with Bayesian and sequential methods for hyperparameters, metrics, and system tuning. It supports complex constraints, noisy observations, parallel suggestions, and early stopping. A research paper detailing the system's architecture, features, and performance is linked in the article.

[### pgFirstAid (GitHub Repo)](https://github.com/randoneering/pgFirstAid?utm_source=tldrdata)

pgFirstAid is a lightweight, single-function PostgreSQL health check that instantly returns prioritized performance and stability issues with recommended fixes. It covers key areas like missing primary keys, bloat, outdated statistics, and inefficient indexes. pgFirstAid is safe to run in production. It's designed for anyone to use, not just DBAs.

üéÅ

### Miscellaneous

[### Training a Tokenizer for BERT Models (4 minute read)](https://machinelearningmastery.com/training-a-tokenizer-for-bert-models/?utm_source=tldrdata)

Training a custom WordPiece tokenizer for BERT using Hugging Face's tokenizers and datasets libraries involves loading a corpus, training the tokenizer from an iterator with a 30,522-word vocabulary and BERT special tokens, enabling padding/truncation, and saving the final tokenizer for testing and downstream BERT fine-tuning.

[### How Can You Identify an Agentic AI Use Case? (10 minute read)](https://medium.com/data-from-the-trenches/how-can-you-identify-an-agentic-ai-use-case-b95b3fa45600?utm_source=tldrdata)

Agentic AI can automate complex, reasoning-heavy tasks that are repetitive, expert-dependent, or involve scattered/unstructured data, dramatically cutting human effort, provided the scope is clearly bounded, tools are well-defined (potentially with subagents), and sufficient upfront documentation is invested to eliminate ambiguity and prevent incomplete automation.

‚ö°Ô∏è

### Quick Links

[### All You Can Do Before Airflow (5 minute read)](https://dataengineeringcentral.substack.com/p/all-you-can-do-before-airflow?utm_source=tldrdata)

Start with simple orchestration and scale only when complexity demands it.

[### State, Scale, and Signals: Rethinking Orchestration with Durable Execution (52 minute podcast)](https://www.dataengineeringpodcast.com/durable-execution-data-ai-orchestration-episode-489?utm_source=tldrdata)

Durable execution shifts distributed system reliability from an application concern to a platform guarantee.

## Curated deep dives, tools and trends in big data, data science and data engineering üìä

Subscribe

Join 400,000 readers for [one daily email](/api/latest/data)

[Privacy](/privacy)[Careers](https://jobs.ashbyhq.com/tldr.tech)[Advertise](/data/advertise)

Timestamp: 1763651240