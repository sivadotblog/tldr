Discipline Wins in 2026 üß±, Live SQL Observability üëÄ, Open Source MySQL Alternative üîÑ

[TLDR](/)

[Newsletters](/newsletters)

[Advertise](https://advertise.tldr.tech/)

[TLDR](/)

# TLDR Data 2026-02-16

## Discipline Wins in 2026 üß±, Live SQL Observability üëÄ, Open Source MySQL Alternative üîÑ

üì±

### Deep Dives

[### Automating RDS Postgres to Aurora Postgres Migration (21 minute read)](https://netflixtechblog.com/automating-rds-postgres-to-aurora-postgres-migration-261ca045447f?utm_source=tldrdata)

Netflix automated the migration of 400 RDS PostgreSQL clusters to Aurora PostgreSQL using a self-service, credential-free workflow that leverages Aurora read replicas for near-zero data loss and minimal downtime. The process involved creating an Aurora read replica for continuous WAL streaming, validating replication lag, and promoting the replica during cutover.

[### Cloud-Native Storage Engine: How GreptimeDB Cuts IoT Storage Costs by 10x (5 minute read)](https://emqx.com/en/blog/cloud-native-storage-engine-how-greptimedb-cuts-iot-storage-costs-by-10x?utm_source=tldrdata)

GreptimeDB is an open-source, real-time observability database with decoupled compute and storage that cuts IoT time-series storage costs by up to 10x versus traditional EBS-backed databases by using object storage and write-optimized LSM trees. It stores data in columnar Parquet with ztsd compression and delta encoding, combines multi-level caching for disk-speed access to hot data, and delivers up to 4x higher write throughput and 10x faster queries for high-cardinality workloads.

[### Building Embedding Models for Large-Scale Real-World Applications (44 minute presentation)](https://www.infoq.com/presentations/llm-large-scale-applications/?utm_source=tldrdata)

Building embedding models consists of multiple steps, from contrastive learning, distillation, evaluation, to serving via document indexing and approximate nearest-neighbor search in vector DBs with optimization on query latency (batching, quantization). Reducing hallucinations in RAG, handling real-time vs. offline pipelines, and mitigating staleness are key to serving embeddings at scale in production.

üöÄ

### Opinions & Advice

[### Introducing Logical-First Architecture, the Real ‚ÄúZero-Copy‚Äù Architecture (4 minute read)](https://www.datamanagementblog.com/introducing-logical-first-architecture-the-real-zero-copy-architecture/?utm_source=tldrdata)

A Logical-First Architecture is a true zero-copy approach that prioritizes logical access over physical data movement, leaving data in its original sources while enabling unified views, federation, and querying without duplication. Unlike traditional ETL/ELT pipelines or lakehouses that replicate data, it avoids storage bloat, latency, governance complexity, and unnecessary cost.

[### Where Data Engineering Is Heading in 2026 - 5+ Trends (6 minute read)](https://joereis.substack.com/p/where-data-engineering-is-heading?utm_source=tldrdata)

In 2026, data engineering is splitting into two clear camps: elite teams that stick to solid foundations, while others drown in mounting technical debt. AI is supercharging both disciplined progress and reckless shortcuts, with orchestration platforms likely consolidating or getting swallowed by bigger ecosystems, AI becoming a core part of every workflow, and the distinction between data warehouses and lakehouses continuing to blur.

[### Why Declarative (Lakeflow) Pipelines Are the Future of Spark (5 minute read)](https://www.confessionsofadataguy.com/why-declarative-lakeflow-pipelines-are-the-future-of-spark/?utm_source=tldrdata)

Declarative Lakeflow Pipelines mirror dbt's success in imposing order on SQL workflows by shifting from chaotic, imperative code to a structured, declarative framework that lets engineers define data flows, datasets, and pipelines in SQL or Python, with Spark handling execution, orchestration, reliability, and deployment automatically.

üíª

### Launches & Tools

[### sql-tap (GitHub Repo)](https://github.com/mickamy/sql-tap?utm_source=tldrdata)

sql-tap is a lightweight SQL proxy plus terminal UI that lets you watch all PostgreSQL or MySQL queries from your app in real time, without changing any code. It sits between your app and the database to capture queries, transactions, timings, and EXPLAIN output in an interactive TUI.

[### VillageSQL Server (GitHub Repo)](https://github.com/villagesql/villagesql-server?utm_source=tldrdata)

VillageSQL Server is an open-source tracking fork of MySQL 8.4.6 LTS that serves as a drop-in replacement while introducing the VillageSQL Extension Framework (VEF), enabling developers to create, bundle, and dynamically load custom data types and high-performance functions directly into the database using SQL commands. Positioned as the innovation platform for MySQL in the agentic AI era, it supports domain-specific logic, AI integrations, and extensibility.

[### Introducing Polyglot - A Rust/Wasm SQL Transpilation Library (6 minute read)](https://tobilg.com/posts/introducing-polyglot-a-rust-wasm-sql-transpilation-library/?utm_source=tldrdata)

Polyglot is a Rust-based SQL transpilation library, inspired by Python's SQLGlot and compiled to WebAssembly, that enables fast, portable parsing, transpilation, generation, formatting, validation, and programmatic building of SQL queries across 33 database dialects directly in browsers, Node.js, or native Rust environments.

[### What's New in Croissant 1.1: Extensible, Agent-Ready ML Dataset Standard (3 minute read)](https://mlcommons.org/2026/02/croissant-1-1-standard/?utm_source=tldrdata)

Croissant is a community metadata format for machine learning datasets that extends the MLCommons standard with machine-actionable provenance via W3C PROV-O, interoperable domain vocabularies, and automated governance using DUO and ODRL. It enables end-to-end lineage, automated consent and license enforcement, and rich data modeling, with native support across 700,000+ datasets and major ML frameworks including TensorFlow, PyTorch, Dataverse, and CKAN.

üéÅ

### Miscellaneous

[### Context Graphs: Building Production World Models for the Age of AI Agents (17 minute read)](https://hackernoon.com/context-graphs-building-production-world-models-for-the-age-of-ai-agents?utm_source=tldrdata)

Current AI and production systems excel at storing present state but fail to capture the underlying decision logic (why and how key actions occurred), creating organizational knowledge gaps. Context graphs are emerging as the solution, integrating decision traces, evidence, constraints, and outcomes across fragmented systems and roles into a unified, replayable organizational ‚Äúworld model.‚Äù Unlike traditional ontologies, context graphs leverage agent-driven trajectories to dynamically discover implicit relationships and operational heuristics, enabling true simulation and auditability of decisions.

[### How low-bit inference enables efficient AI (6 minute read)](https://dropbox.tech/machine-learning/how-low-bit-inference-enables-efficient-ai?utm_source=tldrdata)

Dropbox applied low-bit inference via quantization to lower the precision of weights and activations for Dash's multimodal features, choosing strategies (weight-only vs. activation quantization) based on workload type. Its team used post-training adjustments (especially for MXFP4) and custom kernels to achieve low latency, high reliability, and cost savings while meeting real-world constraints.

‚ö°Ô∏è

### Quick Links

[### Python Data Engineering Claude Skills (GitHub Repo)](https://github.com/honnibal/claude-skills?utm_source=tldrdata)

This repository contains a set of reusable Claude Code slash commands that help data engineers systematically harden Python data pipelines and libraries.

[### Fast Lane to Insights: Slash Debezium Snapshot Duration by 70% (6 minute read)](https://medium.com/guidewire-engineering-blog/fast-lane-to-insights-slash-debezium-snapshot-duration-by-70-8eadafb7f63a?utm_source=tldrdata)

Guidewire reduced Debezium snapshot times for a 7TB PostgreSQL database from 68.5 to 20 hours without impacting production workloads by leveraging AWS Aurora's Copy-on-Write database cloning, intelligent load balancing, and Timefold-powered constraint-based partitioning.

## Curated deep dives, tools and trends in big data, data science and data engineering üìä

Subscribe

Join 400,000 readers for [one daily email](/api/latest/data)

[Privacy](/privacy)[Careers](https://jobs.ashbyhq.com/tldr.tech)[Advertise](/data/advertise)

Timestamp: 1771255125