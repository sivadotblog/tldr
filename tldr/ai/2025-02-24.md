DeepSeek open source week üåê, OpenAI‚Äôs shift from Microsoft to SoftBank üëã, Meta‚Äôs DINOv2 ü¶ñ

[TLDR](/)

[Newsletters](/newsletters)

[Advertise](https://advertise.tldr.tech/)

[TLDR](/)

# TLDR AI 2025-02-24

## DeepSeek open source week üåê, OpenAI‚Äôs shift from Microsoft to SoftBank üëã, Meta‚Äôs DINOv2 ü¶ñ

üöÄ

### Headlines & Launches

[### Meta's DINOv2 for Cancer Research (3 minute read)](https://ai.meta.com/blog/orakl-oncology-dinov2-accelerating-cancer-treatment/?utm_source=tldrai)

Orakl Oncology uses Meta's DINOv2 model to accelerate cancer drug discovery, improving efficiency by quickly analyzing organoid images to predict patient treatment responses.

[### DeepSeek to open source parts of online services code (2 minute read)](https://techcrunch.com/2025/02/21/deepseek-to-open-source-parts-of-online-services-code/?utm_source=tldrai)

Chinese AI lab DeepSeek plans to open source portions of its online services' code as part of an ‚Äúopen source week‚Äù event.

[### OpenAI's Shift from Microsoft to SoftBank (1 minute read)](https://techcrunch.com/2025/02/21/report-openai-plans-to-shift-compute-needs-from-microsoft-to-softbank/?utm_source=tldrai)

OpenAI plans to source most of its computing power from SoftBank's Stargate project by 2030, marking a significant departure from its reliance on Microsoft.

üß†

### Research & Innovation

[### SigLIP2 (18 minute read)](https://github.com/google-research/big_vision/blob/main/big_vision/configs/proj/image_text/README_siglip2.md?utm_source=tldrai)

SigLIP was an immensely popular joint image and text encoder model. It has now been improved in a number of axes. Most notable is the substantially improved zero-shot classification performance, which was the hallmark result of the original CLIP work.

[### Step-Level Calibration for LLM Agents (16 minute read)](https://arxiv.org/abs/2502.14276v1?utm_source=tldrai)

STeCa is a novel framework designed to improve LLM agents in long-horizon tasks by automatically identifying and correcting suboptimal actions.

[### GemmaX2 Translation Model (Hugging Face Hub)](https://huggingface.co/ModelSpace/GemmaX2-28-2B-v0.1?utm_source=tldrai)

With modern post-training techniques, this 2B model trained on Gemma achieves state-of-the-art translation performance between 28 different languages.

üë®‚Äçüíª

### Engineering & Resources

[### Moonlight 16B Muon trained model (GitHub Repo)](https://github.com/MoonshotAI/Moonlight?utm_source=tldrai)

This is the first (public) large scale model trained with the Muon optimizer. It was trained for 5.7T tokens and is a very similar architecture to DeepSeek v3.

[### Triton implementation of Naive Sparse Attention (GitHub Repo)](https://github.com/fla-org/native-sparse-attention?utm_source=tldrai)

The DeepSeek NSA paper made waves last week for its scalable and efficient long context attention algorithm. However, there was no code available. This work is a Triton replication that can be slotted into any PyTorch codebase.

[### LLM Deployment with OmniServe (GitHub Repo)](https://github.com/mit-han-lab/omniserve?utm_source=tldrai)

OmniServe offers a unified framework for efficient large-scale LLM deployment, combining innovations in low-bit quantization and sparse attention to enhance both speed and cost-effectiveness.

üéÅ

### Miscellaneous

[### CUDA for Python Programmers (35 minute read)](https://www.pyspur.dev/blog/introduction_cuda_programming?utm_source=tldrai)

A great introduction to CUDA programming for those familiar with Python programming.

[### US AI Safety Institute Budget Cuts (11 minute read)](https://www.hpbl.co.in/market/us-ai-safety-institute-could-face-big-cuts-implications-challenges-and-future-prospects/?utm_source=tldrai)

This article explores the potential impacts of funding cuts to the US AI Safety Institute, including implications for national security, AI research, and international competition.

[### Microsoft prepares for OpenAI's GPT-5 model (9 minute read)](https://www.theverge.com/notepad-microsoft-newsletter/616464/microsoft-prepares-for-openais-gpt-5-model?utm_source=tldrai)

Microsoft is preparing to host OpenAI's GPT-4.5 model as early as next week, with a more significant GPT-5 release anticipated by late May. The GPT-5 system will integrate OpenAI's new o3 reasoning model, aiming to create a unified AI functionality. Both releases align with key tech events, such as Microsoft Build and Google I/O, underscoring Microsoft's strategic positioning in the AI space.

‚ö°Ô∏è

### Quick Links

[### Parallelizing Muon (7 minute read)](https://main-horse.github.io/posts/parallelizing-muon/?utm_source=tldrai)

Various novel strategies to parallelize the up-and-coming Muon optimizer.

[### ChatGPT reaches 400M weekly active users (1 minute read)](https://www.engadget.com/ai/chatgpt-reaches-400m-weekly-active-users-203635884.html?utm_source=tldrai)

ChatGPT has reached 400 million weekly active users, doubling its count since August 2024.

[### Google's AI Co-scientist is 'test-time scaling' on steroids. What that means for research (5 minute read)](https://www.zdnet.com/article/googles-ai-co-scientist-is-test-time-scaling-on-steroids-what-that-means-for-research/?utm_source=tldrai)

Google has enhanced its Gemini 2.0 LLM to generate scientific hypotheses significantly faster than human researchers.

## The most important AI, ML, and data science news in a free daily email.

Subscribe

Join 500,000 readers for one daily email

[Privacy](/privacy)[Careers](https://jobs.ashbyhq.com/tldr.tech)[Advertise](/ai/advertise)

Timestamp: 1744590668