Brave Search launches AI summarization üîç, 100 most cited AI papers in 2022 üíØ, applying LLMs to robots ü§ñ

[TLDR](/)

[Newsletters](/newsletters)

[Advertise](https://advertise.tldr.tech/)

[TLDR](/)

# TLDR AI 2023-03-06

## Brave Search launches AI summarization üîç, 100 most cited AI papers in 2022 üíØ, applying LLMs to robots ü§ñ

üöÄ

### Headlines & Launches

[### 100 most cited AI papers in 2022 (6 minute read)](https://www.zeta-alpha.com/post/must-read-the-100-most-cited-ai-papers-in-2022?utm_source=tldrai)

Google still holds the top spot in AI research and UC Berkeley has that honor among academic groups. Given the volume of papers from groups like Google and Deepmind, it is impressive to see much smaller groups have far fewer but potentially more impactful papers.

[### 200-year-old math opens up AI‚Äôs mysterious black box (4 minute read)](https://spectrum.ieee.org/black-box-ai?utm_source=tldrai)

Whether it‚Äôs designing microchips or dreaming up new proteins, sometimes it seems like neural networks can do anything. Infamously, however, these brain-inspired AI systems work in mysterious ways, raising concerns that what they are doing might not make any sense. New research suggests that 200-year-old math could help shed light on how neural networks perform complex tasks such as predicting climate or modeling turbulence, a new study finds. This in turn could help boost the accuracy of neural networks and the speed at which they learn, researchers say.

[### Brave Search launches an AI-powered summarization feature (3 minute read)](https://techcrunch.com/2023/03/02/brave-search-launches-an-ai-powered-summarization-feature/?utm_source=tldrai)

It is raining AI-powered features all across search engines. Brave Search launched a new ‚ÄúSummarizer‚Äù feature, which is powered by different large language models (LLMs) ‚Äî OpenAI‚Äôs GPT tech isn‚Äôt one of them. Just like the name suggests, its job is to provide a synopsis of a search query using different sources. The summary feature is available to all Brave Search users on desktop and mobile ‚Äî accessible through any browser.

üß†

### Research & Innovation

[### A better of understanding of diffusion through the lens of ELBO maximization (22 minute read)](https://arxiv.org/abs/2303.00848?utm_source=tldrai)

Diffusion is currently one of the best generative methods in terms of sample quality. This comes from the nice properties of the loss and denoising process. In the community, different weighting schemes are used for noise per step which leads to different empirical results. It turns out that these non-uniform weighting schemes can be understood from the lens of likelihood maximization. This is a great step towards a deeper theoretical understanding of this modern workhorse.

[### Guide text generation with grounding for robotic control (7 minute read)](https://grounded-decoding.github.io/?utm_source=tldrai)

Applying large language models to robots is challenging due to their lack of experience with the physical world. To overcome this, a guided decoding strategy is used to construct an action sequence that is both likely according to the language model and realizable in the environment. This strategy solves complex tasks in a robotic setting by leveraging the knowledge of both models. They compare their work to SayCan.

[### Unlimited-Size Diffusion Restoration (12 minute read)](https://arxiv.org/abs/2303.00354v1?utm_source=tldrai)

This paper discusses the use of diffusion models for zero-shot image restoration and proposes a method to handle images of arbitrary sizes while maintaining the excellent characteristics of zero-shot. The proposed method, called Mask-Shift Restoration, addresses local incoherence, and Hierarchical Restoration alleviates out-of-domain issues. These simple, parameter-free approaches can be used not only for image restoration but also for image generation of unlimited sizes.

üë®‚Äçüíª

### Engineering & Resources

[### Interested in speeding up multiGPU training? Gradient accumulation is your bottleneck (6 minute read)](https://muellerzr.github.io/blog/gradient_accumulation.html?utm_source=tldrai)

Distributed Pytorch can be immensely slow if you‚Äôre not careful. When running a model on multiple GPUs, you take the average of all the gradient updates at a predefined interval. However, if you‚Äôre not careful, your code will be synchronizing across GPUs for every backwards pass.

[### Context Clusters: A New Paradigm for Visual Representation (Github Repo)](https://github.com/ma-xu/context-cluster?utm_source=tldrai)

This work introduces Context Clusters (CoCs), a new paradigm for visual representation that views an image as a set of unorganized points and extracts features via a simplified clustering algorithm. CoCs are convolution- and attention-free and only rely on clustering for spatial interaction. Despite not targeting state-of-the-art performance, CoCs achieve comparable or even better results than ConvNets or ViTs on several benchmarks.

[### Early Dropout for Mitigating Underfitting in Neural Networks (Github Repo)](https://github.com/facebookresearch/dropout?utm_source=tldrai)

The study shows that early dropout, applied only during the initial phases of training, can mitigate underfitting by reducing the directional variance of gradients and aligning them with the entire dataset‚Äôs gradient. The proposed method consistently improves generalization accuracy in various vision tasks and encourages further research on regularization in deep learning.

üéÅ

### Miscellaneous

[### The Inside Story Of How ChatGPT Was Built (10 minute read)](https://archive.ph/Dk7ka?utm_source=tldrai)

To get the inside story behind the ChatGPT ‚Äî how it was made, how OpenAI has been updating it since release, and how its makers feel about its success ‚Äî the author talked to four people who helped build it. They are Sandhini Agarwal (works on policy at OpenAI), Liam Fedus (scientist who worked on ChatGPT), John Schulman (cofounder of OpenAI), and Jan Leike (leader of OpenAI‚Äôs alignment team)

[### Is OpenAI making money off ChatGPT API (1 minute read)](https://threadreaderapp.com/thread/1631485296754987014.html?utm_source=tldrai)

With some assumptions, it seems like the newest GPT-Turbo model might still be massively profitable despite the steep drop in price.

[### Why AI Won‚Äôt Cause Unemployment (5 minute read)](https://pmarca.substack.com/p/why-ai-wont-cause-unemployment?utm_source=tldrai)

In this article, Marc Andreessen makes the case that similar to other previously new technologies, AI will not cause mass unemployment.

‚ö°Ô∏è

### Quick Links

[### There‚Äôs an AI for that (1 minute read)](https://theresanaiforthat.com/?utm_source=tldrai)

An extensive list of many useful AI tools being built today for various applications. A great place to gather resources or inspiration.

[### OpenAI-Python (GitHub Repo)](https://github.com/openai/openai-python?utm_source=tldrai)

OpenAI-Python provides access to the OpenAI API from applications written in Python.

[### OpenAI PHP (GitHub Repo)](https://github.com/openai-php/client?utm_source=tldrai)

OpenAI PHP is a PHP API client that allows you to interact with the OpenAI AI API.

[### Machine Learning Notes (GitHub Repo)](https://github.com/rasbt/machine-learning-notes?utm_source=tldrai)

Machine Learning Notes is a collection of useful machine learning codes and snippets.

## The most important AI, ML, and data science news in a free daily email.

Subscribe

Join 500,000 readers for one daily email

[Privacy](/privacy)[Careers](https://jobs.ashbyhq.com/tldr.tech)[Advertise](/ai/advertise)

Timestamp: 1744590543