Google rushing to add AI to search ü§ñ, world‚Äôs simplest text-to-image model üñºÔ∏è, OpenAI consistency models code üßë‚Äçüíª

[TLDR](/)

[Newsletters](/newsletters)

[Advertise](https://advertise.tldr.tech/)

[TLDR](/)

# TLDR AI 2023-04-17

## Google rushing to add AI to search ü§ñ, world‚Äôs simplest text-to-image model üñºÔ∏è, OpenAI consistency models code üßë‚Äçüíª

üöÄ

### Headlines & Launches

[### World's simplest text-to-image model? (6 minute read)](https://laion.ai/blog/paella/?utm_source=tldrai)

Laion just released the Paella image generation model. It uses a discrete, token based, latent space and a cross entropy denoising objective. One goal of this project is to make an image synthetics model that almost anyone can understand. Their sampling code is only 12 lines long!

[### Princeton launches center for language and intelligence (2 minute read)](https://climbing-mantis.10web.site/?utm_source=tldrai)

Recent progress in large language models has been dominated by large industry players. However, many of the key ideas still come from academia. Princeton is hoping to continue contributions to language and intelligence with this new center. They‚Äôre hiring for research scientists, engineers, and postdoc scholars.

[### Google Rushing To Add AI To Search Engine (5 minute read)](https://www.nytimes.com/2023/04/16/technology/google-search-engine-ai.html?utm_source=tldrai)

Google is feeling the heat from AI competitors like the new Bing, as Samsung considers making Bing its default search engine. Google's search business, which was worth $162 billion last year, could take a significant hit if Samsung decides to switch. In response, Google is working on an AI-powered search engine project called Magi to provide a more personalized experience. While details on the new search technology are still under wraps, Google has over 160 people working on the project, with plans to roll out new features to the public in the coming months.

üß†

### Research & Innovation

[### Scaling, Emergence, and Reasoning in LLMs (6 minute read)](https://docs.google.com/presentation/d/1EUV7W7X_w0BDrscDhPg7lMGzJCkeaPkGCJ3bN8dluXc/edit?resourcekey=0-7Nz5A7y8JozyVrnDtcEKJA#slide=id.g16197112905_0_0?utm_source=tldrai)

A presentation given by Jason Wei about how language model performance changes with scale. It‚Äôs a lovely discussion about emergent capabilities in these models and what we can do with those new skills. One interesting take-away is that we haven‚Äôt come close to exhausting all possible tasks, so there could be a variety of skills in the model that we haven‚Äôt uncovered.

[### So long hyperparameters (38 minute read)](https://arxiv.org/abs/2304.05187?utm_source=tldrai)

What if we didn‚Äôt have to tune the pile of hyperparameters we do to make deep learning work? This is a pretty technical paper that presents Automatic Gradient Descent which is a new optimizer that takes into account network architectures and eliminates a set of previously required hyperparameters. They work through all the math thoroughly, and the resulting optimizer seems to be fairly simple. They don‚Äôt have it working yet for all network architectures, but that feels like only a matter of time.

[### DINOv2: Learning Robust Visual Features without Supervision (18 minute read)](https://arxiv.org/abs/2304.07193?utm_source=tldrai)

Recent breakthroughs in natural language processing have paved the way for foundation models in computer vision, producing all-purpose visual features through self-supervised methods and large, diverse datasets. By utilizing an automatic pipeline for curated data and training a 1-billion-parameter ViT model, researchers have created smaller models that surpass existing all-purpose features in most benchmarks at image and pixel levels.

üë®‚Äçüíª

### Engineering & Resources

[### OpenAI consistency models code (GitHub Repo)](https://github.com/openai/consistency_models?utm_source=tldrai)

We wrote recently about the new type of genitive model from OpenAI that is faster than diffusion and more stable than a GAN. They just released code and model checkpoints on a set of small experimental data sets. This model is a cool next step in generative image modeling, and now we can play around with it!

[### SpectFormer: Frequency and Attention is what you need in a Vision Transformer (3 minute read)](https://badripatro.github.io/SpectFormers/?utm_source=tldrai)

Combining spectral and multi-headed attention layers in the novel SpectFormer architecture improves transformer performance for image recognition tasks. This approach achieves state-of-the-art accuracy on ImageNet-1K and consistently strong results in transfer learning, object detection, and instance segmentation tasks across various datasets.

[### SVMs are better than KNNs for retrieval (Jupyter Notebook)](https://github.com/karpathy/randomfun/blob/master/knn_vs_svm.ipynb?utm_source=tldrai)

The former head of AI from Tesla Andrej Karpathy has a gift for teaching difficult concepts. This notebook shows how to use SVMs instead of KNNs to find matches in an embedding index. There is a lot of talk around using embeddings to solve hallucination and this is a concrete example on how to improve performance generally with a different traditional ML method.

üéÅ

### Miscellaneous

[### Cost to train Anthropic's Next model (4 minute read)](https://orenleung.com/anthropic-claude-next-cost?utm_source=tldrai)

Anthropic recently announced plans to spend a billion dollars on training a massive language model. They announced this plan before their strategic partnership with Google, which may offer cheaper compute via their TPU pods. In any case it seems like one of the largest expenses of this endeavor is actually AI researcher salaries, followed closely by compute costs.

[### OpenAssistant‚Äôs huge chat dataset (HuggingFace Dataset)](https://huggingface.co/datasets/OpenAssistant/oasst1?utm_source=tldrai)

There are 161,443 messages with well over 400k quality annotations such as toxicity and quality. This open dataset is intended to help researchers working on alignment of language models.

[### 91% Of ML Models Degrade Over Time (6 minute read)](https://www.nannyml.com/blog/91-of-ml-perfomance-degrade-in-time?utm_source=tldrai)

A study reveals that 91% of machine learning models experience performance degradation over time, emphasizing the need for ongoing monitoring, maintenance, and adaptation to ensure these models remain effective and relevant in dynamic environments.

‚ö°Ô∏è

### Quick Links

[### GPT5 Is Not In Training Yet (1 minute read)](https://gizmodo.com/sam-altman-open-ai-chatbot-gpt4-gpt5-1850337299?utm_source=tldrai)

Sam Altman has said that GPT5 is not yet in training.

[### Wolverine (GitHub Repo)](https://github.com/biobootloader/wolverine?utm_source=tldrai)

Wolverine uses GPT-4 to edit your scripts and tell you what went wrong.

[### Marketing Co-Pilot AI (Product Launch)](https://www.producthunt.com/posts/marketing-co-pilot-ai?utm_source=tldrai)

Describe your content style ‚Äî get a personalized list of 60 tweet ideas. Powered by AI. Generate up to 5 content plans per account. Save them in PDF or copy them to Notion.

[### European Parliament Prepares Tough AI Measures (2 minute read)](https://archive.ph/J5fUz?utm_source=tldrai)

The European parliament is preparing tough new measures over the use of artificial intelligence, including forcing chatbot makers to reveal if they use copyrighted material.

[### Generating code with tests (GitHub Repo)](https://github.com/microsoft/CodeT/tree/main/CodeT?utm_source=tldrai)

They achieved a new state of the art (already surpassed by Reflexion) on the challenging Human Eval benchmark by first generating tests then determining if the code generated passes those tests.

## The most important AI, ML, and data science news in a free daily email.

Subscribe

Join 500,000 readers for one daily email

[Privacy](/privacy)[Careers](https://jobs.ashbyhq.com/tldr.tech)[Advertise](/ai/advertise)

Timestamp: 1744590550