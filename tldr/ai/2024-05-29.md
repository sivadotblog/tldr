OpenAI Safety Board ü¶∫, Jan Leike Joins Anthropic üéâ, Reproducing GPT-2 by Andrej Karpathy üíª

[TLDR](/)

[Newsletters](/newsletters)

[Advertise](https://advertise.tldr.tech/)

[TLDR](/)

# TLDR AI 2024-05-29

## OpenAI Safety Board ü¶∫, Jan Leike Joins Anthropic üéâ, Reproducing GPT-2 by Andrej Karpathy üíª

### 

[### Hire your AI dream team ‚Äî with experts from Harvard, Stanford, and MIT (Sponsor)](https://ae.studio/?utm_campaign=tldrai-52924&amp;utm_source=tldrai&amp;utm_medium=newsletter)

Building an AI product is hard. Engineers who understand AI are expensive and difficult to find. And there's no way of telling who's legit and who's not.

That's why companies around the world trust [AE Studio](https://ae.studio/?utm_campaign=tldrai-52924&utm_source=tldrai&utm_medium=newsletter). They'll help you craft and implement the optimal Al solution for your business with a [team of world class Al experts](https://ae.studio/?utm_campaign=tldrai-52924&utm_source=tldrai&utm_medium=newsletter).

Their [development, design, and data science teams](https://ae.studio/?utm_campaign=tldrai-52924&utm_source=tldrai&utm_medium=newsletter) work closely with founders and executives to create [custom software and Al solutions](https://ae.studio/?utm_campaign=tldrai-52924&utm_source=tldrai&utm_medium=newsletter) that get the job done for a fraction of the cost of searching and hiring your own team.

Book a free consultation session today. [Get in touch now](https://ae.studio/?utm_campaign=tldrai-52924&utm_source=tldrai&utm_medium=newsletter)

üöÄ

### Headlines & Launches

[### OpenAI Safety Board (2 minute read)](https://openai.com/index/openai-board-forms-safety-and-security-committee/?utm_source=tldrai)

OpenAI formed a Safety and Security Committee after announcing the training of its new foundation model. This committee will be tasked with issuing recommendations to the board about actions to take as model capabilities continue to improve.

[### Jan Leike Joins Anthropic (1 minute read)](https://techcrunch.com/2024/05/28/anthropic-hires-former-openai-safety-lead-to-head-up-new-team/?utm_source=tldrai)

Jan Leike, a former OpenAI researcher who resigned over AI safety concerns, has joined Anthropic to lead a new "superalignment" team focusing on AI safety and security. Leike's team will address scalable oversight, weak-to-strong generalization, and automated alignment research.

[### New agent capabilities in Microsoft Copilot unlock business value (3 minute read)](https://www.microsoft.com/en-us/microsoft-365/blog/2024/05/21/new-agent-capabilities-in-microsoft-copilot-unlock-business-value?utm_source=tldrai)

Microsoft unveiled new features for Copilot at Build 2024, including Team Copilot for team collaboration, custom AI Agents to automate workflows, and Copilot Extensions and Connectors for easy customization. These enhancements aim to improve productivity and business process efficiency. The updates are currently in limited private preview, with broader availability expected later in 2024.

üß†

### Research & Innovation

[### gzip Predicts Data-dependent Scaling Laws (16 minute read)](https://arxiv.org/abs/2405.16684?utm_source=tldrai)

Scaling laws are a way to predict how well models will perform at certain sizes with a certain amount of data. They are expensive to obtain. This paper explores using gzip compression ratio as a strong signal to predict a data-dependent scaling law.

[### Transformers Learn to Solve Complex Arithmetic with New Embeddings (18 minute read)](https://arxiv.org/abs/2405.17399v1?utm_source=tldrai)

Researchers have improved transformer performance on arithmetic tasks by adding embeddings that encode each digit's position relative to the start of the number.

[### Schedule Free Optimizer (23 minute read)](https://arxiv.org/abs/2405.15682?utm_source=tldrai)

A few weeks ago, a new optimizer from Meta made the rounds as a promising alternative to Adam. This paper outlines the method in more detail, including the online updates portion of the method. In general, it seems like a promising result, especially when the total number of desired training steps is unknown at the beginning of training.

üë®‚Äçüíª

### Engineering & Resources

[### Syncly - Surface customer insights that NPS and CSAT don't tell (Sponsor)](https://syncly.app/?utm_source=tldrai)

Implement AI to get product and customer sentiment insight based on everyday customer interactions.

* Let AI identify the most critical issues with [Smart Insight](https://syncly.app/blog/syncly-insight-beta?utm_source=newsletter&utm_medium=email&utm_campaign=TLDR&utm_id=123) without manual work.
* Ask Syncly AI to identify what drives negativity in customer experience.
* With Customer AI, monitor happy/unhappy customers to prevent churn.

[Try Syncly for free!](https://syncly.app/?utm_source=newsletter&utm_medium=email&utm_campaign=TLDR&utm_id=123)

[### Improving Makeup Transfer (GitHub Repo)](https://github.com/snowfallingplum/csd-mt?utm_source=tldrai)

Content-Style Decoupled Makeup Transfer (CSD-MT) is a novel method for enhancing makeup transfer tasks without relying on real target images.

[### Reproducing GPT-2 (124M) in llm.c in 90 minutes for $20 (12 minute read)](https://github.com/karpathy/llm.c/discussions/481?utm_source=tldrai)

Andrej Karpathy has released an update to LLM C, a single self contained GPT-2 implementation aimed at reproducing the 2019 suite of models. With this most recent update, the library is able to train the smallest of these models in just 90 minutes. It runs end to end and has minimal dependencies.

[### Marigold Depth in Diffusers (8 minute read)](https://huggingface.co/docs/diffusers/main/en/using-diffusers/marigold_usage?utm_source=tldrai)

One of the best depth models is now available as a pipeline in diffusers. This tutorial discusses how to use the model, what you can do with it, and a trick to make it work seamlessly with videos by conditioning on the first frame's latents.

üéÅ

### Miscellaneous

[### Driving World Model (1 minute read)](https://vista-demo.github.io/?utm_source=tldrai)

Vista is a driving world model designed to improve prediction accuracy and action controllability for autonomous driving.

[### ‚ÄúI lost trust‚Äù: Why the OpenAI team in charge of safeguarding humanity imploded (9 minute read)](https://www.vox.com/future-perfect/2024/5/17/24158403/openai-resignations-ai-safety-ilya-sutskever-jan-leike-artificial-intelligence?utm_source=tldrai)

Key members of OpenAI's safety team, including leaders of the superalignment team tasked with ensuring AI alignment, have departed partly due to lost faith in CEO Sam Altman. These departures follow internal conflicts and concerns about OpenAI's safety priorities and the company's direction under Altman's leadership. OpenAI is updating its offboarding documents after criticisms regarding potential equity cancellations for those who refuse to sign non-disparagement agreements.

[### What GPT-4o illustrates about AI Regulation (4 minute read)](https://hyperdimensional.substack.com/p/what-gpt-4o-illustrates-about-ai?utm_source=tldrai)

This article discusses different approaches to AI regulation, contrasting model-level, use-level, and conduct-level frameworks. It argues that conduct-level regulation, which applies existing laws to new technology without excessive specificity, is preferable to use-level regulation, which could result in impractical restrictions and unnecessary complications for AI deployment. The recent EU AI Act's restrictions on AI's ability to infer emotions is cited as an example of the pitfalls of a use-level approach.

‚ö°Ô∏è

### Quick Links

[### Opera Adds Google's Gemini To Its Browsers (1 minute read)](https://www.theverge.com/2024/5/28/24166295/opera-google-gemini-aria-read-aloud-ai?utm_source=tldrai)

Opera browser has integrated Google's Gemini AI models into its Aria AI extension, providing more up-to-date information and conversational responses.

[### OpenAI sends internal memo releasing former employees from controversial exit agreements (3 minute read)](https://www.cnbc.com/2024/05/24/openai-sends-internal-memo-releasing-former-employees-from-non-disparagement-agreements-sam-altman.html?utm_source=tldrai)

OpenAI reversed a policy that would have revoked vested equity from former employees who didn't sign a non-disparagement agreement.

[### LaVague (GitHub Repo)](https://github.com/lavague-ai/LaVague?utm_source=tldrai)

An open-source large action model framework for developing AI web agents.

## The most important AI, ML, and data science news in a free daily email.

Subscribe

Join 500,000 readers for one daily email

[Privacy](/privacy)[Careers](https://jobs.ashbyhq.com/tldr.tech)[Advertise](/ai/advertise)

Timestamp: 1744590620